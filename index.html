<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="google-site-verification" content="FeA8wZzKiI6DkLLqCJMlBhKi6PsRnR1Dmdc2TDgTYcI" />
        <title>Antoine Yang</title>

        <link rel='icon' href='img/favicon.ico' type='image/x-icon'/>
        <link href="./css/bootstrap.min.css" rel="stylesheet">
        <link rel="stylesheet" href="./assets/academicons-1.7.0/css/academicons.css"/>
        <link rel="stylesheet" href="./assets/font-awesome-4.7.0/css/font-awesome.min.css"/>
	<script async defer src="https://buttons.github.io/buttons.js"></script>
    </head>

    <body>

      <!-- Navigation bar -->
      <div class="navbar navbar-default  navbar-fixed-top bg-info">
        <div class="container">
          <div class="navbar-header">

            <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
          </div>
          <div class="navbar-collapse collapse" id="navbar-main">

            <ul class="nav navbar-nav navbar-left">
              <li ><a href="#home">Home</a></li>
              <li ><a href="#news">News</a></li>
              <li ><a href="#research">Research</a></li>
              <li ><a href="#talks">Talks</a></li>
              <li ><a href="#teaching">Teaching</a></li>
              <li ><a href="#misc">Misc</a></li>
            </ul>
          </div>
        </div>
      </div>

      <!-- end of navigation bar -->
      <div style="height:40px;" id="home"></div>
      <div style="height:40px;"></div>

      <!-- CONTENTS -->
      <div class="container">
        <!-- Aboutme -->
        <div class="row" >
          <div class="col-xs-6 col-sm-4 col-md-2">
            <a class="thumbnail">
              <img src="./img/antoine.jpg" alt="Antoine Yang" class="img-rounded">
            </a>
          </div>

          <div class="col-xs-10 col-sm-6 col-md-4">
            <h1 class="text-info">Antoine Yang</h1>
            <h4 class="text-info">Senior Research Scientist, Google DeepMind</h4>
            <h5>
              <a href="mailto:antoineyang@google.com" class="text-info" title="e-Mail"><i class="fa fa-envelope-square fa-2x"></i></a>
	          <a href="https://scholar.google.com/citations?user=SlMKN6IAAAAJ" class="text-info" title="Google Scholar"><i class="ai ai-google-scholar-square ai-2x"></i></a>
              <a href="https://github.com/antoyang" class="text-info" title="GitHub"><i class="fa fa-github-square fa-2x"></i></a>
              <a href="https://www.linkedin.com/in/antoyang/" class="text-info" title="LinkedIn"><i class="fa fa-linkedin-square fa-2x"></i></a>
              <a href="https://twitter.com/AntoineYang2" class="text-info" title="Twitter"><i class="fa fa-twitter-square fa-2x"></i></a>
              <a href="https://www.youtube.com/channel/UCYJ4NEILh5DeYf-62eb1RBg" class="text-info" title="Twitter"><i class="fa fa-youtube-square fa-2x"></i></a>
		  </h5>
          </div>
        </div>
        <!-- end of Aboutme -->

         <p align="justify">
             I am a Senior Research Scientist at <a href="https://www.deepmind.com/">Google DeepMind</a> in London, working on the multi-modal capabilities of <a href="https://deepmind.google/models/gemini/">Gemini</a>.
             I am notably focusing on audio-visual, long, temporal and efficient video understanding, including modeling, data and evals on both the pretraining and the post-training fronts.
             These capabilities are used in a variety of Google products, including <a href="https://deepmind.google/technologies/project-astra/">Project Astra</a>, <a href="https://aistudio.google.com">AI Studio</a>, <a href="https://ai.google.dev/gemini-api/docs/video-understanding">the Gemini API</a>, <a href="https://www.youtube.com/">YouTube</a>, <a href="https://cloud.google.com/products/gemini">Google Cloud</a> and <a href="https://gemini.google.com/corp/">the Gemini app</a>.

             In 2023, I completed my PhD in the <a href="http://www.di.ens.fr/willow">WILLOW team</a> of <a href="http://www.inria.fr/">Inria Paris</a> and <a href="http://www.ens.fr">&Eacute;cole Normale Sup&eacute;rieure</a>, advised by <a href="https://antoine77340.github.io/">Antoine Miech</a>, <a href="http://people.ciirc.cvut.cz/~sivic/">Josef Sivic</a>, <a href="http://www.di.ens.fr/~laptev">Ivan Laptev</a> and <a href="https://www.di.ens.fr/willow/people_webpages/cordelia/">Cordelia Schmid</a>.
             My thesis, supported by a <a href="https://research.google/outreach/phd-fellowship/">Google PhD Fellowship</a>, focused on learning visual language models for video understanding.
             In 2020, I received an <a href="https://programmes.polytechnique.edu/cycle-ingenieur-polytechnicien/cycle-ingenieur-polytechnicien"> engineering degree </a> from <a href="https://www.polytechnique.edu/">École Polytechnique </a> and a MSc degree in <a href="https://www.master-mva.com/">Mathematics, Vision and Learning</a> from <a href="http://ens-paris-saclay.fr/">ENS Paris-Saclay</a>.
             I previously interned at <a href="http://www.noahlab.com.hk/">Huawei Noah's Ark Lab</a> and <a href="https://research.google">Google Research</a> <a href="https://research.google/teams/perception/">Perception</a>.
             See my <a href="https://www.linkedin.com/in/antoyang/">LinkedIn profile</a> for a full resume.
         </p>
        <hr>

        <!-- News header-->
        <div class="row" id="news" style="padding-top:30px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>News</h2>
          </div>
        </div>
        <!-- End news header -->

    <!-- Gemini 2.5 Update -->
    <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">05 / 2025</span>
          </div>
          <div class="col-sm-11 col-md-11">
              We released <a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/">Gemini 2.5 thinking model series</a> and <a href="https://developers.googleblog.com/en/gemini-2-5-video-understanding/">video blog</a> showing the performance and new use cases possible with Gemini 2.5!
              </div>
        </div>
    <div style="height:3px;"></div>

    <!-- Chapter-Llama accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">03 / 2025</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="https://imagine.enpc.fr/~lucas.ventura/chapter-llama/">Chapter-Llama</a> is accepted at <a href="https://cvpr.thecvf.com/">CVPR 2025</a>!
          </div>
        </div>
        <div style="height:3px;"></div>

   <!-- Gemma 3 Update -->
    <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">03 / 2025</span>
          </div>
          <div class="col-sm-11 col-md-11">
              We released <a href="https://blog.google/technology/developers/gemma-3/">Gemma 3</a>, the most capable model you can run on a single GPU or TPU!
              </div>
        </div>
    <div style="height:3px;"></div>

    <!-- Gemini 2.0 Update -->
    <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">12 / 2024</span>
          </div>
          <div class="col-sm-11 col-md-11">
              We released <a href="https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/">Gemini 2.0 Flash</a>, improving over Gemini 1.5 Pro at a lower cost and capable of native audio and image output!
              </div>
        </div>
    <div style="height:3px;"></div>

    <!-- September Update -->
    <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">09 / 2024</span>
          </div>
          <div class="col-sm-11 col-md-11">
              We released updated <a href="https://developers.googleblog.com/en/updated-production-ready-gemini-models-reduced-15-pro-pricing-increased-rate-limits-and-more/">Gemini 1.5 Pro, Flash</a> and <a href="https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/">Flash 8B</a> models!
              </div>
        </div>
    <div style="height:3px;"></div>

    <!-- CoVR-2 accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">09 / 2024</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="http://imagine.enpc.fr/~ventural/covr/">CoVR-2</a> is accepted at <a href="https://ieeexplore.ieee.org/abstract/document/10685001">TPAMI</a>!
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Astra release -->
    <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">05 / 2024</span>
          </div>
          <div class="col-sm-11 col-md-11">
              We introduced <a href="https://deepmind.google/technologies/gemini/project-astra/">Project Astra</a>, a prototype for real-time multi-modal interactions!
              </div>
        </div>
    <div style="height:3px;"></div>

    <!-- Gemini 1.5 Update -->
    <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">05 / 2024</span>
          </div>
          <div class="col-sm-11 col-md-11">
              We released an <a href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/#gemini-model-updates"> improved Gemini 1.5 Pro and a new Gemini 1.5 Flash</a>! Gemini 1.5 Pro also achieves SoTA on various new benchmarks including <a href="https://arxiv.org/abs/2405.08813">Cinepile</a>, <a href="https://arxiv.org/abs/2405.21075">Video-MME</a>, <a href="https://arxiv.org/abs/2411.04998">HourVideo</a> and <a href="https://research.google/blog/neptune-the-long-orbit-to-benchmarking-long-video-understanding/">Neptune</a>.
              </div>
        </div>
    <div style="height:3px;"></div>

    <!-- PhD Award -->
    <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">04 / 2024</span>
          </div>
          <div class="col-sm-11 col-md-11">
              My thesis received a special mention from the <a href="http://afrif.irisa.fr/?page_id=54/">AFRIF</a>.
              </div>
        </div>
    <div style="height:3px;"></div>

    <!-- Gemini 1.5 -->
    <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">02 / 2024</span>
          </div>
          <div class="col-sm-11 col-md-11">
              We released <a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/">Gemini 1.5 Pro</a>, capable of processing hours of video content!
              </div>
        </div>
    <div style="height:3px;"></div>

    <!-- CoVR accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">12 / 2023</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="http://imagine.enpc.fr/~ventural/covr/">CoVR</a> is accepted at <a href="https://aaai.org/conference/aaai/aaai-24/">AAAI 2024</a>!
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- PhD Defense -->
    <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">11 / 2023</span>
          </div>
          <div class="col-sm-11 col-md-11">
              I have successfully defended my <a href="https://hal.science/tel-04307117">PhD thesis</a>!
              </div>
        </div>
    <div style="height:3px;"></div>

    <!-- DeepMind RS -->
    <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">10 / 2023</span>
          </div>
          <div class="col-sm-11 col-md-11">
              I have joined <a href="https://www.deepmind.com/">Google DeepMind</a>'s Computer Vision team in London full-time as a Research Scientist!
          </div>
        </div>
    <div style="height:3px;"></div>

    <!-- ICCV'23 Consortium -->
    <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">10 / 2023</span>
          </div>
          <div class="col-sm-11 col-md-11">
              I am attending the <a href="https://iccv2023.thecvf.com//">ICCV 2023 Doctoral Consortium</a> in Paris.
          </div>
        </div>
    <div style="height:3px;"></div>

    <!-- VidChapters accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">09 / 2023</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="vidchapters.html">VidChapters-7M</a> is accepted at <a href="https://nips.cc/Conferences/2023/CallForDatasetsBenchmarks">NeurIPS 2023 Track on Datasets and Benchmarks</a>!
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- ICVSS -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">07 / 2023</span>
          </div>
          <div class="col-sm-11 col-md-11">
              I am attending the <a href="https://iplab.dmi.unict.it/icvss2023/Home"> ICVSS Summer School</a> in Sampieri.
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Ellis workshop -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">05 / 2023</span>
          </div>
          <div class="col-sm-11 col-md-11">
              I am attending an <a href="https://ellis.eu/">ELLIS computer vision workshop</a> in Metzingen.
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Vid2Seq Blog and Code -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">03 / 2023</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="vid2seq.html">Vid2Seq</a> is featured on the <a href="https://ai.googleblog.com/2023/03/vid2seq-pretrained-visual-language.html">Google AI Blog</a>, and the code is released <a href="https://github.com/google-research/scenic/tree/main/scenic/projects/vid2seq"> here</a>.
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Vid2Seq accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">02 / 2023</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="vid2seq.html">Vid2Seq</a> is accepted at <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>!
          </div>
        </div>
        <div style="height:3px;"></div>
	      
    <!-- FrozenBiLM accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">09 / 2022</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="frozenbilm.html">FrozenBiLM</a> is accepted at <a href="https://nips.cc/Conferences/2022">NeurIPS 2022</a>!
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Google internship -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">06 / 2022</span>
          </div>
          <div class="col-sm-11 col-md-11">
              I am starting a 6-month research internship at <a href="https://research.google/">Google Research</a> (<a href="https://research.google/teams/perception/">Perception team</a>) in Grenoble.
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Just Ask extension accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">04 / 2022</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="just-ask.html">Just Ask extension</a> is accepted at the <a href="https://www.computer.org/csdl/journal/tp">TPAMI Special Issue on the Best Papers of ICCV 2021</a>!
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- TubeDETR accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">03 / 2022</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="tubedetr.html">TubeDETR</a> is accepted at <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a> as an oral!
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Just Ask accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">07 / 2021</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="just-ask.html">Just Ask</a> is accepted at <a href="http://iccv2021.thecvf.com/home">ICCV 2021</a> as an oral!
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Just Ask - HVU CVPR 2021 Talk -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">06 / 2021</span>
          </div>
          <div class="col-sm-11 col-md-11">
              I gave talks about <a href="just-ask.html">Just Ask</a> at the <a href="https://holistic-video-understanding.github.io/workshops/cvpr2021.html"> CVPR 2021 Holistic Video Understanding Workshop</a> and at the <a href="https://www.rocq.inria.fr/semdoc/">Inria Junior Seminar</a>.
          </div>
        </div>
        <div style="height:3px;"></div>

	<!-- Willow PhD -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">09 / 2020</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I am starting my PhD at <a href="http://www.di.ens.fr/willow">Inria WILLOW</a> in Paris.
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- MVA degree -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">09 / 2020</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I have received a <a href="https://www.master-mva.com/"> MSc degree </a> with highest honors and jury congratulations from <a href="https://ens-paris-saclay.fr/">ENS Paris-Saclay</a>.
          </div>
        </div>
        <div style="height:3px;"></div>

	<!-- Willow internship -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">04 / 2020</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I am starting a 5-month research internship at <a href="http://www.di.ens.fr/willow">Inria WILLOW</a> in Paris.
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- NASEFH accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">12 &nbsp;/ 2019 </span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="nasefh.html">NAS evaluation is frustratingly hard</a> is accepted at <a href="https://iclr.cc/Conferences/2020">ICLR 2020</a>!
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- X degree -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">09 / 2019</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I have received an <a href="https://programmes.polytechnique.edu/cycle-ingenieur-polytechnicien/cycle-ingenieur-polytechnicien"> engineering degree </a> from <a href="https://www.polytechnique.edu/">École Polytechnique</a>.
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Huawei internship -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">04 / 2019</span>
          </div>
          <div class="col-sm-11 col-md-11">
              I am starting a 5-month research internship at <a href="http://www.noahlab.com.hk/">Huawei Noah's Ark Lab</a> (<a href="http://dev3.noahlab.com.hk/research.html">AI Theory team</a>) in London.
          </div>
        </div>
        <div style="height:3px;"></div>

        <!-- end of news -->
        <hr>

        <!-- Research -->
        <div class="row"  id="research" style="padding-top:30px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>Research</h2>
              <p>See my <a href="https://scholar.google.com/citations?user=SlMKN6IAAAAJ">Google Scholar </a> and <a href="https://github.com/antoyang"> GitHub </a> profiles for more information.</p>

<!-- Gemini 2.5 -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4" style='height:120px'>
		<a class="thumbnail">
		<!-- <img src="./img/gemini_2p5.png" height="100%" alt="Gemini 2.5"> -->
		<img src="./img/gemini_2p0.png" height="100%" alt="Gemini 2">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities.</strong> </br>
	Gemini Team, Google<br>
          Google Blog 2025 <br>
	<a href="https://arxiv.org/pdf/2507.06261"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex15">bibtex</button>
	<div id="bibtex15" class="collapse">
	<pre><tt>@misc{comanici2025gemini25pushingfrontier,
      title={Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities}, 
      author={Gheorghe Comanici and Eric Bieber and Mike Schaekermann and Ice Pasupat and Noveen Sachdeva and Inderjit Dhillon and Marcel Blistein and Ori Ram and Dan Zhang and Evan Rosen and Luke Marris and Sam Petulla and Colin Gaffney and Asaf Aharoni and Nathan Lintz and Tiago Cardal Pais and Henrik Jacobsson and Idan Szpektor and Nan-Jiang Jiang and Krishna Haridasan and Ahmed Omran and Nikunj Saunshi and Dara Bahri and Gaurav Mishra and Eric Chu and Toby Boyd and Brad Hekman and Aaron Parisi and Chaoyi Zhang and Kornraphop Kawintiranon and Tania Bedrax-Weiss and Oliver Wang and Ya Xu and Ollie Purkiss and Uri Mendlovic and Ilaï Deutel and Nam Nguyen and Adam Langley and Flip Korn and Lucia Rossazza and Alexandre Ramé and Sagar Waghmare and Helen Miller and Vaishakh Keshava and Ying Jian and Xiaofan Zhang and Raluca Ada Popa and Kedar Dhamdhere and Blaž Bratanič and Kyuyeun Kim and Terry Koo and Ferran Alet and Yi-ting Chen and Arsha Nagrani and Hannah Muckenhirn and Zhiyuan Zhang and Corbin Quick and Filip Pavetić and Duc Dung Nguyen and Joao Carreira and Michael Elabd and Haroon Qureshi and Fabian Mentzer and Yao-Yuan Yang and Danielle Eisenbud and Anmol Gulati and Ellie Talius and Eric Ni and Sahra Ghalebikesabi and Edouard Yvinec and Alaa Saade and Thatcher Ulrich and Lorenzo Blanco and Dan A. Calian and Muhuan Huang and Aäron van den Oord and Naman Goyal and Terry Chen and Praynaa Rawlani and Christian Schallhart and Swachhand Lokhande and Xianghong Luo and Jyn Shan and Ceslee Montgomery and Victoria Krakovna and Federico Piccinini and Omer Barak and Jingyu Cui and Yiling Jia and Mikhail Dektiarev and Alexey Kolganov and Shiyu Huang and Zhe Chen and Xingyu Wang and Jessica Austin and Peter de Boursac and Evgeny Sluzhaev and Frank Ding and Huijian Li and Surya Bhupatiraju and Mohit Agarwal and Sławek Kwasiborski and Paramjit Sandhu and Patrick Siegler and Ahmet Iscen and Eyal Ben-David and Shiraz Butt and Miltos Allamanis and Seth Benjamin and Robert Busa-Fekete and Felix Hernandez-Campos and Sasha Goldshtein and Matt Dibb and Weiyang Zhang and Annie Marsden and Carey Radebaugh and Stephen Roller and Abhishek Nayyar and Jacob Austin and Tayfun Terzi and Bhargav Kanagal Shamanna and Pete Shaw and Aayush Singh and Florian Luisier and Artur Mendonça and Vaibhav Aggarwal and Larisa Markeeva and Claudio Fantacci and Sergey Brin and HyunJeong Choe and Guanyu Wang and Hartwig Adam and Avigail Dabush and Tatsuya Kiyono and Eyal Marcus and Jeremy Cole and Theophane Weber and Hongrae Lee and Ronny Huang and Alex Muzio and Leandro Kieliger and Maigo Le and Courtney Biles and Long Le and Archit Sharma and Chengrun Yang and Avery Lamp and Dave Dopson and Nate Hurley and Katrina and Xu and Zhihao Shan and Shuang Song and Jiewen Tan and Alexandre Senges and George Zhang and Chong You and Yennie Jun and David Raposo and Susanna Ricco and Xuan Yang and Weijie Chen and Prakhar Gupta and Arthur Szlam and Kevin Villela and Chun-Sung Ferng and Daniel Kasenberg and Chen Liang and Rui Zhu and Arunachalam Narayanaswamy and Florence Perot and Paul Pucciarelli and Anna Shekhawat and Alexey Stern and Rishikesh Ingale and Stefani Karp and Sanaz Bahargam and Adrian Goedeckemeyer and Jie Han and Sicheng Li and Andrea Tacchetti and Dian Yu and Abhishek Chakladar and Zhiying Zhang and Mona El Mahdy and Xu Gao and Dale Johnson and Samrat Phatale and AJ Piergiovanni and Hyeontaek Lim and Clement Farabet and Carl Lebsack and Theo Guidroz and John Blitzer and Nico Duduta and David Madras and Steve Li and Daniel von Dincklage and Xin Li and Mahdis Mahdieh and George Tucker and Ganesh Jawahar and Owen Xiao and Danny Tarlow and Robert Geirhos and Noam Velan and Daniel Vlasic and Kalesha Bullard and SK Park and Nishesh Gupta and Kellie Webster and Ayal Hitron and Jieming Mao and Julian Eisenschlos and Laurel Prince and Nina D'Souza and Kelvin Zheng and Sara Nasso and Gabriela Botea and Carl Doersch and Caglar Unlu and Chris Alberti and Alexey Svyatkovskiy and Ankita Goel and Krzysztof Choromanski and Pan-Pan Jiang and Richard Nguyen and Four Flynn and Daria Ćurko and Peter Chen and Nicholas Roth and Kieran Milan and Caleb Habtegebriel and Shashi Narayan and Michael Moffitt and Jake Marcus and Thomas Anthony and Brendan McMahan and Gowoon Cheon and Ruibo Liu and Megan Barnes and Lukasz Lew and Rebeca Santamaria-Fernandez and Mayank Upadhyay and Arjun Akula and Arnar Mar Hrafnkelsson and Alvaro Caceres and Andrew Bunner and Michal Sokolik and Subha Puttagunta and Lawrence Moore and Berivan Isik and Weilun Chen and Jay Hartford and Lawrence Chan and Pradeep Shenoy and Dan Holtmann-Rice and Jane Park and Fabio Viola and Alex Salcianu and Sujeevan Rajayogam and Ian Stewart-Binks and Zelin Wu and Richard Everett and Xi Xiong and Pierre-Antoine Manzagol and Gary Leung and Carl Saroufim and Bo Pang and Dawid Wegner and George Papamakarios and Jennimaria Palomaki and Helena Pankov and Guangda Lai and Guilherme Tubone and Shubin Zhao and Theofilos Strinopoulos and Seth Neel and Mingqiu Wang and Joe Kelley and Li Li and Pingmei Xu and Anitha Vijayakumar and Andrea D'olimpio and Omer Levy and Massimo Nicosia and Grigory Rozhdestvenskiy and Ni Lao and Sirui Xie and Yash Katariya and Jon Simon and Sanjiv Kumar and Florian Hartmann and Michael Kilgore and Jinhyuk Lee and Aroma Mahendru and Roman Ring and Tom Hennigan and Fiona Lang and Colin Cherry and David Steiner and Dawsen Hwang and Ray Smith and Pidong Wang and Jeremy Chen and Ming-Hsuan Yang and Sam Kwei and Philippe Schlattner and Donnie Kim and Ganesh Poomal Girirajan and Nikola Momchev and Ayushi Agarwal and Xingyi Zhou and Ilkin Safarli and Zachary Garrett and AJ Pierigiovanni and Sarthak Jauhari and Alif Raditya Rochman and Shikhar Vashishth and Quan Yuan and Christof Angermueller and Jon Blanton and Xinying Song and Nitesh Bharadwaj Gundavarapu and Thi Avrahami and Maxine Deines and Subhrajit Roy and Manish Gupta and Christopher Semturs and Shobha Vasudevan and Aditya Srikanth Veerubhotla and Shriya Sharma and Josh Jacob and Zhen Yang and Andreas Terzis and Dan Karliner and Auriel Wright and Tania Rojas-Esponda and Ashley Brown and Abhijit Guha Roy and Pawan Dogra and Andrei Kapishnikov and Peter Young and Wendy Kan and Vinodh Kumar Rajendran and Maria Ivanova and Salil Deshmukh and Chia-Hua Ho and Mike Kwong and Stav Ginzburg and Annie Louis and KP Sawhney and Slav Petrov and Jing Xie and Yunfei Bai and Georgi Stoyanov and Alex Fabrikant and Rajesh Jayaram and Yuqi Li and Joe Heyward and Justin Gilmer and Yaqing Wang and Radu Soricut and Luyang Liu and Qingnan Duan and Jamie Hayes and Maura O'Brien and Gaurav Singh Tomar and Sivan Eiger and Bahar Fatemi and Jeffrey Hui and Catarina Barros and Adaeze Chukwuka and Alena Butryna and Saksham Thakur and Austin Huang and Zhufeng Pan and Haotian Tang and Serkan Cabi and Tulsee Doshi and Michiel Bakker and Sumit Bagri and Ruy Ley-Wild and Adam Lelkes and Jennie Lees and Patrick Kane and David Greene and Shimu Wu and Jörg Bornschein and Gabriela Surita and Sarah Hodkinson and Fangtao Li and Chris Hidey and Sébastien Pereira and Sean Ammirati and Phillip Lippe and Adam Kraft and Pu Han and Sebastian Gerlach and Zifeng Wang and Liviu Panait and Feng Han and Brian Farris and Yingying Bi and Hannah DeBalsi and Miaosen Wang and Gladys Tyen and James Cohan and Susan Zhang and Jarred Barber and Da-Woon Chung and Jaeyoun Kim and Markus Kunesch and Steven Pecht and Nami Akazawa and Abe Friesen and James Lyon and Ali Eslami and Junru Wu and Jie Tan and Yue Song and Ravi Kumar and Chris Welty and Ilia Akolzin and Gena Gibson and Sean Augenstein and Arjun Pillai and Nancy Yuen and Du Phan and Xin Wang and Iain Barr and Heiga Zen and Nan Hua and Casper Liu and Jilei and Wang and Tanuj Bhatia and Hao Xu and Oded Elyada and Pushmeet Kohli and Mirek Olšák and Ke Chen and Azalia Mirhoseini and Noam Shazeer and Shoshana Jakobovits and Maggie Tran and Nolan Ramsden and Tarun Bharti and Fred Alcober and Yunjie Li and Shilpa Shetty and Jing Chen and Dmitry Kalashnikov and Megha Nawhal and Sercan Arik and Hanwen Chen and Michiel Blokzijl and Shubham Gupta and James Rubin and Rigel Swavely and Sophie Bridgers and Ian Gemp and Chen Su and Arun Suggala and Juliette Pluto and Mary Cassin and Alain Vaucher and Kaiyang Ji and Jiahao Cai and Andrew Audibert and Animesh Sinha and David Tian and Efrat Farkash and Amy Hua and Jilin Chen and Duc-Hieu Tran and Edward Loper and Nicole Brichtova and Lara McConnaughey and Ballie Sandhu and Robert Leland and Doug DeCarlo and Andrew Over and James Huang and Xing Wu and Connie Fan and Eric Li and Yun Lei and Deepak Sharma and Cosmin Paduraru and Luo Yu and Matko Bošnjak and Phuong Dao and Min Choi and Sneha Kudugunta and Jakub Adamek and Carlos Guía and Ali Khodaei and Jie Feng and Wenjun Zeng and David Welling and Sandeep Tata and Christina Butterfield and Andrey Vlasov and Seliem El-Sayed and Swaroop Mishra and Tara Sainath and Shentao Yang and RJ Skerry-Ryan and Jeremy Shar and Robert Berry and Arunkumar Rajendran and Arun Kandoor and Andrea Burns and Deepali Jain and Tom Stone and Wonpyo Park and Shibo Wang and Albin Cassirer and Guohui Wang and Hayato Kobayashi and Sergey Rogulenko and Vineetha Govindaraj and Mikołaj Rybiński and Nadav Olmert and Colin Evans and Po-Sen Huang and Kelvin Xu and Premal Shah and Terry Thurk and Caitlin Sikora and Mu Cai and Jin Xie and Elahe Dabir and Saloni Shah and Norbert Kalb and Carrie Zhang and Shruthi Prabhakara and Amit Sabne and Artiom Myaskovsky and Vikas Raunak and Blanca Huergo and Behnam Neyshabur and Jon Clark and Ye Zhang and Shankar Krishnan and Eden Cohen and Dinesh Tewari and James Lottes and Yumeya Yamamori and Hui and Li and Mohamed Elhawaty and Ada Maksutaj Oflazer and Adrià Recasens and Sheryl Luo and Duy Nguyen and Taylor Bos and Kalyan Andra and Ana Salazar and Ed Chi and Jeongwoo Ko and Matt Ginsberg and Anders Andreassen and Anian Ruoss and Todor Davchev and Elnaz Davoodi and Chenxi Liu and Min Kim and Santiago Ontanon and Chi Ming To and Dawei Jia and Rosemary Ke and Jing Wang and Anna Korsun and Moran Ambar and Ilya Kornakov and Irene Giannoumis and Toni Creswell and Denny Zhou and Yi Su and Ishaan Watts and Aleksandr Zaks and Evgenii Eltyshev and Ziqiang Feng and Sidharth Mudgal and Alex Kaskasoli and Juliette Love and Kingshuk Dasgupta and Sam Shleifer and Richard Green and Sungyong Seo and Chansoo Lee and Dale Webster and Prakash Shroff and Ganna Raboshchuk and Isabel Leal and James Manyika and Sofia Erell and Daniel Murphy and Zhisheng Xiao and Anton Bulyenov and Julian Walker and Mark Collier and Matej Kastelic and Nelson George and Sushant Prakash and Sailesh Sidhwani and Alexey Frolov and Steven Hansen and Petko Georgiev and Tiberiu Sosea and Chris Apps and Aishwarya Kamath and David Reid and Emma Cooney and Charlotte Magister and Oriana Riva and Alec Go and Pu-Chin Chen and Sebastian Krause and Nir Levine and Marco Fornoni and Ilya Figotin and Nick Roy and Parsa Mahmoudieh and Vladimir Magay and Mukundan Madhavan and Jin Miao and Jianmo Ni and Yasuhisa Fujii and Ian Chou and George Scrivener and Zak Tsai and Siobhan Mcloughlin and Jeremy Selier and Sandra Lefdal and Jeffrey Zhao and Abhijit Karmarkar and Kushal Chauhan and Shivanker Goel and Zhaoyi Zhang and Vihan Jain and Parisa Haghani and Mostafa Dehghani and Jacob Scott and Erin Farnese and Anastasija Ilić and Steven Baker and Julia Pawar and Li Zhong and Josh Camp and Yoel Zeldes and Shravya Shetty and Anand Iyer and Vít Listík and Jiaxian Guo and Luming Tang and Mark Geller and Simon Bucher and Yifan Ding and Hongzhi Shi and Carrie Muir and Dominik Grewe and Ramy Eskander and Octavio Ponce and Boqing Gong and Derek Gasaway and Samira Khan and Umang Gupta and Angelos Filos and Weicheng Kuo and Klemen Kloboves and Jennifer Beattie and Christian Wright and Leon Li and Alicia Jin and Sandeep Mariserla and Miteyan Patel and Jens Heitkaemper and Dilip Krishnan and Vivek Sharma and David Bieber and Christian Frank and John Lambert and Paul Caron and Martin Polacek and Mai Giménez and Himadri Choudhury and Xing Yu and Sasan Tavakkol and Arun Ahuja and Franz Och and Rodolphe Jenatton and Wojtek Skut and Bryan Richter and David Gaddy and Andy Ly and Misha Bilenko and Megh Umekar and Ethan Liang and Martin Sevenich and Mandar Joshi and Hassan Mansoor and Rebecca Lin and Sumit Sanghai and Abhimanyu Singh and Xiaowei Li and Sudheendra Vijayanarasimhan and Zaheer Abbas and Yonatan Bitton and Hansa Srinivasan and Manish Reddy Vuyyuru and Alexander Frömmgen and Yanhua Sun and Ralph Leith and Alfonso Castaño and DJ Strouse and Le Yan and Austin Kyker and Satish Kambala and Mary Jasarevic and Thibault Sellam and Chao Jia and Alexander Pritzel and Raghavender R and Huizhong Chen and Natalie Clay and Sudeep Gandhe and Sean Kirmani and Sayna Ebrahimi and Hannah Kirkwood and Jonathan Mallinson and Chao Wang and Adnan Ozturel and Kuo Lin and Shyam Upadhyay and Vincent Cohen-Addad and Sean Purser-haskell and Yichong Xu and Ebrahim Songhori and Babi Seal and Alberto Magni and Almog Gueta and Tingting Zou and Guru Guruganesh and Thais Kagohara and Hung Nguyen and Khalid Salama and Alejandro Cruzado Ruiz and Justin Frye and Zhenkai Zhu and Matthias Lochbrunner and Simon Osindero and Wentao Yuan and Lisa Lee and Aman Prasad and Lam Nguyen Thiet and Daniele Calandriello and Victor Stone and Qixuan Feng and Han Ke and Maria Voitovich and Geta Sampemane and Lewis Chiang and Ling Wu and Alexander Bykovsky and Matt Young and Luke Vilnis and Ishita Dasgupta and Aditya Chawla and Qin Cao and Bowen Liang and Daniel Toyama and Szabolcs Payrits and Anca Stefanoiu and Dimitrios Vytiniotis and Ankesh Anand and Tianxiao Shen and Blagoj Mitrevski and Michael Tschannen and Sreenivas Gollapudi and Aishwarya P S and José Leal and Zhe Shen and Han Fu and Wei Wang and Arvind Kannan and Doron Kukliansky and Sergey Yaroshenko and Svetlana Grant and Umesh Telang and David Wood and Alexandra Chronopoulou and Alexandru Ţifrea and Tao Zhou and Tony and Nguy\~ên and Muge Ersoy and Anima Singh and Meiyan Xie and Emanuel Taropa and Woohyun Han and Eirikur Agustsson and Andrei Sozanschi and Hui Peng and Alex Chen and Yoel Drori and Efren Robles and Yang Gao and Xerxes Dotiwalla and Ying Chen and Anudhyan Boral and Alexei Bendebury and John Nham and Chris Tar and Luis Castro and Jiepu Jiang and Canoee Liu and Felix Halim and Jinoo Baek and Andy Wan and Jeremiah Liu and Yuan Cao and Shengyang Dai and Trilok Acharya and Ruoxi Sun and Fuzhao Xue and Saket Joshi and Morgane Lustman and Yongqin Xian and Rishabh Joshi and Deep Karkhanis and Nora Kassner and Jamie Hall and Xiangzhuo Ding and Gan Song and Gang Li and Chen Zhu and Yana Kulizhskaya and Bin Ni and Alexey Vlaskin and Solomon Demmessie and Lucio Dery and Salah Zaiem and Yanping Huang and Cindy Fan and Felix Gimeno and Ananth Balashankar and Koji Kojima and Hagai Taitelbaum and Maya Meng and Dero Gharibian and Sahil Singla and Wei Chen and Ambrose Slone and Guanjie Chen and Sujee Rajayogam and Max Schumacher and Suyog Kotecha and Rory Blevins and Qifei Wang and Mor Hazan Taege and Alex Morris and Xin Liu and Fayaz Jamil and Richard Zhang and Pratik Joshi and Ben Ingram and Tyler Liechty and Ahmed Eleryan and Scott Baird and Alex Grills and Gagan Bansal and Shan Han and Kiran Yalasangi and Shawn Xu and Majd Al Merey and Isabel Gao and Felix Weissenberger and Igor Karpov and Robert Riachi and Ankit Anand and Gautam Prasad and Kay Lamerigts and Reid Hayes and Jamie Rogers and Mandy Guo and Ashish Shenoy and Qiong and Hu and Kyle He and Yuchen Liu and Polina Zablotskaia and Sagar Gubbi and Yifan Chang and Jay Pavagadhi and Kristian Kjems and Archita Vadali and Diego Machado and Yeqing Li and Renshen Wang and Dipankar Ghosh and Aahil Mehta and Dana Alon and George Polovets and Alessio Tonioni and Nate Kushman and Joel D'sa and Lin Zhuo and Allen Wu and Rohin Shah and John Youssef and Jiayu Ye and Justin Snyder and Karel Lenc and Senaka Buthpitiya and Matthew Tung and Jichuan Chang and Tao Chen and David Saxton and Jenny Lee and Lydia Lihui Zhang and James Qin and Prabakar Radhakrishnan and Maxwell Chen and Piotr Ambroszczyk and Metin Toksoz-Exley and Yan Zhong and Nitzan Katz and Brendan O'Donoghue and Tamara von Glehn and Adi Gerzi Rosenthal and Aga Świetlik and Xiaokai Zhao and Nick Fernando and Jinliang Wei and Jieru Mei and Sergei Vassilvitskii and Diego Cedillo and Pranjal Awasthi and Hui Zheng and Koray Kavukcuoglu and Itay Laish and Joseph Pagadora and Marc Brockschmidt and Christopher A. Choquette-Choo and Arunkumar Byravan and Yifeng Lu and Xu Chen and Mia Chen and Kenton Lee and Rama Pasumarthi and Sijal Bhatnagar and Aditya Shah and Qiyin Wu and Zhuoyuan Chen and Zack Nado and Bartek Perz and Zixuan Jiang and David Kao and Ganesh Mallya and Nino Vieillard and Lantao Mei and Sertan Girgin and Mandy Jordan and Yeongil Ko and Alekh Agarwal and Yaxin Liu and Yasemin Altun and Raoul de Liedekerke and Anastasios Kementsietsidis and Daiyi Peng and Dangyi Liu and Utku Evci and Peter Humphreys and Austin Tarango and Xiang Deng and Yoad Lewenberg and Kevin Aydin and Chengda Wu and Bhavishya Mittal and Tsendsuren Munkhdalai and Kleopatra Chatziprimou and Rodrigo Benenson and Uri First and Xiao Ma and Jinning Li and Armand Joulin and Hamish Tomlinson and Tingnan Zhang and Milad Nasr and Zhi Hong and Michaël Sander and Lisa Anne Hendricks and Anuj Sharma and Andrew Bolt and Eszter Vértes and Jiri Simsa and Tomer Levinboim and Olcan Sercinoglu and Divyansh Shukla and Austin Wu and Craig Swanson and Danny Vainstein and Fan Bu and Bo Wang and Ryan Julian and Charles Yoon and Sergei Lebedev and Antonious Girgis and Bernd Bandemer and David Du and Todd Wang and Xi Chen and Ying Xiao and Peggy Lu and Natalie Ha and Vlad Ionescu and Simon Rowe and Josip Matak and Federico Lebron and Andreas Steiner and Lalit Jain and Manaal Faruqui and Nicolas Lacasse and Georgie Evans and Neesha Subramaniam and Dean Reich and Giulia Vezzani and Aditya Pandey and Joe Stanton and Tianhao Zhou and Liam McCafferty and Henry Griffiths and Verena Rieser and Soheil Hassas Yeganeh and Eleftheria Briakou and Lu Huang and Zichuan Wei and Liangchen Luo and Erik Jue and Gabby Wang and Victor Cotruta and Myriam Khan and Jongbin Park and Qiuchen Guo and Peiran Li and Rong Rong and Diego Antognini and Anastasia Petrushkina and Chetan Tekur and Eli Collins and Parul Bhatia and Chester Kwak and Wenhu Chen and Arvind Neelakantan and Immanuel Odisho and Sheng Peng and Vincent Nallatamby and Vaibhav Tulsyan and Fabian Pedregosa and Peng Xu and Raymond Lin and Yulong Wang and Emma Wang and Sholto Douglas and Reut Tsarfaty and Elena Gribovskaya and Renga Aravamudhan and Manu Agarwal and Mara Finkelstein and Qiao Zhang and Elizabeth Cole and Phil Crone and Sarmishta Velury and Anil Das and Chris Sauer and Luyao Xu and Danfeng Qin and Chenjie Gu and Dror Marcus and CJ Zheng and Wouter Van Gansbeke and Sobhan Miryoosefi and Haitian Sun and YaGuang Li and Charlie Chen and Jae Yoo and Pavel Dubov and Alex Tomala and Adams Yu and Paweł Wesołowski and Alok Gunjan and Eddie Cao and Jiaming Luo and Nikhil Sethi and Arkadiusz Socala and Laura Graesser and Tomas Kocisky and Arturo BC and Minmin Chen and Edward Lee and Sophie Wang and Weize Kong and Qiantong Xu and Nilesh Tripuraneni and Yiming Li and Xinxin Yu and Allen Porter and Paul Voigtlaender and Biao Zhang and Arpi Vezer and Sarah York and Qing Wei and Geoffrey Cideron and Mark Kurzeja and Seungyeon Kim and Benny Li and Angéline Pouget and Hyo Lee and Kaspar Daugaard and Yang Li and Dave Uthus and Aditya Siddhant and Paul Cavallaro and Sriram Ganapathy and Maulik Shah and Rolf Jagerman and Jeff Stanway and Piermaria Mendolicchio and Li Xiao and Kayi Lee and Tara Thompson and Shubham Milind Phal and Jason Chase and Sun Jae Lee and Adrian N Reyes and Disha Shrivastava and Zhen Qin and Roykrong Sukkerd and Seth Odoom and Lior Madmoni and John Aslanides and Jonathan Herzig and Elena Pochernina and Sheng Zhang and Parker Barnes and Daisuke Ikeda and Qiujia Li and Shuo-yiin Chang and Shakir Mohamed and Jim Sproch and Richard Powell and Bidisha Samanta and Domagoj Ćevid and Anton Kovsharov and Shrestha Basu Mallick and Srinivas Tadepalli and Anne Zheng and Kareem Ayoub and Andreas Noever and Christian Reisswig and Zhuo Xu and Junhyuk Oh and Martin Matysiak and Tim Blyth and Shereen Ashraf and Julien Amelot and Boone Severson and Michele Bevilacqua and Motoki Sano and Ethan Dyer and Ofir Roval and Anu Sinha and Yin Zhong and Sagi Perel and Tea Sabolić and Johannes Mauerer and Willi Gierke and Mauro Verzetti and Rodrigo Cabrera and Alvin Abdagic and Steven Hemingray and Austin Stone and Jong Lee and Farooq Ahmad and Karthik Raman and Lior Shani and Jonathan Lai and Orhan Firat and Nathan Waters and Eric Ge and Mo Shomrat and Himanshu Gupta and Rajeev Aggarwal and Tom Hudson and Bill Jia and Simon Baumgartner and Palak Jain and Joe Kovac and Junehyuk Jung and Ante Žužul and Will Truong and Morteza Zadimoghaddam and Songyou Peng and Marco Liang and Rachel Sterneck and Balaji Lakshminarayanan and Machel Reid and Oliver Woodman and Tong Zhou and Jianling Wang and Vincent Coriou and Arjun Narayanan and Jay Hoover and Yenai Ma and Apoorv Jindal and Clayton Sanford and Doug Reid and Swaroop Ramaswamy and Alex Kurakin and Roland Zimmermann and Yana Lunts and Dragos Dena and Zalán Borsos and Vered Cohen and Shujian Zhang and Will Grathwohl and Robert Dadashi and Morgan Redshaw and Joshua Kessinger and Julian Odell and Silvano Bonacina and Zihang Dai and Grace Chen and Ayush Dubey and Pablo Sprechmann and Mantas Pajarskas and Wenxuan Zhou and Niharika Ahuja and Tara Thomas and Martin Nikoltchev and Matija Kecman and Bharath Mankalale and Andrey Ryabtsev and Jennifer She and Christian Walder and Jiaming Shen and Lu Li and Carolina Parada and Sheena Panthaplackel and Okwan Kwon and Matt Lawlor and Utsav Prabhu and Yannick Schroecker and Marc'aurelio Ranzato and Pete Blois and Iurii Kemaev and Ting Yu and Dmitry and Lepikhin and Hao Xiong and Sahand Sharifzadeh and Oleaser Johnson and Jeremiah Willcock and Rui Yao and Greg Farquhar and Sujoy Basu and Hidetoshi Shimokawa and Nina Anderson and Haiguang Li and Khiem Pham and Yizhong Liang and Sebastian Borgeaud and Alexandre Moufarek and Hideto Kazawa and Blair Kutzman and Marcin Sieniek and Sara Smoot and Ruth Wang and Natalie Axelsson and Nova Fallen and Prasha Sundaram and Yuexiang Zhai and Varun Godbole and Petros Maniatis and Alek Wang and Ilia Shumailov and Santhosh Thangaraj and Remi Crocker and Nikita Gupta and Gang Wu and Phil Chen and Gellért Weisz and Celine Smith and Mojtaba Seyedhosseini and Boya Fang and Xiyang Luo and Roey Yogev and Zeynep Cankara and Andrew Hard and Helen Ran and Rahul Sukthankar and George Necula and Gaël Liu and Honglong Cai and Praseem Banzal and Daniel Keysers and Sanjay Ghemawat and Connie Tao and Emma Dunleavy and Aditi Chaudhary and Wei Li and Maciej Mikuła and Chen-Yu Lee and Tiziana Refice and Krishna Somandepalli and Alexandre Fréchette and Dan Bahir and John Karro and Keith Rush and Sarah Perrin and Bill Rosgen and Xiaomeng Yang and Clara Huiyi Hu and Mahmoud Alnahlawi and Justin Mao-Jones and Roopal Garg and Hoang Nguyen and Bat-Orgil Batsaikhan and Iñaki Iturrate and Anselm Levskaya and Avi Singh and Ashyana Kachra and Tony Lu and Denis Petek and Zheng Xu and Mark Graham and Lukas Zilka and Yael Karov and Marija Kostelac and Fangyu Liu and Yaohui Guo and Weiyue Wang and Bernd Bohnet and Emily Pitler and Tony Bruguier and Keisuke Kinoshita and Chrysovalantis Anastasiou and Nilpa Jha and Ting Liu and Jerome Connor and Phil Wallis and Philip Pham and Eric Bailey and Shixin Li and Heng-Tze Cheng and Sally Ma and Haiqiong Li and Akanksha Maurya and Kate Olszewska and Manfred Warmuth and Christy Koh and Dominik Paulus and Siddhartha Reddy Jonnalagadda and Enrique Piqueras and Ali Elqursh and Geoff Brown and Hadar Shemtov and Loren Maggiore and Fei Xia and Ryan Foley and Beka Westberg and George van den Driessche and Livio Baldini Soares and Arjun Kar and Michael Quinn and Siqi Zuo and Jialin Wu and Kyle Kastner and Anna Bortsova and Aijun Bai and Ales Mikhalap and Luowei Zhou and Jennifer Brennan and Vinay Ramasesh and Honglei Zhuang and John Maggs and Johan Schalkwyk and Yuntao Xu and Hui Huang and Andrew Howard and Sasha Brown and Linting Xue and Gloria Shen and Brian Albert and Neha Jha and Daniel Zheng and Varvara Krayvanova and Spurthi Amba Hombaiah and Olivier Lacombe and Gautam Vasudevan and Dan Graur and Tian Xie and Meet Gandhi and Bangju Wang and Dustin Zelle and Harman Singh and Dahun Kim and Sébastien Cevey and Victor Ungureanu and Natasha Noy and Fei Liu and Annie Xie and Fangxiaoyu Feng and Katerina Tsihlas and Daniel Formoso and Neera Vats and Quentin Wellens and Yinan Wang and Niket Kumar Bhumihar and Samrat Ghosh and Matt Hoffman and Tom Lieber and Oran Lang and Kush Bhatia and Tom Paine and Aroonalok Pyne and Ronny Votel and Madeleine Clare Elish and Benoit Schillings and Alex Panagopoulos and Haichuan Yang and Adam Raveret and Zohar Yahav and Shuang Liu and Warren Chen and Dalia El Badawy and Nishant Agrawal and Mohammed Badawi and Mahdi Mirzazadeh and Carla Bromberg and Fan Ye and Chang Liu and Tatiana Sholokhova and George-Cristian Muraru and Gargi Balasubramaniam and Jonathan Malmaud and Alen Carin and Danilo Martins and Irina Jurenka and Pankil Botadra and Dave Lacey and Richa Singh and Mariano Schain and Dan Zheng and Isabelle Guyon and Victor Lavrenko and Seungji Lee and Xiang Zhou and Demis Hassabis and Jeshwanth Challagundla and Derek Cheng and Nikhil Mehta and Matthew Mauger and Michela Paganini and Pushkar Mishra and Kate Lee and Zhang Li and Lexi Baugher and Ondrej Skopek and Max Chang and Amir Zait and Gaurav Menghani and Lizzetth Bellot and Guangxing Han and Jean-Michel Sarr and Sharat Chikkerur and Himanshu Sahni and Rohan Anil and Arun Narayanan and Chandu Thekkath and Daniele Pighin and Hana Strejček and Marko Velic and Fred Bertsch and Manuel Tragut and Keran Rong and Alicia Parrish and Kai Bailey and Jiho Park and Isabela Albuquerque and Abhishek Bapna and Rajesh Venkataraman and Alec Kosik and Johannes Griesser and Zhiwei Deng and Alek Andreev and Qingyun Dou and Kevin Hui and Fanny Wei and Xiaobin Yu and Lei Shu and Avia Aharon and David Barker and Badih Ghazi and Sebastian Flennerhag and Chris Breaux and Yuchuan Liu and Matthew Bilotti and Josh Woodward and Uri Alon and Stephanie Winkler and Tzu-Kuo Huang and Kostas Andriopoulos and João Gabriel Oliveira and Penporn Koanantakool and Berkin Akin and Michael Wunder and Cicero Nogueira dos Santos and Mohammad Hossein Bateni and Lin Yang and Dan Horgan and Beer Changpinyo and Keyvan Amiri and Min Ma and Dayeong Lee and Lihao Liang and Anirudh Baddepudi and Tejasi Latkar and Raia Hadsell and Jun Xu and Hairong Mu and Michael Han and Aedan Pope and Snchit Grover and Frank Kim and Ankit Bhagatwala and Guan Sun and Yamini Bansal and Amir Globerson and Alireza Nazari and Samira Daruki and Hagen Soltau and Jane Labanowski and Laurent El Shafey and Matt Harvey and Yanif Ahmad and Elan Rosenfeld and William Kong and Etienne Pot and Yi-Xuan Tan and Aurora Wei and Victoria Langston and Marcel Prasetya and Petar Veličković and Richard Killam and Robin Strudel and Darren Ni and Zhenhai Zhu and Aaron Archer and Kavya Kopparapu and Lynn Nguyen and Emilio Parisotto and Hussain Masoom and Sravanti Addepalli and Jordan Grimstad and Hexiang Hu and Joss Moore and Avinatan Hassidim and Le Hou and Mukund Raghavachari and Jared Lichtarge and Adam R. Brown and Hilal Dib and Natalia Ponomareva and Justin Fu and Yujing Zhang and Altaf Rahman and Joana Iljazi and Edouard Leurent and Gabriel Dulac-Arnold and Cosmo Du and Chulayuth Asawaroengchai and Larry Jin and Ela Gruzewska and Ziwei Ji and Benigno Uria and Daniel De Freitas and Paul Barham and Lauren Beltrone and Víctor Campos and Jun Yan and Neel Kovelamudi and Arthur Nguyen and Elinor Davies and Zhichun Wu and Zoltan Egyed and Kristina Toutanova and Nithya Attaluri and Hongliang Fei and Peter Stys and Siddhartha Brahma and Martin Izzard and Siva Velusamy and Scott Lundberg and Vincent Zhuang and Kevin Sequeira and Adam Santoro and Ehsan Amid and Ophir Aharoni and Shuai Ye and Mukund Sundararajan and Lijun Yu and Yu-Cheng Ling and Stephen Spencer and Hugo Song and Josip Djolonga and Christo Kirov and Sonal Gupta and Alessandro Bissacco and Clemens Meyer and Mukul Bhutani and Andrew Dai and Weiyi Wang and Siqi Liu and Ashwin Sreevatsa and Qijun Tan and Maria Wang and Lucy Kim and Yicheng Wang and Alex Irpan and Yang Xiao and Stanislav Fort and Yifan He and Alex Gurney and Bryan Gale and Yue Ma and Monica Roy and Viorica Patraucean and Taylan Bilal and Golnaz Ghiasi and Anahita Hosseini and Melvin Johnson and Zhuowan Li and Yi Tay and Benjamin Beyret and Katie Millican and Josef Broder and Mayank Lunayach and Danny Swisher and Eugen Vušak and David Parkinson and MH Tessler and Adi Mayrav Gilady and Richard Song and Allan Dafoe and Yves Raimond and Masa Yamaguchi and Itay Karo and Elizabeth Nielsen and Kevin Kilgour and Mike Dusenberry and Rajiv Mathews and Jiho Choi and Siyuan Qiao and Harsh Mehta and Sahitya Potluri and Chris Knutsen and Jialu Liu and Tat Tan and Kuntal Sengupta and Keerthana Gopalakrishnan and Abodunrinwa Toki and Mencher Chiang and Mike Burrows and Grace Vesom and Zafarali Ahmed and Ilia Labzovsky and Siddharth Vashishtha and Preeti Singh and Ankur Sharma and Ada Ma and Jinyu Xie and Pranav Talluri and Hannah Forbes-Pollard and Aarush Selvan and Joel Wee and Loic Matthey and Tom Funkhouser and Parthasarathy Gopavarapu and Lev Proleev and Cheng Li and Matt Thomas and Kashyap Kolipaka and Zhipeng Jia and Ashwin Kakarla and Srinivas Sunkara and Joan Puigcerver and Suraj Satishkumar Sheth and Emily Graves and Chen Wang and Sadh MNM Khan and Kai Kang and Shyamal Buch and Fred Zhang and Omkar Savant and David Soergel and Kevin Lee and Linda Friso and Xuanyi Dong and Rahul Arya and Shreyas Chandrakaladharan and Connor Schenck and Greg Billock and Tejas Iyer and Anton Bakalov and Leslie Baker and Alex Ruiz and Angad Chandorkar and Trieu Trinh and Matt Miecnikowski and Yanqi Zhou and Yangsibo Huang and Jiazhong Nie and Ali Shah and Ashish Thapliyal and Sam Haves and Lun Wang and Uri Shaham and Patrick Morris-Suzuki and Soroush Radpour and Leonard Berrada and Thomas Strohmann and Chaochao Yan and Jingwei Shen and Sonam Goenka and Tris Warkentin and Petar Dević and Dan Belov and Albert Webson and Madhavi Yenugula and Puranjay Datta and Jerry Chang and Nimesh Ghelani and Aviral Kumar and Vincent Perot and Jessica Lo and Yang Song and Herman Schmit and Jianmin Chen and Vasilisa Bashlovkina and Xiaoyue Pan and Diana Mincu and Paul Roit and Isabel Edkins and Andy Davis and Yujia Li and Ben Horn and Xinjian Li and Pradeep Kumar S and Eric Doi and Wanzheng Zhu and Sri Gayatri Sundara Padmanabhan and Siddharth Verma and Jasmine Liu and Heng Chen and Mihajlo Velimirović and Malcolm Reynolds and Priyanka Agrawal and Nick Sukhanov and Abhinit Modi and Siddharth Goyal and John Palowitch and Nima Khajehnouri and Wing Lowe and David Klinghoffer and Sharon Silver and Vinh Tran and Candice Schumann and Francesco Piccinno and Xi Liu and Mario Lučić and Xiaochen Yang and Sandeep Kumar and Ajay Kannan and Ragha Kotikalapudi and Mudit Bansal and Fabian Fuchs and Javad Hosseini and Abdelrahman Abdelhamed and Dawn Bloxwich and Tianhe Yu and Ruoxin Sang and Gregory Thornton and Karan Gill and Yuchi Liu and Virat Shejwalkar and Jason Lin and Zhipeng Yan and Kehang Han and Thomas Buschmann and Michael Pliskin and Zhi Xing and Susheel Tatineni and Junlin Zhang and Sissie Hsiao and Gavin Buttimore and Marcus Wu and Zefei Li and Geza Kovacs and Legg Yeung and Tao Huang and Aaron Cohen and Bethanie Brownfield and Averi Nowak and Mikel Rodriguez and Tianze Shi and Hado van Hasselt and Kevin Cen and Deepanway Ghoshal and Kushal Majmundar and Weiren Yu and Warren and Chen and Danila Sinopalnikov and Hao Zhang and Vlado Galić and Di Lu and Zeyu Zheng and Maggie Song and Gary Wang and Gui Citovsky and Swapnil Gawde and Isaac Galatzer-Levy and David Silver and Ivana Balazevic and Dipanjan Das and Kingshuk Majumder and Yale Cong and Praneet Dutta and Dustin Tran and Hui Wan and Junwei Yuan and Daniel Eppens and Alanna Walton and Been Kim and Harry Ragan and James Cobon-Kerr and Lu Liu and Weijun Wang and Bryce Petrini and Jack Rae and Rakesh Shivanna and Yan Xiong and Chace Lee and Pauline Coquinot and Yiming Gu and Lisa Patel and Blake Hechtman and Aviel Boag and Orion Jankowski and Alex Wertheim and Alex Lee and Paul Covington and Hila Noga and Sam Sobell and Shanthal Vasanth and William Bono and Chirag Nagpal and Wei Fan and Xavier Garcia and Kedar Soparkar and Aybuke Turker and Nathan Howard and Sachit Menon and Yuankai Chen and Vikas Verma and Vladimir Pchelin and Harish Rajamani and Valentin Dalibard and Ana Ramalho and Yang Guo and Kartikeya Badola and Seojin Bang and Nathalie Rauschmayr and Julia Proskurnia and Sudeep Dasari and Xinyun Chen and Mikhail Sushkov and Anja Hauth and Pauline Sho and Abhinav Singh and Bilva Chandra and Allie Culp and Max Dylla and Olivier Bachem and James Besley and Heri Zhao and Timothy Lillicrap and Wei Wei and Wael Al Jishi and Ning Niu and Alban Rrustemi and Raphaël Lopez Kaufman and Ryan Poplin and Jewel Zhao and Minh Truong and Shikhar Bharadwaj and Ester Hlavnova and Eli Stickgold and Cordelia Schmid and Georgi Stephanov and Zhaoqi Leng and Frederick Liu and Léonard Hussenot and Shenil Dodhia and Juliana Vicente Franco and Lesley Katzen and Abhanshu Sharma and Sarah Cogan and Zuguang Yang and Aniket Ray and Sergi Caelles and Shen Yan and Ravin Kumar and Daniel Gillick and Renee Wong and Joshua Ainslie and Jonathan Hoech and Séb Arnold and Dan Abolafia and Anca Dragan and Ben Hora and Grace Hu and Alexey Guseynov and Yang Lu and Chas Leichner and Jinmeng Rao and Abhimanyu Goyal and Nagabhushan Baddi and Daniel Hernandez Diaz and Tim McConnell and Max Bain and Jake Abernethy and Qiqi Yan and Rylan Schaeffer and Paul Vicol and Will Thompson and Montse Gonzalez Arenas and Mathias Bellaiche and Pablo Barrio and Stefan Zinke and Riccardo Patana and Pulkit Mehta and JK Kearns and Avraham Ruderman and Scott Pollom and David D'Ambrosio and Cath Hope and Yang Yu and Andrea Gesmundo and Kuang-Huei Lee and Aviv Rosenberg and Yiqian Zhou and Yaoyiran Li and Drew Garmon and Yonghui Wu and Safeen Huda and Gil Fidel and Martin Baeuml and Jian Li and Phoebe Kirk and Rhys May and Tao Tu and Sara Mc Carthy and Toshiyuki Fukuzawa and Miranda Aperghis and Chih-Kuan Yeh and Toshihiro Yoshino and Bo Li and Austin Myers and Kaisheng Yao and Ben Limonchik and Changwan Ryu and Rohun Saxena and Alex Goldin and Ruizhe Zhao and Rocky Rhodes and Tao Zhu and Divya Tyam and Heidi Howard and Nathan Byrd and Hongxu Ma and Yan Wu and Ryan Mullins and Qingze Wang and Aida Amini and Sebastien Baur and Yiran Mao and Subhashini Venugopalan and Will Song and Wen Ding and Paul Collins and Sashank Reddi and Megan Shum and Andrei Rusu and Luisa Zintgraf and Kelvin Chan and Sheela Goenka and Mathieu Blondel and Michael Collins and Renke Pan and Marissa Giustina and Nikolai Chinaev and Christian Schuler and Ce Zheng and Jonas Valfridsson and Alyssa Loo and Alex Yakubovich and Jamie Smith and Tao Jiang and Rich Munoz and Gabriel Barcik and Rishabh Bansal and Mingyao Yang and Yilun Du and Pablo Duque and Mary Phuong and Alexandra Belias and Kunal Lad and Zeyu Liu and Tal Schuster and Karthik Duddu and Jieru Hu and Paige Kunkle and Matthew Watson and Jackson Tolins and Josh Smith and Denis Teplyashin and Garrett Bingham and Marvin Ritter and Marco Andreetto and Divya Pitta and Mohak Patel and Shashank Viswanadha and Trevor Strohman and Catalin Ionescu and Jincheng Luo and Yogesh Kalley and Jeremy Wiesner and Dan Deutsch and Derek Lockhart and Peter Choy and Rumen Dangovski and Chawin Sitawarin and Cat Graves and Tanya Lando and Joost van Amersfoort and Ndidi Elue and Zhouyuan Huo and Pooya Moradi and Jean Tarbouriech and Henryk Michalewski and Wenting Ye and Eunyoung Kim and Alex Druinsky and Florent Altché and Xinyi Chen and Artur Dwornik and Da-Cheng Juan and Rivka Moroshko and Horia Toma and Jarrod Kahn and Hai Qian and Maximilian Sieb and Irene Cai and Roman Goldenberg and Praneeth Netrapalli and Sindhu Raghuram and Yuan Gong and Lijie Fan and Evan Palmer and Yossi Matias and Valentin Gabeur and Shreya Pathak and Tom Ouyang and Don Metzler and Geoff Bacon and Srinivasan Venkatachary and Sridhar Thiagarajan and Alex Cullum and Eran Ofek and Vytenis Sakenas and Mohamed Hammad and Cesar Magalhaes and Mayank Daswani and Oscar Chang and Ashok Popat and Ruichao Li and Komal Jalan and Yanhan Hou and Josh Lipschultz and Antoine He and Wenhao Jia and Pier Giuseppe Sessa and Prateek Kolhar and William Wong and Sumeet Singh and Lukas Haas and Jay Whang and Hanna Klimczak-Plucińska and Georges Rotival and Grace Chung and Yiqing Hua and Anfal Siddiqui and Nicolas Serrano and Dongkai Chen and Billy Porter and Libin Bai and Keshav Shivam and Sho Arora and Partha Talukdar and Tom Cobley and Sangnie Bhardwaj and Evgeny Gladchenko and Simon Green and Kelvin Guu and Felix Fischer and Xiao Wu and Eric Wang and Achintya Singhal and Tatiana Matejovicova and James Martens and Hongji Li and Roma Patel and Elizabeth Kemp and Jiaqi Pan and Lily Wang and Blake JianHang Chen and Jean-Baptiste Alayrac and Navneet Potti and Erika Gemzer and Eugene Ie and Kay McKinney and Takaaki Saeki and Edward Chou and Pascal Lamblin and SQ Mah and Zach Fisher and Martin Chadwick and Jon Stritar and Obaid Sarvana and Andrew Hogue and Artem Shtefan and Hadi Hashemi and Yang Xu and Jindong Gu and Sharad Vikram and Chung-Ching Chang and Sabela Ramos and Logan Kilpatrick and Weijuan Xi and Jenny Brennan and Yinghao Sun and Abhishek Jindal and Ionel Gog and Dawn Chen and Felix Wu and Jason Lee and Sudhindra Kopalle and Srinadh Bhojanapalli and Oriol Vinyals and Natan Potikha and Burcu Karagol Ayan and Yuan Yuan and Michael Riley and Piotr Stanczyk and Sergey Kishchenko and Bing Wang and Dan Garrette and Antoine Yang and Vlad Feinberg and CJ Carey and Javad Azizi and Viral Shah and Erica Moreira and Chongyang Shi and Josh Feldman and Elizabeth Salesky and Thomas Lampe and Aneesh Pappu and Duhyeon Kim and Jonas Adler and Avi Caciularu and Brian Walker and Yunhan Xu and Yochai Blau and Dylan Scandinaro and Terry Huang and Sam El-Husseini and Abhishek Sinha and Lijie Ren and Taylor Tobin and Patrik Sundberg and Tim Sohn and Vikas Yadav and Mimi Ly and Emily Xue and Jing Xiong and Afzal Shama Soudagar and Sneha Mondal and Nikhil Khadke and Qingchun Ren and Ben Vargas and Stan Bileschi and Sarah Chakera and Cindy Wang and Boyu Wang and Yoni Halpern and Joe Jiang and Vikas Sindhwani and Petre Petrov and Pranavaraj Ponnuramu and Sanket Vaibhav Mehta and Yu Watanabe and Betty Chan and Matheus Wisniewski and Trang Pham and Jingwei Zhang and Conglong Li and Dario de Cesare and Art Khurshudov and Alex Vasiloff and Melissa Tan and Zoe Ashwood and Bobak Shahriari and Maryam Majzoubi and Garrett Tanzer and Olga Kozlova and Robin Alazard and James Lee-Thorp and Nguyet Minh Phu and Isaac Tian and Junwhan Ahn and Andy Crawford and Lauren Lax and Yuan and Shangguan and Iftekhar Naim and David Ross and Oleksandr Ferludin and Tongfei Guo and Andrea Banino and Hubert Soyer and Xiaoen Ju and Dominika Rogozińska and Ishaan Malhi and Marcella Valentine and Daniel Balle and Apoorv Kulshreshtha and Maciej Kula and Yiwen Song and Sophia Austin and John Schultz and Roy Hirsch and Arthur Douillard and Apoorv Reddy and Michael Fink and Summer Yue and Khyatti Gupta and Adam Zhang and Norman Rink and Daniel McDuff and Lei Meng and András György and Yasaman Razeghi and Ricky Liang and Kazuki Osawa and Aviel Atias and Matan Eyal and Tyrone Hill and Nikolai Grigorev and Zhengdong Wang and Nitish Kulkarni and Rachel Soh and Ivan Lobov and Zachary Charles and Sid Lall and Kazuma Hashimoto and Ido Kessler and Victor Gomes and Zelda Mariet and Danny Driess and Alessandro Agostini and Canfer Akbulut and Jingcao Hu and Marissa Ikonomidis and Emily Caveness and Kartik Audhkhasi and Saurabh Agrawal and Ioana Bica and Evan Senter and Jayaram Mudigonda and Kelly Chen and Jingchen Ye and Xuanhui Wang and James Svensson and Philipp Fränken and Josh Newlan and Li Lao and Eva Schnider and Sami Alabed and Joseph Kready and Jesse Emond and Afief Halumi and Tim Zaman and Chengxi Ye and Naina Raisinghani and Vilobh Meshram and Bo Chang and Ankit Singh Rawat and Axel Stjerngren and Sergey Levi and Rui Wang and Xiangzhu Long and Mitchelle Rasquinha and Steven Hand and Aditi Mavalankar and Lauren Agubuzu and Sudeshna Roy and Junquan Chen and Jarek Wilkiewicz and Hao Zhou and Michal Jastrzebski and Qiong Hu and Agustin Dal Lago and Ramya Sree Boppana and Wei-Jen Ko and Jennifer Prendki and Yao Su and Zhi Li and Eliza Rutherford and Girish Ramchandra Rao and Ramona Comanescu and Adrià Puigdomènech and Qihang Chen and Dessie Petrova and Christine Chan and Vedrana Milutinovic and Felipe Tiengo Ferreira and Chin-Yi Cheng and Ming Zhang and Tapomay Dey and Sherry Yang and Ramesh Sampath and Quoc Le and Howard Zhou and Chu-Cheng Lin and Hoi Lam and Christine Kaeser-Chen and Kai Hui and Dean Hirsch and Tom Eccles and Basil Mustafa and Shruti Rijhwani and Morgane Rivière and Yuanzhong Xu and Junjie Wang and Xinyang Geng and Xiance Si and Arjun Khare and Cheolmin Kim and Vahab Mirrokni and Kamyu Lee and Khuslen Baatarsukh and Nathaniel Braun and Lisa Wang and Pallavi LV and Richard Tanburn and Yuvein and Zhu and Fangda Li and Setareh Ariafar and Dan Goldberg and Ken Burke and Daniil Mirylenka and Meiqi Guo and Olaf Ronneberger and Hadas Natalie Vogel and Liqun Cheng and Nishita Shetty and Johnson Jia and Thomas Jimma and Corey Fry and Ted Xiao and Martin Sundermeyer and Ryan Burnell and Yannis Assael and Mario Pinto and JD Chen and Rohit Sathyanarayana and Donghyun Cho and Jing Lu and Rishabh Agarwal and Sugato Basu and Lucas Gonzalez and Dhruv Shah and Meng Wei and Dre Mahaarachchi and Rohan Agrawal and Tero Rissa and Yani Donchev and Ramiro Leal-Cavazos and Adrian Hutter and Markus Mircea and Alon Jacovi and Faruk Ahmed and Jiageng Zhang and Shuguang Hu and Bo-Juen Chen and Jonni Kanerva and Guillaume Desjardins and Andrew Lee and Nikos Parotsidis and Asier Mujika and Tobias Weyand and Jasper Snoek and Jo Chick and Kai Chen and Paul Chang and Ethan Mahintorabi and Zi Wang and Tolly Powell and Orgad Keller and Abhirut Gupta and Claire Sha and Kanav Garg and Nicolas Heess and Ágoston Weisz and Cassidy Hardin and Bartek Wydrowski and Ben Coleman and Karina Zainullina and Pankaj Joshi and Alessandro Epasto and Terry Spitz and Binbin Xiong and Kai Zhao and Arseniy Klimovskiy and Ivy Zheng and Johan Ferret and Itay Yona and Waleed Khawaja and Jean-Baptiste Lespiau and Maxim Krikun and Siamak Shakeri and Timothee Cour and Bonnie Li and Igor Krivokon and Dan Suh and Alex Hofer and Jad Al Abdallah and Nikita Putikhin and Oscar Akerlund and Silvio Lattanzi and Anurag Kumar and Shane Settle and Himanshu Srivastava and Folawiyo Campbell-Ajala and Edouard Rosseel and Mihai Dorin Istin and Nishanth Dikkala and Anand Rao and Nick Young and Kate Lin and Dhruva Bhaswar and Yiming Wang and Jaume Sanchez Elias and Kritika Muralidharan and James Keeling and Dayou Du and Siddharth Gopal and Gregory Dibb and Charles Blundell and Manolis Delakis and Jacky Liang and Marco Tulio Ribeiro and Georgi Karadzhov and Guillermo Garrido and Ankur Bapna and Jiawei Cao and Adam Sadovsky and Pouya Tafti and Arthur Guez and Coline Devin and Yixian Di and Jinwei Xing and Chuqiao and Xu and Hanzhao Lin and Chun-Te Chu and Sameera Ponda and Wesley Helmholz and Fan Yang and Yue Gao and Sara Javanmardi and Wael Farhan and Alex Ramirez and Ricardo Figueira and Khe Chai Sim and Yuval Bahat and Ashwin Vaswani and Liangzhe Yuan and Gufeng Zhang and Leland Rechis and Hanjun Dai and Tayo Oguntebi and Alexandra Cordell and Eugénie Rives and Kaan Tekelioglu and Naveen Kumar and Bing Zhang and Aurick Zhou and Nikolay Savinov and Andrew Leach and Alex Tudor and Sanjay Ganapathy and Yanyan Zheng and Mirko Rossini and Vera Axelrod and Arnaud Autef and Yukun Zhu and Zheng Zheng and Mingda Zhang and Baochen Sun and Jie Ren and Nenad Tomasev and Nithish Kannan and Amer Sinha and Charles Chen and Louis O'Bryan and Alex Pak and Aditya Kusupati and Weel Yang and Deepak Ramachandran and Patrick Griffin and Seokhwan Kim and Philipp Neubeck and Craig Schiff and Tammo Spalink and Mingyang Ling and Arun Nair and Ga-Young Joung and Linda Deng and Avishkar Bhoopchand and Lora Aroyo and Tom Duerig and Jordan Griffith and Gabe Barth-Maron and Jake Ades and Alex Haig and Ankur Taly and Yunting Song and Paul Michel and Dave Orr and Dean Weesner and Corentin Tallec and Carrie Grimes Bostock and Paul Niemczyk and Andy Twigg and Mudit Verma and Rohith Vallu and Henry Wang and Marco Gelmi and Kiranbir Sodhia and Aleksandr Chuklin and Omer Goldman and Jasmine George and Liang Bai and Kelvin Zhang and Petar Sirkovic and Efrat Nehoran and Golan Pundak and Jiaqi Mu and Alice Chen and Alex Greve and Paulo Zacchello and David Amos and Heming Ge and Eric Noland and Colton Bishop and Jeffrey Dudek and Youhei Namiki and Elena Buchatskaya and Jing Li and Dorsa Sadigh and Masha Samsikova and Dan Malkin and Damien Vincent and Robert David and Rob Willoughby and Phoenix Meadowlark and Shawn Gao and Yan Li and Raj Apte and Amit Jhindal and Stein Xudong Lin and Alex Polozov and Zhicheng Wang and Tomas Mery and Anirudh GP and Varun Yerram and Sage Stevens and Tianqi Liu and Noah Fiedel and Charles Sutton and Matthew Johnson and Xiaodan Song and Kate Baumli and Nir Shabat and Muqthar Mohammad and Hao Liu and Marco Selvi and Yichao Zhou and Mehdi Hafezi Manshadi and Chu-ling Ko and Anthony Chen and Michael Bendersky and Jorge Gonzalez Mendez and Nisarg Kothari and Amir Zandieh and Yiling Huang and Daniel Andor and Ellie Pavlick and Idan Brusilovsky and Jitendra Harlalka and Sally Goldman and Andrew Lampinen and Guowang Li and Asahi Ushio and Somit Gupta and Lei Zhang and Chuyuan Kelly Fu and Madhavi Sewak and Timo Denk and Jed Borovik and Brendan Jou and Avital Zipori and Prateek Jain and Junwen Bai and Thang Luong and Jonathan Tompson and Alice Li and Li Liu and George Powell and Jiajun Shen and Alex Feng and Grishma Chole and Da Yu and Yinlam Chow and Tongxin Yin and Eric Malmi and Kefan Xiao and Yash Pande and Shachi Paul and Niccolò Dal Santo and Adil Dostmohamed and Sergio Guadarrama and Aaron Phillips and Thanumalayan Sankaranarayana Pillai and Gal Yona and Amin Ghafouri and Preethi Lahoti and Benjamin Lee and Dhruv Madeka and Eren Sezener and Simon Tokumine and Adrian Collister and Nicola De Cao and Richard Shin and Uday Kalra and Parker Beak and Emily Nottage and Ryo Nakashima and Ivan Jurin and Vikash Sehwag and Meenu Gaba and Junhao Zeng and Kevin R. McKee and Fernando Pereira and Tamar Yakar and Amayika Panda and Arka Dhar and Peilin Zhong and Daniel Sohn and Mark Brand and Lars Lowe Sjoesund and Viral Carpenter and Sharon Lin and Shantanu Thakoor and Marcus Wainwright and Ashwin Chaugule and Pranesh Srinivasan and Muye Zhu and Bernett Orlando and Jack Weber and Ayzaan Wahid and Gilles Baechler and Apurv Suman and Jovana Mitrović and Gabe Taubman and Honglin Yu and Helen King and Josh Dillon and Cathy Yip and Dhriti Varma and Tomas Izo and Levent Bolelli and Borja De Balle Pigem and Julia Di Trapani and Fotis Iliopoulos and Adam Paszke and Nishant Ranka and Joe Zou and Francesco Pongetti and Jed McGiffin and Alex Siegman and Rich Galt and Ross Hemsley and Goran Žužić and Victor Carbune and Tao Li and Myle Ott and Félix de Chaumont Quitry and David Vilar Torres and Yuri Chervonyi and Tomy Tsai and Prem Eruvbetine and Samuel Yang and Matthew Denton and Jake Walker and Slavica Andačić and Idan Heimlich Shtacher and Vittal Premachandran and Harshal Tushar Lehri and Cip Baetu and Damion Yates and Lampros Lamprou and Mariko Iinuma and Ioana Mihailescu and Ben Albrecht and Shachi Dave and Susie Sargsyan and Bryan Perozzi and Lucas Manning and Chiyuan Zhang and Denis Vnukov and Igor Mordatch and Raia Hadsell Wolfgang Macherey and Ryan Kappedal and Jim Stephan and Aditya Tripathi and Klaus Macherey and Jun Qian and Abhishek Bhowmick and Shekoofeh Azizi and Rémi Leblond and Shiva Mohan Reddy Garlapati and Timothy Knight and Matthew Wiethoff and Wei-Chih Hung and Anelia Angelova and Georgios Evangelopoulos and Pawel Janus and Dimitris Paparas and Matthew Rahtz and Ken Caluwaerts and Vivek Sampathkumar and Daniel Jarrett and Shadi Noghabi and Antoine Miech and Chak Yeung and Geoff Clark and Henry Prior and Fei Zheng and Jean Pouget-Abadie and Indro Bhattacharya and Kalpesh Krishna and Will Bishop and Zhe Yuan and Yunxiao Deng and Ashutosh Sathe and Kacper Krasowiak and Ciprian Chelba and Cho-Jui Hsieh and Kiran Vodrahalli and Buhuang Liu and Thomas Köppe and Amr Khalifa and Lubo Litchev and Pichi Charoenpanit and Reed Roberts and Sachin Yadav and Yasumasa Onoe and Desi Ivanov and Megha Mohabey and Vighnesh Birodkar and Nemanja Rakićević and Pierre Sermanet and Vaibhav Mehta and Krishan Subudhi and Travis Choma and Will Ng and Luheng He and Kathie Wang and Tasos Kementsietsidis and Shane Gu and Mansi Gupta and Andrew Nystrom and Mehran Kazemi and Timothy Chung and Nacho Cano and Nikhil Dhawan and Yufei Wang and Jiawei Xia and Trevor Yacovone and Eric Jia and Mingqing Chen and Simeon Ivanov and Ashrith Sheshan and Sid Dalmia and Paweł Stradomski and Pengcheng Yin and Salem Haykal and Congchao Wang and Dennis Duan and Neslihan Bulut and Greg Kochanski and Liam MacDermed and Namrata Godbole and Shitao Weng and Jingjing Chen and Rachana Fellinger and Ramin Mehran and Daniel Suo and Hisham Husain and Tong He and Kaushal Patel and Joshua Howland and Randall Parker and Kelvin Nguyen and Sharath Maddineni and Chris Rawles and Mina Khan and Shlomi Cohen-Ganor and Amol Mandhane and Xinyi Wu and Chenkai Kuang and Iulia Comşa and Ramya Ganeshan and Hanie Sedghi and Adam Bloniarz and Nuo Wang Pierse and Anton Briukhov and Petr Mitrichev and Anita Gergely and Serena Zhan and Allan Zhou and Nikita Saxena and Eva Lu and Josef Dean and Ashish Gupta and Nicolas Perez-Nieves and Renjie Wu and Cory McLean and Wei Liang and Disha Jindal and Anton Tsitsulin and Wenhao Yu and Kaiz Alarakyia and Tom Schaul and Piyush Patil and Peter Sung and Elijah Peake and Hongkun Yu and Feryal Behbahani and JD Co-Reyes and Alan Ansell and Sean Sun and Clara Barbu and Jonathan Lee and Seb Noury and James Allingham and Bilal Piot and Mohit Sharma and Christopher Yew and Ivan Korotkov and Bibo Xu and Demetra Brady and Goran Petrovic and Shibl Mourad and Claire Cui and Aditya Gupta and Parker Schuh and Saarthak Khanna and Anna Goldie and Abhinav Arora and Vadim Zubov and Amy Stuart and Mark Epstein and Yun Zhu and Jianqiao Liu and Yury Stuken and Ziyue Wang and Karolis Misiunas and Dee Guo and Ashleah Gill and Ale Hartman and Zaid Nabulsi and Aurko Roy and Aleksandra Faust and Jason Riesa and Ben Withbroe and Mengchao Wang and Marco Tagliasacchi and Andreea Marzoca and James Noraky and Serge Toropov and Malika Mehrotra and Bahram Raad and Sanja Deur and Steve Xu and Marianne Monteiro and Zhongru Wu and Yi Luan and Sam Ritter and Nick Li and Håvard Garnes and Yanzhang He and Martin Zlocha and Jifan Zhu and Matteo Hessel and Will Wu and Spandana Raj Babbula and Chizu Kawamoto and Yuanzhen Li and Mehadi Hassen and Yan Wang and Brian Wieder and James Freedman and Yin Zhang and Xinyi Bai and Tianli Yu and David Reitter and XiangHai Sheng and Mateo Wirth and Aditya Kini and Dima Damen and Mingcen Gao and Rachel Hornung and Michael Voznesensky and Brian Roark and Adhi Kuncoro and Yuxiang Zhou and Rushin Shah and Anthony Brohan and Kuangyuan Chen and James Wendt and David Rim and Paul Kishan Rubenstein and Jonathan Halcrow and Michelle Liu and Ty Geri and Yunhsuan Sung and Jane Shapiro and Shaan Bijwadia and Chris Duvarney and Christina Sorokin and Paul Natsev and Reeve Ingle and Pramod Gupta and Young Maeng and Ndaba Ndebele and Kexin Zhu and Valentin Anklin and Katherine Lee and Yuan Liu and Yaroslav Akulov and Shaleen Gupta and Guolong Su and Flavien Prost and Tianlin Liu and Vitaly Kovalev and Pol Moreno and Martin Scholz and Sam Redmond and Zongwei Zhou and Alex Castro-Ros and André Susano Pinto and Dia Kharrat and Michal Yarom and Rachel Saputro and Jannis Bulian and Ben Caine and Ji Liu and Abbas Abdolmaleki and Shariq Iqbal and Tautvydas Misiunas and Mikhail Sirotenko and Shefali Garg and Guy Bensky and Huan Gui and Xuezhi Wang and Raphael Koster and Mike Bernico and Da Huang and Romal Thoppilan and Trevor Cohn and Ben Golan and Wenlei Zhou and Andrew Rosenberg and Markus Freitag and Tynan Gangwani and Vincent Tsang and Anand Shukla and Xiaoqi Ren and Minh Giang and Chi Zou and Andre Elisseeff and Charline Le Lan and Dheeru Dua and Shuba Lall and Pranav Shyam and Frankie Garcia and Sarah Nguyen and Michael Guzman and AJ Maschinot and Marcello Maggioni and Ming-Wei Chang and Karol Gregor and Lotte Weerts and Kumaran Venkatesan and Bogdan Damoc and Leon Liu and Jan Wassenberg and Lewis Ho and Becca Roelofs and Majid Hadian and François-Xavier Aubet and Yu Liang and Sami Lachgar and Danny Karmon and Yong Cheng and Amelio Vázquez-Reina and Angie Chen and Zhuyun Dai and Andy Brock and Shubham Agrawal and Chenxi Pang and Peter Garst and Mariella Sanchez-Vargas and Ivor Rendulic and Aditya Ayyar and Andrija Ražnatović and Olivia Ma and Roopali Vij and Neha Sharma and Ashwin Balakrishna and Bingyuan Liu and Ian Mackinnon and Sorin Baltateanu and Petra Poklukar and Gabriel Ibagon and Colin Ji and Hongyang Jiao and Isaac Noble and Wojciech Stokowiec and Zhihao Li and Jeff Dean and David Lindner and Mark Omernick and Kristen Chiafullo and Mason Dimarco and Vitor Rodrigues and Vittorio Selo and Garrett Honke and Xintian and Wu and Wei He and Adam Hillier and Anhad Mohananey and Vihari Piratla and Chang Ye and Chase Malik and Sebastian Riedel and Samuel Albanie and Zi Yang and Kenny Vassigh and Maria Bauza and Sheng Li and Yiqing Tao and Nevan Wichers and Andrii Maksai and Abe Ittycheriah and Ross Mcilroy and Bryan Seybold and Noah Goodman and Romina Datta and Steven M. Hernandez and Tian Shi and Yony Kochinski and Anna Bulanova and Ken Franko and Mikita Sazanovich and Nicholas FitzGerald and Praneeth Kacham and Shubha Srinivas Raghvendra and Vincent Hellendoorn and Alexander Grushetsky and Julian Salazar and Angeliki Lazaridou and Jason Chang and Jan-Thorsten Peter and Sushant Kafle and Yann Dauphin and Abhishek Rao and Filippo Graziano and Izhak Shafran and Yuguo Liao and Tianli Ding and Geng Yan and Grace Chu and Zhao Fu and Vincent Roulet and Gabriel Rasskin and Duncan Williams and Shahar Drath and Alex Mossin and Raphael Hoffmann and Jordi Orbay and Francesco Bertolini and Hila Sheftel and Justin Chiu and Siyang Xue and Yuheng Kuang and Ferjad Naeem and Swaroop Nath and Nana Nti and Phil Culliton and Kashyap Krishnakumar and Michael Isard and Pei Sun and Ayan Chakrabarti and Nathan Clement and Regev Cohen and Arissa Wongpanich and GS Oh and Ashwin Murthy and Hao Zheng and Jessica Hamrick and Oskar Bunyan and Suhas Ganesh and Nitish Gupta and Roy Frostig and John Wieting and Yury Malkov and Pierre Marcenac and Zhixin and Lai and Xiaodan Tang and Mohammad Saleh and Fedir Zubach and Chinmay Kulkarni and Huanjie Zhou and Vicky Zayats and Nan Ding and Anshuman Tripathi and Arijit Pramanik and Patrik Zochbauer and Harish Ganapathy and Vedant Misra and Zach Behrman and Hugo Vallet and Mingyang Zhang and Mukund Sridhar and Ye Jin and Mohammad Babaeizadeh and Siim Põder and Megha Goel and Divya Jain and Tajwar Nasir and Shubham Mittal and Tim Dozat and Diego Ardila and Aliaksei Severyn and Fabio Pardo and Sammy Jerome and Siyang Qin and Louis Rouillard and Amir Yazdanbakhsh and Zizhao Zhang and Shivani Agrawal and Kaushik Shivakumar and Caden Lu and Praveen Kallakuri and Rachita Chhaparia and Kanishka Rao and Charles Kwong and Asya Fadeeva and Shitij Nigam and Yan Virin and Yuan Zhang and Balaji Venkatraman and Beliz Gunel and Marc Wilson and Huiyu Wang and Abhinav Gupta and Xiaowei Xu and Adrien Ali Taïga and Kareem Mohamed and Doug Fritz and Daniel Rodriguez and Zoubin Ghahramani and Harry Askham and Lior Belenki and James Zhao and Rahul Gupta and Krzysztof Jastrzębski and Takahiro Kosakai and Kaan Katircioglu and Jon Schneider and Rina Panigrahy and Konstantinos Bousmalis and Peter Grabowski and Prajit Ramachandran and Chaitra Hegde and Mihaela Rosca and Angelo Scorza Scarpati and Kyriakos Axiotis and Ying Xu and Zach Gleicher and Assaf Hurwitz Michaely and Mandar Sharma and Sanil Jain and Christoph Hirnschall and Tal Marian and Xuhui Jia and Kevin Mather and Kilol Gupta and Linhai Qiu and Nigamaa Nayakanti and Lucian Ionita and Steven Zheng and Lucia Loher and Kurt Shuster and Igor Petrovski and Roshan Sharma and Rahma Chaabouni and Angel Yeh and James An and Arushi Gupta and Steven Schwarcz and Seher Ellis and Sam Conway-Rahman and Javier Snaider and Alex Zhai and James Atwood and Daniel Golovin and Liqian Peng and Te I and Vivian Xia and Salvatore Scellato and Mahan Malihi and Arthur Bražinskas and Vlad-Doru Ion and Younghoon Jun and James Swirhun and Soroosh Mariooryad and Jiao Sun and Steve Chien and Rey Coaguila and Ariel Brand and Yi Gao and Tom Kwiatkowski and Roee Aharoni and Cheng-Chun Lee and Mislav Žanić and Yichi Zhang and Dan Ethier and Vitaly Nikolaev and Pranav Nair and Yoav Ben Shalom and Hen Fitoussi and Jai Gupta and Hongbin Liu and Dee Cattle and Tolga Bolukbasi and Ben Murdoch and Fantine Huot and Yin Li and Chris Hahn},
      year={2025},
      eprint={2507.06261},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.06261}, 
}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract15">abstract</button>
	<div id="abstract15" class="collapse">
        <p style="text-align: justify;">
	In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. 
	In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. 
	Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. 
	Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. 
	Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.
	</p>
        </div>
	  <a href="https://developers.googleblog.com/en/gemini-2-5-video-understanding/"><button type="button" class="btn btn-danger btn-xs">video blog</button></a>
	  <a href="https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024"><button type="button" class="btn btn-danger btn-xs">blog (Dec)</button></a>
          <a href="https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/#gemini-2-5-thinking"><button type="button" class="btn btn-danger btn-xs">blog (Mar)</button></a>
          <a href="https://developers.googleblog.com/en/gemini-2-5-thinking-model-updates/"><button type="button" class="btn btn-danger btn-xs">blog (June)</button></a>
	  <a href="https://deepmind.google/technologies/project-astra/"><button type="button" class="btn btn-danger btn-xs">Project Astra</button></a>
	  <a href="https://aistudio.google.com"><button type="button" class="btn btn-primary btn-xs">AI Studio</button></a>
          <a href="https://aistudio.google.com/apps/bundled/video-to-learning-app?showPreview=true"><button type="button" class="btn btn-primary btn-xs">Video to learning app</button></a>
	  <a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb"><button type="button" class="btn btn-primary btn-xs">colab</button></a>
      </div>
    </div>
<!-- end of Gemini 2.5 -->
		  
<!-- Chapter-Llama -->
    <div class="row" style="margin-top: 50px;">
	<div class="col-xs-10 col-sm-4 col-md-4" style='height:120px'>
		<a class="thumbnail">
		<img src="https://imagine.enpc.fr/~lucas.ventura/chapter-llama/images/teaser.png" height="100%" alt="Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs</strong> </br>
	Lucas Ventura, <u>Antoine Yang</u>, Cordelia Schmid, Gül Varol<br>
          CVPR 2025 <br>
          <a href="https://arxiv.org/pdf/2504.00072"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	<a href="https://imagine.enpc.fr/~lucas.ventura/chapter-llama/"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex14">bibtex</button>
	<div id="bibtex14" class="collapse">
	  <pre><tt>@inproceedings{ventura2025chapterllama,
title={Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs},
author={Lucas Ventura and Antoine Yang and Cordelia Schmid and G{\"u}l Varol},
booktitle={CVPR},
year={2025}}
</tt></pre></tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract14">abstract</button>
	<div id="abstract14" class="collapse">
        <p style="text-align: justify;">
	        We address the task of video chaptering, i.e., partitioning a long video timeline into semantic units and generating corresponding chapter titles. 
		While relatively underexplored, automatic chaptering has the potential to enable efficient navigation and content retrieval in long-form videos. 
		In this paper, we achieve strong chaptering performance on hour-long videos by efficiently addressing the problem in the text domain with our 'Chapter-Llama' framework. 
		Specifically, we leverage a pretrained large language model (LLM) with large context window, and feed as input (i) speech transcripts and (ii) captions describing video frames, along with their respective timestamps. 
		Given the inefficiency of exhaustively captioning all frames, we propose a lightweight speech-guided frame selection strategy based on speech transcript content, and experimentally demonstrate remarkable advantages. 
		We train the LLM to output timestamps for the chapter boundaries, as well as free-form chapter titles. This simple yet powerful approach scales to processing one-hour long videos in a single forward pass. 
		Our results demonstrate substantial improvements (e.g., 45.3 vs 26.7 F1 score) over the state of the art on the recent VidChapters-7M benchmark. 
		To promote further research, we release our code and models.
            </p>
        </div>
          <a href="https://github.com/lucas-ventura/chapter-llama"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          <a class="github-button" href="https://github.com/lucas-ventura/chapter-llama" data-icon="octicon-star" data-show-count="true" aria-label="Star google-research/scenic on GitHub">Star</a>
      </div>
    </div>
<!-- end of Chapter-Llama -->

<!-- Gemma 3 -->
    <div class="row" style="margin-top: 100px;">
	<div class="col-xs-10 col-sm-4 col-md-4" style='height:120px'>
		<a class="thumbnail">
		<img src="https://storage.googleapis.com/gweb-uniblog-publish-prod/images/Gemma3_KeywordBlog_RD3_V01b.width-2200.format-webp.webp" height="100%" alt="Gemma 3">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>Gemma 3 Technical Report</strong> </br>
	Gemma Team, Google<br>
          Google Blog 2025 <br>
          <a href="https://arxiv.org/pdf/2503.19786"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
    <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex13">bibtex</button>
    <div id="bibtex13" class="collapse">
	<pre><tt>@article{team2025gemma,
  title={Gemma 3 technical report},
  author={Team, Gemma and Kamath, Aishwarya and Ferret, Johan and Pathak, Shreya and Vieillard, Nino and Merhej, Ramona and Perrin, Sarah and Matejovicova, Tatiana and Ram{\'e}, Alexandre and Rivi{\`e}re, Morgane and others},
  journal={arXiv preprint arXiv:2503.19786},
  year={2025}
}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract13">abstract</button>
	<div id="abstract13" class="collapse">
        <p style="text-align: justify;">
	We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. 
	This version introduces vision understanding abilities, a wider coverage of languages and longer context - at least 128K tokens. 
	We also change the architecture of the model to reduce the KV-cache memory that tends to explode with long context. 
	This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short. 
	The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. 
	In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. 
	We release all our models to the community.
	</p>
        </div>
          <a href="https://blog.google/technology/developers/gemma-3/"><button type="button" class="btn btn-danger btn-xs">blog</button></a>
          <a href="https://huggingface.co/google/gemma-3-27b-it"><button type="button" class="btn btn-primary btn-xs">Hugging Face</button></a>
      </div>
    </div>
<!-- end of Gemma 3 -->  
		  
<!-- Gemini 1.5 -->
    <div class="row" style="margin-top: 75px;">
	<div class="col-xs-10 col-sm-4 col-md-4" style='height:120px'>
		<a class="thumbnail">
		<img src="./img/gemini_1p5.png" height="100%" alt="Gemini 1.5">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</strong> </br>
	Gemini Team, Google<br>
          Google Blog 2024 <br>
          <a href="https://arxiv.org/pdf/2403.05530"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
    <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex11">bibtex</button>
    <div id="bibtex11" class="collapse">
	<pre><tt>@article{reid2024gemini,
title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
journal={arXiv preprint arXiv:2403.05530},
year={2024}
}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract11">abstract</button>
	<div id="abstract11" class="collapse">
        <p style="text-align: justify;">
	    In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly  compute-efficient multimodal models capable of recalling and reasoning over fine-grained information  from millions of tokens of context, including multiple long documents and hours of video and audio.
        The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality.
        Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra’s state-of-the-art performance across a broad set of benchmarks.
        Studying the limits of Gemini 1.5’s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k).
        Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professions on their completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.
</p>
        </div>
          <a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/"><button type="button" class="btn btn-danger btn-xs">blog (Feb)</button></a>
          <a href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/#gemini-model-updates"><button type="button" class="btn btn-danger btn-xs">blog (May)</button></a>
	  <a href="https://developers.googleblog.com/en/updated-production-ready-gemini-models-reduced-15-pro-pricing-increased-rate-limits-and-more/"><button type="button" class="btn btn-danger btn-xs">blog (Sep)</button></a>
          <a href="https://aistudio.google.com"><button type="button" class="btn btn-primary btn-xs">AI Studio</button></a>
          <a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_1_5_pro.ipynb"><button type="button" class="btn btn-primary btn-xs">colab</button></a>
          <a href="https://github.com/trippedout/gemini-video-scrubber"><button type="button" class="btn btn-primary btn-xs">Gemini Video Scrubber</button></a>
      </div>
    </div>
<!-- end of Gemini 1.5 -->

<!-- PhD Thesis -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4" style='height:120px'>
		<a class="thumbnail">
		<img src="./img/thesis-header.png" height="100%" alt="Learning Visual Language Models for Video Understanding">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>Learning Visual Language Models for Video Understanding</strong> </br>
	<u>Antoine Yang</u><br>
          PhD Thesis 2023 <br>
          <a href="https://hal.science/tel-04307117"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex10">bibtex</button>
	<div id="bibtex10" class="collapse">
	  <pre><tt>@phdthesis{yang:tel-04307117,
  TITLE = {{Learning Visual Language Models for Video Understanding}},
  AUTHOR = {Yang, Antoine},
  URL = {https://hal.science/tel-04307117},
  SCHOOL = {{Ecole Normale Superieure de Paris - ENS Paris}},
  YEAR = {2023},
  MONTH = Nov,
  KEYWORDS = {Machine learning ; Computer vision ; Artificial intelligence ; Natural language processing ; Video understanding ; Deep learning ; Apprentissage automatique ; Vision par ordinateur ; Intelligence artificielle ; Traitement du langage naturel ; Compr{\'e}hension de vid{\'e}os ; Apprentissage profond},
  TYPE = {Theses},
  PDF = {https://hal.science/tel-04307117v2/file/PhD.pdf},
  HAL_ID = {tel-04307117},
  HAL_VERSION = {v2},
}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract10">abstract</button>
	<div id="abstract10" class="collapse">
        <p style="text-align: justify;">
	        The goal of this thesis is to build and train machine learning models that combine the power of natural language processing with visual understanding, enabling a comprehensive and detailed comprehension of the content within videos. 
		First, we propose two scalable approaches to develop video question answering models without the need for costly manual annotation. 
		We automatically generate video question answering data from narrated videos using text-only question-generation models. 
		We then show that a multi-modal transformer trained contrastively on the generated data can answer visual questions in a zero-shot manner. 
		In order to bypass the data generation procedure, we present an alternative approach, dubbed FrozenBiLM, that directly leverages bidirectional masked language models. 
		Second, we develop TubeDETR, a transformer model that can spatially and temporally localize a natural language query in an untrimmed video. 
		Unlike prior spatio-temporal grounding approaches, TubeDETR can be effectively trained end-to-end on untrimmed videos. 
		Third, we present a new model and a new dataset for multi-event understanding in untrimmed videos. 
		We introduce the Vid2Seq model which generates dense natural language descriptions and corresponding temporal boundaries for all events in an untrimmed video by predicting a single sequence of tokens. 
		Moreover, Vid2Seq can be effectively pretrained on narrated videos at scale using transcribed speech as pseudo-supervision. 
		Finally, we introduce VidChapters-7M, a large-scale dataset of user-chaptered videos. Based on this dataset, we evaluate state-of-the-art models on three tasks including video chapter generation. 
		We also show that video chapter generation models transfer well to dense video captioning in both zero-shot and finetuning settings.
            </p>
        </div>
      </div>
    </div>
<!-- end of PhD thesis -->
		  
<!-- VidChapters-7M, NeurIPS 2023 D&B -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4" style='height:120px'>
		<a class="thumbnail">
		<img src="./img/vidchapters-header.png" height="100%" alt="VidChapters-7M: Video Chapters at Scale">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>VidChapters-7M: Video Chapters at Scale</strong> </br>
	<u>Antoine Yang</u>, Arsha Nagrani, Ivan Laptev, Josef Sivic, Cordelia Schmid<br>
          NeurIPS 2023 Track on Datasets and Benchmarks <br>
          <a href="https://arxiv.org/pdf/2309.13952.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	<a href="vidchapters.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex9">bibtex</button>
	<div id="bibtex9" class="collapse">
	  <pre><tt>@inproceedings{yang2023vidchapters,
title={VidChapters-7M: Video Chapters at Scale},
author={Antoine Yang and Arsha Nagrani and Ivan Laptev and Josef Sivic and Cordelia Schmid},
booktitle={NeurIPS},
year = {2023}}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract9">abstract</button>
	<div id="abstract9" class="collapse">
        <p style="text-align: justify;">
	        Segmenting long videos into chapters enables users to quickly navigate to the information of their interest.
            This important topic has been understudied due to the lack of publicly released datasets.
            To address this issue, we present VidChapters-7M, a dataset of 817K user-chaptered videos including 7M chapters in total.
            VidChapters-7M is automatically created from videos online in a scalable manner by scraping user-annotated chapters and hence without any additional manual annotation.
            We introduce the following three tasks based on this data.
            First, the video chapter generation task consists of temporally segmenting the video and generating a chapter title for each segment.
            To further dissect the problem, we also define two variants of this task: video chapter generation given ground-truth boundaries, which requires generating a chapter title given an annotated video segment, and video chapter grounding, which requires temporally localizing a chapter given its annotated title.
            We benchmark both simple baselines and state-of-the-art video-language models for these three tasks.
            We also show that pretraining on VidChapters-7M transfers well to dense video captioning tasks in both zero-shot and finetuning settings, largely improving the state of the art on the YouCook2 and ViTT benchmarks.
            Finally, our experiments reveal that downstream performance scales well with the size of the pretraining dataset.
            </p>
        </div>
          <a href="https://github.com/antoyang/VidChapters"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          <a class="github-button" href="https://github.com/antoyang/VidChapters" data-icon="octicon-star" data-show-count="true" aria-label="Star google-research/scenic on GitHub">Star</a>
      </div>
    </div>
<!-- end of VidChapters -->

<!-- CoVR -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4" style='height:120px'>
		<a class="thumbnail">
		<img src="http://imagine.enpc.fr/~ventural/covr/images/teaser.png" height="100%" alt="CoVR: Learning Composed Video Retrieval from Web Video Captions">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>CoVR: Learning Composed Video Retrieval from Web Video Captions</strong> </br>
	Lucas Ventura, <u>Antoine Yang</u>, Cordelia Schmid, Gül Varol<br>
          AAAI 2024 <br>
          Journal extension in TPAMI <br>
          <a href="https://arxiv.org/pdf/2308.14746v3"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	  <a href="https://ieeexplore.ieee.org/abstract/document/10685001"><button type="button" class="btn btn-primary btn-xs">journal extension</button></a>
	<a href="http://imagine.enpc.fr/~ventural/covr/"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex8">bibtex</button>
	<div id="bibtex8" class="collapse">
	  <pre><tt>@inproceedings{ventura2023covr,
title={CoVR: Learning Composed Video Retrieval from Web Video Captions},
author={Lucas Ventura and Antoine Yang and Cordelia Schmid and G{\"u}l Varol},
booktitle={AAAI},
year={2024}}
	  
@article{ventura2024covr2,
title={CoVR-2: Automatic Data Construction for Composed Video Retrieval},
author={Lucas Ventura and Antoine Yang and Cordelia Schmid and G{\"u}l Varol},
journal={IEEE TPAMI},
year={2024}}</tt></pre></tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract8">abstract</button>
	<div id="abstract8" class="collapse">
        <p style="text-align: justify;">
	        Composed Image Retrieval (CoIR) has recently gained popularity as a task that considers both text and image queries together, to search for relevant images in a database.
            Most CoIR approaches require manually annotated datasets, containing image-text-image triplets, where the text describes a modification from the query image to the target image.
            However, manual curation of CoIR triplets is expensive and prevents scalability.
            In this work, we instead propose a scalable automatic dataset creation methodology that generates triplets given video-caption pairs.
            To this end, we mine paired videos with a similar caption from a large database, and leverage a large language model to generate the corresponding modification text.
            We automatically construct our WebVid-CoVR dataset by applying this procedure to the large WebVid2M collection, resulting in 1.6M triplets.
            Moreover, we introduce a new benchmark for composed video retrieval (CoVR) and contribute a manually annotated evaluation set, along with baseline results.
            We further show that training a CoVR model on our dataset transfers well to CoIR, improving the state of the art in the zero-shot setup on both the CIRR and FashionIQ benchmarks.
            </p>
        </div>
          <a href="https://github.com/lucas-ventura/covr/"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          <a class="github-button" href="https://github.com/lucas-ventura/covr/" data-icon="octicon-star" data-show-count="true" aria-label="Star google-research/scenic on GitHub">Star</a>
      </div>
    </div>
<!-- end of CoVR -->

<!-- Vid2Seq, CVPR 2023 -->
    <div class="row" style="margin-top: 25px;">
	<div class="col-xs-10 col-sm-4 col-md-4">
		<a class="thumbnail">
		<img src="./img/vid2seq.png" alt="Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning</strong> </br>
	<u>Antoine Yang</u>, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, Cordelia Schmid<br>
          CVPR 2023 <br>
          <a href="https://arxiv.org/pdf/2302.14115"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	<a href="vid2seq.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex7">bibtex</button>
	<div id="bibtex7" class="collapse">
	  <pre><tt>@inproceedings{yang2023vid2seq,
title = {Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning},
author={Antoine Yang and Arsha Nagrani and Paul Hongsuck Seo and Antoine Miech and Jordi Pont-Tuset and Ivan Laptev and Josef Sivic and Cordelia Schmid},
booktitle={CVPR},
year = {2023}}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract7">abstract</button>
	<div id="abstract7" class="collapse">
        <p style="text-align: justify;">
	        In this work, we introduce Vid2Seq, a multi-modal single-stage dense event captioning model pretrained on narrated videos which are readily-available at scale.
            The Vid2Seq architecture augments a language model with special time tokens, allowing it to seamlessly predict event boundaries and textual descriptions in the same output sequence.
            Such a unified model requires large-scale training data, which is not available in current annotated datasets.
            We show that it is possible to leverage unlabeled narrated videos for dense video captioning, by reformulating sentence boundaries of transcribed speech as pseudo event boundaries, and using the transcribed speech sentences as pseudo event captions.
            The resulting Vid2Seq model pretrained on the YT-Temporal-1B dataset improves the state of the art on a variety of dense video captioning benchmarks including YouCook2, ViTT and ActivityNet Captions.
            Vid2Seq also generalizes well to the tasks of video paragraph captioning and video clip captioning, and to few-shot settings.
        </p>
        </div>
          <a href="https://ai.googleblog.com/2023/03/vid2seq-pretrained-visual-language.html"><button type="button" class="btn btn-danger btn-xs">blog</button></a>
          <a href="https://github.com/google-research/scenic/tree/main/scenic/projects/vid2seq"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          <a class="github-button" href="https://github.com/google-research/scenic" data-icon="octicon-star" data-show-count="true" aria-label="Star google-research/scenic on GitHub">Star</a>
      </div>
    </div>
<!-- end of Vid2Seq -->

<!-- FrozenBiLM, NeurIPS 2022 -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4">
		<a class="thumbnail">
		<img src="./img/frozenbilm.png" alt="Zero-Shot Video Question Answering via Frozen Bidirectional Language Models">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>Zero-Shot Video Question Answering via Frozen Bidirectional Language Models</strong> </br>
	<u>Antoine Yang</u>, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid<br>
          NeurIPS 2022 <br>
          <a href="https://arxiv.org/pdf/2206.08155.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	<a href="frozenbilm.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex6">bibtex</button>
	<div id="bibtex6" class="collapse">
	  <pre><tt>@inproceedings{yang2022frozenbilm,
title = {Zero-Shot Video Question Answering via Frozen Bidirectional Language Models},
author={Antoine Yang and Antoine Miech and Josef Sivic and Ivan Laptev and Cordelia Schmid},
booktitle={NeurIPS}
year = {2022}}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract6">abstract</button>
	<div id="abstract6" class="collapse">
        <p style="text-align: justify;">
	        Video question answering (VideoQA) is a complex task that requires diverse multi-modal data for training.
            Manual annotation of question and answers for videos, however, is tedious and prohibits scalability.
            To tackle this problem, recent methods consider zero-shot settings with no manual annotation of visual question-answer.
            In particular, a promising approach adapts frozen autoregressive language models pretrained on Web-scale text-only data to multi-modal inputs.
            In contrast, we here build on frozen bidirectional language models (BiLM) and show that such an approach provides a stronger and cheaper alternative for zero-shot VideoQA.
            In particular, (i) we combine visual inputs with the frozen BiLM using light trainable modules,
            (ii) we train such modules using Web-scraped multi-modal data, and finally
            (iii) we perform zero-shot VideoQA inference through masked language modeling, where the masked text is the answer to a given question.
            Our proposed approach, FrozenBiLM, outperforms the state of the art in zero-shot VideoQA by a significant margin on a variety of datasets, including LSMDC-FiB, iVQA, MSRVTT-QA, MSVD-QA, ActivityNet-QA, TGIF-FrameQA, How2QA and TVQA.
            It also demonstrates competitive performance in the few-shot and fully-supervised setting.
        </p>
        </div>
          <a href="https://github.com/antoyang/FrozenBiLM"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          <a class="github-button" href="https://github.com/antoyang/FrozenBiLM" data-icon="octicon-star" data-show-count="true" aria-label="Star antoyang/FrozenBiLM on GitHub">Star</a>
      </div>
    </div>
<!-- end of FrozenBiLM -->

<!-- TubeDETR, CVPR 2022 -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4">
		<a class="thumbnail">
		<img src="./img/tubedetr.png" alt="TubeDETR: Spatio-Temporal Video Grounding with Transformers">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>TubeDETR: Spatio-Temporal Video Grounding with Transformers</strong> </br>
	<u>Antoine Yang</u>, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid<br>
          CVPR 2022 (<strong><font color="red">oral</font></strong>: top 4% submissions) <br>
          <a href="https://arxiv.org/pdf/2203.16434.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	<a href="tubedetr.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex4">bibtex</button>
	<div id="bibtex4" class="collapse">
	  <pre><tt>@inproceedings{yang2022tubedetr,
author={Antoine Yang and Antoine Miech and Josef Sivic and Ivan Laptev and Cordelia Schmid},
title={TubeDETR: Spatio-Temporal Video Grounding With Transformers},
booktitle={CVPR},
year={2022}}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract4">abstract</button>
	<div id="abstract4" class="collapse">
        <p style="text-align: justify;">
	        We consider the problem of localizing a spatio-temporal tube in a video corresponding to a given text query.
            This is a challenging task that requires the joint and efficient modeling of temporal, spatial and multi-modal interactions.
            To address this task, we propose TubeDETR, a transformer-based architecture inspired by the recent success of such models for text-conditioned object detection.
            Our model notably includes: (i) an efficient video and text encoder that models spatial multi-modal interactions over sparsely sampled frames and
            (ii) a space-time decoder that jointly performs spatio-temporal localization.
            We demonstrate the advantage of our proposed components through an extensive ablation study.
            We also evaluate our full approach on the spatio-temporal video grounding task and demonstrate improvements over the state of the art on the challenging VidSTG and HC-STVG benchmarks.
	</p>
        </div>
          <a href="http://stvg.paris.inria.fr/"><button type="button" class="btn btn-danger btn-xs">demo</button></a>
          <a href="https://github.com/antoyang/TubeDETR"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          <a class="github-button" href="https://github.com/antoyang/TubeDETR" data-icon="octicon-star" data-show-count="true" aria-label="Star antoyang/TubeDETR on GitHub">Star</a>

      </div>
    </div>
<!-- end of TubeDETR -->

<!-- Just Ask, ICCV 2021 -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4">
		<a class="thumbnail">
		<img src="./img/justask.PNG" alt="Just Ask: Learning to Answer Questions from Millions of Narrated Videos">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <!-- <font color="red">[New]</font> -->
          <strong>Just Ask: Learning to Answer Questions from Millions of Narrated Videos</strong> </br>
	<u>Antoine Yang</u>, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid<br>
          ICCV 2021 (<strong><font color="red">oral</font></strong>: top 3% submissions) <br>
	  Journal extension in TPAMI Special Issue of the Best Papers of 2021 <br>
          <a href="https://arxiv.org/pdf/2012.00451.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	      <a href="https://arxiv.org/pdf/2205.05019v2"><button type="button" class="btn btn-primary btn-xs">journal extension</button></a>
	<a href="just-ask.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex3">bibtex</button>
	<div id="bibtex3" class="collapse">
	  <pre><tt>@inproceedings{yang2021justask,
title={Just Ask: Learning To Answer Questions From Millions of Narrated Videos},
author={Antoine Yang and Antoine Miech and Josef Sivic and Ivan Laptev and Cordelia Schmid},
booktitle={ICCV},
year={2021}}
		  
	  @article{yang2022learningta,
title={Learning to Answer Visual Questions from Web Videos},
author={Antoine Yang and Antoine Miech and Josef Sivic and Ivan Laptev and Cordelia Schmid},
journal={IEEE TPAMI},
year={2022}}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract3">abstract</button>
	<div id="abstract3" class="collapse">
        <p style="text-align: justify;">
	        Recent methods for visual question answering rely on large-scale annotated datasets.
            Manual annotation of questions and answers for videos, however, is tedious, expensive and prevents scalability.
            In this work, we propose to avoid manual annotation and generate a large-scale training dataset for video question answering making use of automatic cross-modal supervision.
            We leverage a question generation transformer trained on text data and use it to generate question-answer pairs from transcribed video narrations.
            Given narrated videos, we then automatically generate the HowToVQA69M dataset with 69M video-question-answer triplets.
            To handle the open vocabulary of diverse answers in this dataset, we propose a training procedure based on a contrastive loss between a video-question multi-modal transformer and an answer transformer.
            We introduce the zero-shot VideoQA task and show excellent results, in particular for rare answers.
            Furthermore, we demonstrate our method to significantly outperform the state of the art on MSRVTT-QA, MSVD-QA, ActivityNet-QA and How2QA.
            Finally, for a detailed evaluation we introduce iVQA, a new VideoQA dataset with reduced language biases and high-quality redundant manual annotations.
	</p>
        </div>
          <a href="http://videoqa.paris.inria.fr/"><button type="button" class="btn btn-danger btn-xs">demo</button></a>
          <a href="https://github.com/antoyang/just-ask"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          <a class="github-button" href="https://github.com/antoyang/just-ask" data-icon="octicon-star" data-show-count="true" aria-label="Star antoyang/just-ask on GitHub">Star</a>
      </div>
    </div>
<!-- end of Just Ask -->

<!-- NASEFH, ICLR 2020 -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="./img/nasefh.PNG" alt="NAS evaluation is frustratingly hard">
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong>NAS evaluation is frustratingly hard</strong><br>
		<u>Antoine Yang</u>, Pedro M. Esperança, Fabio Maria Carlucci<br>
                ICLR 2020<br>
                  <a href="https://arxiv.org/pdf/1912.12522.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
                  <a href="nasefh.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex2">bibtex</button>
                <div id="bibtex2" class="collapse">
                  <pre><tt>@inproceedings{yang2020nasefh,
title={NAS evaluation is frustratingly hard},
author={Antoine Yang and Pedro M. Esperança and Fabio M. Carlucci},
booktitle={ICLR},
year={2020}}</tt></pre>
                </div>
                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract2">abstract</button>
                <div id="abstract2" class="collapse">
		  <div>
              Neural Architecture Search (NAS) is an exciting new field which promises to be as much as a game-changer as Convolutional Neural Networks were in 2012.
              Despite many great works leading to substantial improvements on a variety of tasks, comparison between different methods is still very much an open issue.
              While most algorithms are tested on the same datasets, there is no shared experimental protocol followed by all.
              As such, and due to the under-use of ablation studies, there is a lack of clarity regarding why certain methods are more effective than others. Our first contribution is a benchmark of 8 NAS methods on 5 datasets.
              To overcome the hurdle of comparing methods with different search spaces, we propose using a method’s relative improvement over the randomly sampled average architecture, which effectively removes advantages arising from expertly engineered search spaces or training protocols.
              Surprisingly, we find that many NAS techniques struggle to significantly beat the average architecture baseline. We perform further experiments with the commonly used DARTS search space in order to understand the contribution of each component in the NAS pipeline.
              These experiments highlight that: (i) the use of tricks in the evaluation protocol has a predominant impact on the reported performance of architectures; (ii) the cell-based search space has a very narrow accuracy range, such that the seed has a considerable impact on architecture rankings; (iii) the hand-designed macrostructure (cells) is more important than the searched micro-structure (operations); and (iv) the depth-gap is a real phenomenon, evidenced by the change in rankings between 8 and 20 cell architectures.
              To conclude, we suggest best practices, that we hope will prove useful for the community and help mitigate current NAS pitfalls, e.g. difficulties in reproducibility and comparison of search methods.
                  </div>
                </div>
                  <a href="https://medium.com/@antoyang/is-neural-architecture-search-really-worth-it-2d0b9f28a1ed"><button type="button" class="btn btn-danger btn-xs">blog</button></a>
                  <a href="https://github.com/antoyang/NAS-Benchmark"><button type="button" class="btn btn-primary btn-xs">code</button></a>
                  <a class="github-button" href="https://github.com/antoyang/NAS-Benchmark" data-icon="octicon-star" data-show-count="true" aria-label="Star antoyang/NAS-Benchmark on GitHub">Star</a>
              </div>
            </div>
<!-- end of NASEFH -->

<!-- MANAS -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="./img/manas.PNG" style="height:200px;" alt="MANAS">
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong>MANAS: Multi-Agent Neural Architecture Search</strong><br>
		Vasco Lopes, Fabio Maria Carlucci, Pedro M. Esperança, Marco Singh, <u>Antoine Yang</u>, Victor Gabillon, Hang Xu, Zewei Chen, Jun Wang<br>
                Machine Learning 2023 <br>
                  <a href="https://arxiv.org/pdf/1909.01051.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex1">bibtex</button>
                <div id="bibtex1" class="collapse">
                  <pre><tt>@article{lopes2023manas,
  title={MANAS: Multi-Agent Neural Architecture Search},
  author={Lopes Vasco and Fabio Maria Carlucci and Pedro M Esperan{\c{c}}a and Marco Singh and Antoine Yang and Victor Gabillon and Hang Xu and Zewei Chen and Jun Wang},
  journal={Machine Learning},
  year={2023}}</tt></pre>
                </div>
                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract1">abstract</button>
                <div id="abstract1" class="collapse">
		  <div>
              The Neural Architecture Search (NAS) problem is typically formulated as a graph search problem where the goal is to learn the optimal operations over edges in order to maximise a graph-level global objective.
              Due to the large architecture parameter space, efficiency is a key bottleneck preventing NAS from its practical use.
              In this paper, we address the issue by framing NAS as a multi-agent problem where agents control a subset of the network and coordinate to reach optimal architectures.
              We provide two distinct lightweight implementations, with reduced memory requirements (1/8th of state-of-the-art), and performances above those of much more computationally expensive methods.
              Theoretically, we demonstrate vanishing regrets of the form O(sqrt(T)), with T being the total number of rounds.
              Finally, aware that random search is an, often ignored, effective baseline we perform additional experiments on 3 alternative datasets and 2 network configurations, and achieve favourable results in comparison.
                  </div>
                </div>
              </div>
            </div>
<!-- end of MANAS -->

          </div>
        </div> <!-- end of projects -->

          <hr>

      <!-- Talks -->
        <div class="row" id="talks" style="padding-top:30px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>Talks</h2>
            <ul>
		<li>10/2024 - <a href="https://gemini.google.com/corp/app">Gemini Multimodal Vision Summit, Talk (Mountain View, California) </a> - Gemini Video Understanding Deep Dive </li>
                <li>04/2024 - <a href="https://www.robots.ox.ac.uk/~vgg/">Seminar of the Visual Geometry Group at University of Oxford, Talk (Oxford, United Kingdom) </a> - Vid2Seq and VidChapters-7M projects - <a href="slides/vid2seq-vidchapters.pdf"> [Slides] </a> </li>
                <li>03/2024 - <a href="https://www.qmul.ac.uk/deri/events--seminars/">Seminar of the Digital Environment Research Institute at Queen Mary University of London, 20-min Talk (Virtual) </a> - Vid2Seq and VidChapters-7M projects - <a href="slides/vid2seq-vidchapters.pdf"> [Slides] </a> </li>
                <li>01/2024 - <a href="https://www.bmva.org/meetings/24-01-17-Vision%20and%20Language.html">BMVA Symposium on Vision and Language, Poster Session (London, United Kingdom) </a> - VidChapters-7M project - <a href="slides/vidchapters-neurips-poster.pdf"> [Poster] </a> </li>
                <li>12/2023 - <a href="https://neurips.cc/">NeurIPS 2023, Poster Session (New Orleans, Louisiana) </a> - VidChapters-7M project - <a href="https://www.youtube.com/watch?v=prn9SP8D5uE&ab_channel=AntoineYang">[5 min Video] </a> - <a href="slides/vidchapters-neurips.pdf"> [Slides] </a> - <a href="slides/vidchapters-neurips-poster.pdf"> [Poster] </a> </li>
                <li>11/2023 - <a href="https://theaitalks.org/">The AI Talks, 45-min Talk (Virtual) </a> - VLM for video understanding - <a href="https://www.youtube.com/watch?v=njUDbJz6zWI&ab_channel=TheAITalks"> [Recording] </a> </li>
		        <li>11/2023 - <a href="https://www.ens.psl.eu/">ENS PhD Defense, 45-min Presentation (Paris, France)</a> - VLM for video understanding - <a href="slides/phd-defense.pdf"> [Slides] </a> </li>
                <li>10/2023 - <a href="https://www.hceres.fr/fr">HCERES visit, Poster Session (Paris, France) </a> - VidChapters-7M project - <a href="slides/vid2seq-cvpr-poster.pdf"> [Poster] </a> </li>
                <li>10/2023 - <a href="https://iccv2023.thecvf.com//">ICCV 2023 Doctoral Consortium, 15-min Discussion (Paris, France) </a> - VidChapters-7M project - <a href="slides/vidchapters-neurips.pdf"> [Slides] </a> </li>
                <li>07/2023 - <a href="https://iplab.dmi.unict.it/icvss2023/Home">ICVSS 2023, Poster Session (Sampieri, Italy) </a> - Vid2Seq project - <a href="slides/vid2seq-cvpr-poster.pdf"> [Poster] </a> </li>
                <li>07/2023 - <a href="https://imagine-lab.enpc.fr/">Imagine ENPC Seminar (Champs-sur-Marne, France), 1-min Highlight </a> - Vid2Seq project
                <li>06/2023 - <a href="https://cvpr2023.thecvf.com/">CVPR 2023, Poster Session (Vancouver, British Columbia) </a> - Vid2Seq project - <a href="https://www.youtube.com/watch?v=hXP-2fYzq4g&ab_channel=AntoineYang">[8 min Video] </a> - <a href="slides/vid2seq-cvpr.pdf"> [Slides] </a> - <a href="slides/vid2seq-cvpr-poster.pdf"> [Poster] </a> </li>
                <li>05/2023 - <a href="https://ellis.eu/">ELLIS computer vision workshop, Spotlight and Poster Session (Metzingen, Germany) </a> - Vid2Seq project - <a href="slides/vid2seq-cvpr-poster.pdf"> [Poster] </a> </li>
                <li>02/2023 - <a href="https://research.google/teams/perception/">Google Perception Spotlights, 5-min Talk (Virtual) </a> - Vid2Seq project </li>
                <li>01/2023 - <a href="https://prairie-institute.fr/">PRAIRIE workshop, Poster Session (Paris, France) </a> - FrozenBiLM project - <a href="slides/frozenbilm-neurips-poster.pdf"> [Poster] </a> </li>
                <li>11/2022 - <a href="https://nips.cc/">NeurIPS 2022, Poster Session (New Orleans, Louisiana) </a> - FrozenBiLM project - <a href="https://www.youtube.com/watch?v=dedoSjAiVL4&ab_channel=AntoineYang">[5 min Video] </a> - <a href="slides/frozenbilm-neurips.pdf"> [Slides] </a> - <a href="slides/frozenbilm-neurips-poster.pdf"> [Poster] </a> </li>
                <li>11/2022 - <a href="https://scai.sorbonne-universite.fr/public/events/view/7754b3ff1feea83b10d5/6">NeurIPS@Paris 2022, 6-min Talk (Paris, France) </a> - FrozenBiLM project - <a href="slides/frozenbilm-neurips.pdf"> [Slides] </a> - <a href="slides/frozenbilm-neurips-poster.pdf"> [Poster] </a> </li>
                <li>10/2022 - <a href="https://www.di.ens.fr/willow/">Seminar of Inria Willow and Sierra teams, Poster Session (Saint-Raphaël, France) </a> - FrozenBiLM project - <a href="slides/frozenbilm-neurips-poster.pdf"> [Poster] </a> </li>
                <li>06/2022 - <a href="https://cvpr2022.thecvf.com/">CVPR 2022, Oral and Poster Session (New Orleans, Louisiana) </a> - TubeDETR project - <a href="https://www.youtube.com/watch?v=FT_50ylQMNs&ab_channel=AntoineYang">[5 min Video] </a> - <a href="slides/tubedetr-cvpr.pdf"> [Slides] </a> - <a href="slides/tubedetr-cvpr-poster.pdf"> [Poster] </a> </li>
                <li>06/2022 - <a href="https://www.di.ens.fr/"> Seminar of the Computer Science department of &Eacute;cole Normale Sup&eacute;rieure, 15-min Talk (Mûr-de-Bretagne, France) </a> - FrozenBiLM project - <a href="slides/frozenbilm-diens.pdf"> [Slides] </a> </li>
                <li>10/2021 - <a href="https://www.di.ens.fr/willow/">Seminar of Inria Willow and Sierra teams, Poster Session (Avignon, France) </a> - Just Ask project -  <a href="slides/just-ask-iccv-poster.pdf"> [Poster] </a> </li>
                <li>10/2021 - <a href="https://iccv2021.thecvf.com/home">ICCV 2021, Oral and Poster Session (Virtual) </a> - Just Ask project - <a href="https://www.youtube.com/watch?v=eyEW5Z23Ofs&ab_channel=AntoineYang">[12 min Video] </a> - <a href="slides/just-ask-iccv.pdf"> [Slides] </a> - <a href="slides/just-ask-iccv-poster.pdf"> [Poster] </a> </li>
                <li>06/2021 - <a href="https://holistic-video-understanding.github.io/workshops/cvpr2021.html">CVPR 2021 Holistic Video Understanding Workshop, 10-min Invited Talk (Virtual) </a> - Just Ask project - <a href="slides/just-ask-hvu-cvpr21.pdf"> [Slides] </a> - <a href="https://youtu.be/jzXdRT5W3C4?t=17280"> [Recording] </a> </li>
                <li>05/2021 - <a href="https://www.rocq.inria.fr/semdoc/">Inria Junior Seminar, 30-min Talk (Virtual) </a> - Just Ask project - <a href="https://www.rocq.inria.fr/semdoc/Presentations/just-ask-junior-seminar.pdf"> [Slides] </a> - <a href="https://webconf.math.cnrs.fr/playback/presentation/2.0/playback.html?meetingId=38b91f46f61e82195e2275670f5d26218e2f9bae-1621340178022"> [Recording] </a> </li>
                <li>09/2020 - <a href="https://www.master-mva.com/">MVA Master Defense, Inria Internship 20-min Presentation (Paris, France) </a> - Video Question Answering </li>
                <li>04/2020 - <a href="https://iclr.cc/virtual_2020/poster_HygrdpVKvr.html">ICLR 2020, Poster Session (Virtual) </a> - NAS evaluation project - <a href="https://www.youtube.com/watch?v=jB7vrGR_4ak&ab_channel=AntoineYang">[5 min Video] </a> - <a href="slides/nasefh-iclr20.pdf"> [Slides] </a></li>
                <li>09/2019 - <a href="https://www.polytechnique.edu/">Ecole Polytechnique Master Defense, Huawei Internship 40-min Presentation (Palaiseau, France) </a> - NAS evaluation project </li>
            </ul>
          </div>
        </div>

          <hr>

      <!-- Teaching -->
        <div class="row" id="teaching" style="padding-top:30px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>Teaching</h2>
            <!--
            I also regularly give private lessons, notably for undergraduate students in preparatory classes in mathematics.
            -->
            <!--
            <ul>
                <li>Fall 2022 - Object Recognition and Computer Vision, Teacher Assistant - Master level (MVA) - 40 hours - ENS Paris-Saclay </li>
                <li>Fall 2021 - Object Recognition and Computer Vision, Project advisor - Master level (MVA) - Volunteering - ENS Paris-Saclay </li>
                <li>2021-2022 - Mathematics, Oral examiner - Undergraduate level (MPSI, MP and MP<sup>*</sup>) - 80 hours - Lycée Marcelin Berthelot </li>
                <li>Spring 2021 - Differential equations, Teacher assistant - Undergraduate level (L2) - 38 hours - Sorbonne Université </li>
                <li>Fall 2020 - Object Recognition and Computer Vision, Project advisor - Master level (MVA) - Volunteering - ENS Paris-Saclay </li>
                <li>2019-2020 - Mathematics, Oral examiner - Undergraduate level (MPSI) - 60 hours - Lycée Marcelin Berthelot </li>
                <li>Fall 2019 - Functional programming, Tutor - Undergraduate level (BSc) - 24 hours - École Polytechnique </li>
                <li>2017-2018 - Mathematics, Oral examiner - Undergraduate level (MPSI) - 60 hours - Lycée Marcelin Berthelot </li>
                <li>2017-2018 - Mathematics, Oral examiner - Undergraduate level (MPSI) - 60 hours - Lycée Marcelin Berthelot </li>

            </ul>
            -->

          <!-- PSL 2023 -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Spring 2023</span>
              </div>
              <div class="col-sm-11 col-md-11">
               &nbsp; Introduction to computer vision, Teacher Assistant - Master level - 3 hours - Dauphine-PSL University
              </div>
            </div>

            <!-- ORCV 2022 -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Fall&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2022</span>
              </div>
              <div class="col-sm-11 col-md-11">
               &nbsp; Object Recognition and Computer Vision, Teacher Assistant - Master level (MVA) - 50 hours - ENS Paris-Saclay
              </div>
            </div>

            <!-- PSL 2022 -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Spring 2022</span>
              </div>
              <div class="col-sm-11 col-md-11">
               &nbsp; Introduction to computer vision, Teacher Assistant - Master level - 3 hours - Dauphine-PSL University
              </div>
            </div>

              <!-- ORCV 2021 -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Fall&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2021</span>
              </div>
              <div class="col-sm-11 col-md-11">
               &nbsp; Object Recognition and Computer Vision, Project advisor - Master level (MVA) - Volunteering - ENS Paris-Saclay
              </div>
            </div>

              <!-- Colles PhD2 -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">2021 &nbsp;-&nbsp;2022</span>
              </div>
              <div class="col-sm-11 col-md-11">
                  &nbsp; Mathematics, Oral examiner - Undergraduate level (MPSI, MP and MP<sup>*</sup>) - 80 hours - Lycée Marcelin Berthelot
              </div>
            </div>

              <!-- Equa diff PhD1 -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Spring 2021</span>
              </div>
              <div class="col-sm-11 col-md-11">
               &nbsp; Differential equations, Teacher assistant - Undergraduate level (L2) - 38 hours - Sorbonne Université
              </div>
            </div>

              <!-- ORCV 2020 -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Fall&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2020</span>
              </div>
              <div class="col-sm-11 col-md-11">
               &nbsp; Object Recognition and Computer Vision, Project advisor - Master level (MVA) - Volunteering - ENS Paris-Saclay
              </div>
            </div>

              <!-- Colles 4A -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">2019 &nbsp;-&nbsp;2020</span>
              </div>
              <div class="col-sm-11 col-md-11">
               &nbsp; Mathematics, Oral examiner - Undergraduate level (MPSI) - 60 hours - Lycée Marcelin Berthelot
              </div>
            </div>

              <!-- Tutor bachelors -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Fall&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2019</span>
              </div>
              <div class="col-sm-11 col-md-11">
                &nbsp; Functional programming, Tutor - Undergraduate level (BSc) - 24 hours - École Polytechnique
              </div>
            </div>

              <!-- Colles 2A -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">2017 &nbsp;-&nbsp; 2018</span>
              </div>
              <div class="col-sm-11 col-md-11">
                &nbsp; Mathematics, Oral examiner - Undergraduate level (MPSI) - 60 hours - Lycée Marcelin Berthelot
              </div>
            </div>

              <!-- FH Internship -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Spring&nbsp;&nbsp;2017 </span>
              </div>
              <div class="col-sm-11 col-md-11">
                &nbsp; Multidisciplinary support, Socio-educational facilitator intern - Middle school - Collège Saint-Charles
              </div>
            </div>

          </div>
        </div> <!-- end of teaching -->

          <hr>

          <!-- Misc -->
        <div class="row" id="misc" style="padding-top:30px; margin-top:-60px;">
            <div class="col-md-12">
            <h2>Misc.</h2>
                <p>I am a reviewer for CVPR 2022, ECCV 2022, CVPR 2023, IJCV 2023, ICCV 2023, TPAMI 2023, NeurIPS 2023, ICML 2024 and ICLR 2025.</p>

                <p>Besides research, my passions include running, hiking and travels.</p>
            </div>
        </div> <!-- end of misc -->

      <hr>

      <div class="container">
        <footer>
          <p align="right"><small>Copyright © Antoine Yang &nbsp;/&nbsp; Last update: July 2025 </small></p>
        </footer>
        <div style="height:10px;"></div>
      </div>

      <!-- Bootstrap core JavaScript -->
      <!-- Placed at the end of the document so the pages load faster -->
      <script src="jq/jquery-1.11.1.min.js"></script>
      <script src="js/bootstrap.min.js"></script>
      <script src="js/docs.min.js"></script>

      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-75922772-1', 'auto');
        ga('send', 'pageview');

      </script>
      </div>
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    </body>
</html>
