<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="google-site-verification" content="FeA8wZzKiI6DkLLqCJMlBhKi6PsRnR1Dmdc2TDgTYcI" />
        <title>Antoine Yang</title>

        <link rel='icon' href='img/favicon.ico' type='image/x-icon'/>
        <link href="./css/bootstrap.min.css" rel="stylesheet">
        <link rel="stylesheet" href="./assets/academicons-1.7.0/css/academicons.css"/>
        <link rel="stylesheet" href="./assets/font-awesome-4.7.0/css/font-awesome.min.css"/>
	<script async defer src="https://buttons.github.io/buttons.js"></script>
    </head>

    <body>

      <!-- Navigation bar -->
      <div class="navbar navbar-default  navbar-fixed-top bg-info">
        <div class="container">
          <div class="navbar-header">

            <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
          </div>
          <div class="navbar-collapse collapse" id="navbar-main">

            <ul class="nav navbar-nav navbar-left">
              <li ><a href="#home">Home</a></li>
              <li ><a href="#news">News</a></li>
              <li ><a href="#research">Research</a></li>
              <li ><a href="#talks">Talks</a></li>
              <li ><a href="#teaching">Teaching</a></li>
              <li ><a href="#misc">Misc</a></li>
            </ul>
          </div>
        </div>
      </div>

      <!-- end of navigation bar -->
      <div style="height:40px;" id="home"></div>
      <div style="height:40px;"></div>

      <!-- CONTENTS -->
      <div class="container">
        <!-- Aboutme -->
        <div class="row" >
          <div class="col-xs-6 col-sm-4 col-md-2">
            <a class="thumbnail">
              <img src="./img/antoine.jpg" alt="Antoine Yang" class="img-rounded">
            </a>
          </div>

          <div class="col-xs-10 col-sm-6 col-md-4">
            <h1 class="text-info">Antoine Yang</h1>
            <h4 class="text-info">Research Scientist, Google DeepMind</h4>
            <h5>
              <a href="mailto:antoineyang@google.com" class="text-info" title="e-Mail"><i class="fa fa-envelope-square fa-2x"></i></a>
	          <a href="https://scholar.google.com/citations?user=SlMKN6IAAAAJ" class="text-info" title="Google Scholar"><i class="ai ai-google-scholar-square ai-2x"></i></a>
              <a href="https://github.com/antoyang" class="text-info" title="GitHub"><i class="fa fa-github-square fa-2x"></i></a>
              <a href="https://www.linkedin.com/in/antoyang/" class="text-info" title="LinkedIn"><i class="fa fa-linkedin-square fa-2x"></i></a>
              <a href="https://twitter.com/AntoineYang2" class="text-info" title="Twitter"><i class="fa fa-twitter-square fa-2x"></i></a>
              <a href="https://www.youtube.com/channel/UCYJ4NEILh5DeYf-62eb1RBg" class="text-info" title="Twitter"><i class="fa fa-youtube-square fa-2x"></i></a>
		  </h5>
          </div>
        </div>
        <!-- end of Aboutme -->

         <p align="justify">
             I am a Research Scientist at <a href="https://www.deepmind.com/">Google DeepMind</a> in London, working on the multi-modal capabilities of <a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/">Gemini</a>.
             In 2023, I completed my PhD in the <a href="http://www.di.ens.fr/willow">WILLOW team</a> of <a href="http://www.inria.fr/">Inria Paris</a> and <a href="http://www.ens.fr">&Eacute;cole Normale Sup&eacute;rieure</a>, advised by <a href="https://antoine77340.github.io/">Antoine Miech</a>, <a href="http://people.ciirc.cvut.cz/~sivic/">Josef Sivic</a>, <a href="http://www.di.ens.fr/~laptev">Ivan Laptev</a> and <a href="https://www.di.ens.fr/willow/people_webpages/cordelia/">Cordelia Schmid</a>.
             My thesis, supported by a <a href="https://research.google/outreach/phd-fellowship/">Google PhD Fellowship</a>, focused on learning visual language models for video understanding.
             In 2020, I received an <a href="https://programmes.polytechnique.edu/cycle-ingenieur-polytechnicien/cycle-ingenieur-polytechnicien"> engineering degree </a> from <a href="https://www.polytechnique.edu/">École Polytechnique </a> and a MSc degree in <a href="https://www.master-mva.com/">Mathematics, Vision and Learning</a> from <a href="http://ens-paris-saclay.fr/">ENS Paris-Saclay</a>.
             I previously interned at <a href="http://www.noahlab.com.hk/">Huawei Noah's Ark Lab</a> and <a href="https://research.google">Google Research</a> <a href="https://research.google/teams/perception/">Perception</a>.
             See my <a href="https://www.linkedin.com/in/antoyang/">LinkedIn profile</a> for a full resume.
         </p>
        <hr>

        <!-- News header-->
        <div class="row" id="news" style="padding-top:30px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>News</h2>
          </div>
        </div>
        <!-- End news header -->

    <!-- Gemini 1.5 -->
    <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">02 / 2024</span>
          </div>
          <div class="col-sm-11 col-md-11">
              We released <a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/">Gemini 1.5</a>!
              </div>
        </div>
    <div style="height:3px;"></div>

    <!-- PhD Defense -->
    <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">11 / 2023</span>
          </div>
          <div class="col-sm-11 col-md-11">
              I have successfully defended my <a href="https://hal.science/tel-04307117">PhD thesis</a>!
              </div>
        </div>
    <div style="height:3px;"></div>

    <!-- DeepMind RS -->
    <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">10 / 2023</span>
          </div>
          <div class="col-sm-11 col-md-11">
              I have joined <a href="https://www.deepmind.com/">Google DeepMind</a>'s Computer Vision team in London full-time as a Research Scientist!
          </div>
        </div>
    <div style="height:3px;"></div>

    <!-- ICCV'23 Consortium -->
    <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">10 / 2023</span>
          </div>
          <div class="col-sm-11 col-md-11">
              I am attending the <a href="https://iccv2023.thecvf.com//">ICCV 2023 Doctoral Consortium</a> in Paris.
          </div>
        </div>
    <div style="height:3px;"></div>

    <!-- VidChapters accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">09 / 2023</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="vidchapters.html">VidChapters-7M</a> is accepted at <a href="https://nips.cc/Conferences/2023/CallForDatasetsBenchmarks">NeurIPS 2023 Track on Datasets and Benchmarks</a>!
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- CoVR -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">08 / 2023</span>
          </div>
          <div class="col-sm-11 col-md-11">
              New preprint about Composed Video Retrieval: <a href="http://imagine.enpc.fr/~ventural/covr/">CoVR</a>.
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- ICVSS -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">07 / 2023</span>
          </div>
          <div class="col-sm-11 col-md-11">
              I am attending the <a href="https://iplab.dmi.unict.it/icvss2023/Home"> ICVSS Summer School</a> in Sampieri.
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Ellis workshop -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">05 / 2023</span>
          </div>
          <div class="col-sm-11 col-md-11">
              I am attending an <a href="https://ellis.eu/">ELLIS computer vision workshop</a> in Metzingen.
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Vid2Seq Blog and Code -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">03 / 2023</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="vid2seq.html">Vid2Seq</a> is featured on the <a href="https://ai.googleblog.com/2023/03/vid2seq-pretrained-visual-language.html">Google AI Blog</a>, and the code is released <a href="https://github.com/google-research/scenic/tree/main/scenic/projects/vid2seq"> here</a>.
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Vid2Seq accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">02 / 2023</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="vid2seq.html">Vid2Seq</a> is accepted at <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>!
          </div>
        </div>
        <div style="height:3px;"></div>
	      
    <!-- FrozenBiLM accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">09 / 2022</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="frozenbilm.html">FrozenBiLM</a> is accepted at <a href="https://nips.cc/Conferences/2022">NeurIPS 2022</a>!
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Google internship -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">06 / 2022</span>
          </div>
          <div class="col-sm-11 col-md-11">
              I am starting a 6-month research internship at <a href="https://research.google/">Google Research</a> (<a href="https://research.google/teams/perception/">Perception team</a>) in Grenoble.
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Just Ask extension accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">04 / 2022</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="just-ask.html">Just Ask extension</a> is accepted at the <a href="https://www.computer.org/csdl/journal/tp">TPAMI Special Issue on the Best Papers of ICCV 2021</a>!
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- TubeDETR accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">03 / 2022</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="tubedetr.html">TubeDETR</a> is accepted at <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a> as an oral!
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Just Ask accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">07 / 2021</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="just-ask.html">Just Ask</a> is accepted at <a href="http://iccv2021.thecvf.com/home">ICCV 2021</a> as an oral!
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Just Ask - HVU CVPR 2021 Talk -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">06 / 2021</span>
          </div>
          <div class="col-sm-11 col-md-11">
              I gave talks about <a href="just-ask.html">Just Ask</a> at the <a href="https://holistic-video-understanding.github.io/workshops/cvpr2021.html"> CVPR 2021 Holistic Video Understanding Workshop</a> and at the <a href="https://www.rocq.inria.fr/semdoc/">Inria Junior Seminar</a>.
          </div>
        </div>
        <div style="height:3px;"></div>

	<!-- Willow PhD -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">09 / 2020</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I am starting my PhD at <a href="http://www.di.ens.fr/willow">Inria WILLOW</a> in Paris.
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- MVA degree -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">09 / 2020</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I have received a <a href="https://www.master-mva.com/"> MSc degree </a> with highest honors and jury congratulations from <a href="https://ens-paris-saclay.fr/">ENS Paris-Saclay</a>.
          </div>
        </div>
        <div style="height:3px;"></div>

	<!-- Willow internship -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">04 / 2020</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I am starting a 5-month research internship at <a href="http://www.di.ens.fr/willow">Inria WILLOW</a> in Paris.
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- NASEFH accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">12 &nbsp;/ 2019 </span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="nasefh.html">NAS evaluation is frustratingly hard</a> is accepted at <a href="https://iclr.cc/Conferences/2020">ICLR 2020</a>!
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- X degree -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">09 / 2019</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I have received an <a href="https://programmes.polytechnique.edu/cycle-ingenieur-polytechnicien/cycle-ingenieur-polytechnicien"> engineering degree </a> from <a href="https://www.polytechnique.edu/">École Polytechnique</a>.
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Huawei internship -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">04 / 2019</span>
          </div>
          <div class="col-sm-11 col-md-11">
              I am starting a 5-month research internship at <a href="http://www.noahlab.com.hk/">Huawei Noah's Ark Lab</a> (<a href="http://dev3.noahlab.com.hk/research.html">AI Theory team</a>) in London.
          </div>
        </div>
        <div style="height:3px;"></div>

        <!-- end of news -->
        <hr>

        <!-- Research -->
        <div class="row"  id="research" style="padding-top:30px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>Research</h2>
              <p>See my <a href="https://scholar.google.com/citations?user=SlMKN6IAAAAJ">Google Scholar </a> and <a href="https://github.com/antoyang"> GitHub </a> profiles for more information.</p>

<!-- Gemini 1.5 -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4" style='height:120px'>
		<a class="thumbnail">
		<img src="./img/gemini_1p5.png" height="100%" alt="Gemini 1.5">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</strong> </br>
	Gemini Team, Google<br>
          Google Blog 2024 <br>
          <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract11">abstract</button>
	<div id="abstract11" class="collapse">
        <p style="text-align: justify;">
	    In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio.
        Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra’s state-of-the-art performance across a broad set of benchmarks.
        Studying the limits of Gemini 1.5 Pro’s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k).
        Finally, we highlight surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person learning from the same content.
</p>
        </div>
          <a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/"><button type="button" class="btn btn-danger btn-xs">blogpost</button></a>
      </div>
    </div>
<!-- end of Gemini 1.5 -->

<!-- PhD Thesis -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4" style='height:120px'>
		<a class="thumbnail">
		<img src="./img/thesis-header.png" height="100%" alt="Learning Visual Language Models for Video Understanding">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>Learning Visual Language Models for Video Understanding</strong> </br>
	<u>Antoine Yang</u><br>
          PhD Thesis 2023 <br>
          <a href="https://hal.science/tel-04307117"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex10">bibtex</button>
	<div id="bibtex10" class="collapse">
	  <pre><tt>@phdthesis{yang:tel-04307117,
  TITLE = {{Learning Visual Language Models for Video Understanding}},
  AUTHOR = {Yang, Antoine},
  URL = {https://hal.science/tel-04307117},
  SCHOOL = {{Ecole Normale Superieure de Paris - ENS Paris}},
  YEAR = {2023},
  MONTH = Nov,
  KEYWORDS = {Machine learning ; Computer vision ; Artificial intelligence ; Natural language processing ; Video understanding ; Deep learning ; Apprentissage automatique ; Vision par ordinateur ; Intelligence artificielle ; Traitement du langage naturel ; Compr{\'e}hension de vid{\'e}os ; Apprentissage profond},
  TYPE = {Theses},
  PDF = {https://hal.science/tel-04307117v2/file/PhD.pdf},
  HAL_ID = {tel-04307117},
  HAL_VERSION = {v2},
}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract10">abstract</button>
	<div id="abstract10" class="collapse">
        <p style="text-align: justify;">
	        The goal of this thesis is to build and train machine learning models that combine the power of natural language processing with visual understanding, enabling a comprehensive and detailed comprehension of the content within videos. 
		First, we propose two scalable approaches to develop video question answering models without the need for costly manual annotation. 
		We automatically generate video question answering data from narrated videos using text-only question-generation models. 
		We then show that a multi-modal transformer trained contrastively on the generated data can answer visual questions in a zero-shot manner. 
		In order to bypass the data generation procedure, we present an alternative approach, dubbed FrozenBiLM, that directly leverages bidirectional masked language models. 
		Second, we develop TubeDETR, a transformer model that can spatially and temporally localize a natural language query in an untrimmed video. 
		Unlike prior spatio-temporal grounding approaches, TubeDETR can be effectively trained end-to-end on untrimmed videos. 
		Third, we present a new model and a new dataset for multi-event understanding in untrimmed videos. 
		We introduce the Vid2Seq model which generates dense natural language descriptions and corresponding temporal boundaries for all events in an untrimmed video by predicting a single sequence of tokens. 
		Moreover, Vid2Seq can be effectively pretrained on narrated videos at scale using transcribed speech as pseudo-supervision. 
		Finally, we introduce VidChapters-7M, a large-scale dataset of user-chaptered videos. Based on this dataset, we evaluate state-of-the-art models on three tasks including video chapter generation. 
		We also show that video chapter generation models transfer well to dense video captioning in both zero-shot and finetuning settings.
            </p>
        </div>
      </div>
    </div>
<!-- end of PhD thesis -->
		  
<!-- VidChapters-7M, NeurIPS 2023 D&B -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4" style='height:120px'>
		<a class="thumbnail">
		<img src="./img/vidchapters-header.png" height="100%" alt="VidChapters-7M: Video Chapters at Scale">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>VidChapters-7M: Video Chapters at Scale</strong> </br>
	<u>Antoine Yang</u>, Arsha Nagrani, Ivan Laptev, Josef Sivic, Cordelia Schmid<br>
          NeurIPS 2023 Track on Datasets and Benchmarks <br>
          <a href="https://arxiv.org/pdf/2309.13952.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	<a href="vidchapters.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex9">bibtex</button>
	<div id="bibtex9" class="collapse">
	  <pre><tt>@inproceedings{yang2023vidchapters,
title={VidChapters-7M: Video Chapters at Scale},
author={Antoine Yang and Arsha Nagrani and Ivan Laptev and Josef Sivic and Cordelia Schmid},
booktitle={NeurIPS},
year = {2023}}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract9">abstract</button>
	<div id="abstract9" class="collapse">
        <p style="text-align: justify;">
	        Segmenting long videos into chapters enables users to quickly navigate to the information of their interest.
            This important topic has been understudied due to the lack of publicly released datasets.
            To address this issue, we present VidChapters-7M, a dataset of 817K user-chaptered videos including 7M chapters in total.
            VidChapters-7M is automatically created from videos online in a scalable manner by scraping user-annotated chapters and hence without any additional manual annotation.
            We introduce the following three tasks based on this data.
            First, the video chapter generation task consists of temporally segmenting the video and generating a chapter title for each segment.
            To further dissect the problem, we also define two variants of this task: video chapter generation given ground-truth boundaries, which requires generating a chapter title given an annotated video segment, and video chapter grounding, which requires temporally localizing a chapter given its annotated title.
            We benchmark both simple baselines and state-of-the-art video-language models for these three tasks.
            We also show that pretraining on VidChapters-7M transfers well to dense video captioning tasks in both zero-shot and finetuning settings, largely improving the state of the art on the YouCook2 and ViTT benchmarks.
            Finally, our experiments reveal that downstream performance scales well with the size of the pretraining dataset.
            </p>
        </div>
          <a href="https://github.com/antoyang/VidChapters"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          <a class="github-button" href="https://github.com/antoyang/VidChapters" data-icon="octicon-star" data-show-count="true" aria-label="Star google-research/scenic on GitHub">Star</a>
      </div>
    </div>
<!-- end of VidChapters -->

<!-- CoVR -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4" style='height:120px'>
		<a class="thumbnail">
		<img src="http://imagine.enpc.fr/~ventural/covr/images/teaser.png" height="100%" alt="CoVR: Learning Composed Video Retrieval from Web Video Captions">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>CoVR: Learning Composed Video Retrieval from Web Video Captions</strong> </br>
	Lucas Ventura, <u>Antoine Yang</u>, Cordelia Schmid, Gül Varol<br>
          AAAI 2024 <br>
          <a href="https://arxiv.org/pdf/2308.14746"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	<a href="http://imagine.enpc.fr/~ventural/covr/"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex8">bibtex</button>
	<div id="bibtex8" class="collapse">
	  <pre><tt>@inproceedings{ventura2023covr,
title={CoVR: Learning Composed Video Retrieval from Web Video Captions},
author={Lucas Ventura and Antoine Yang and Cordelia Schmid and G{\"u}l Varol},
booktitle={arXiv},
year={2023}}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract8">abstract</button>
	<div id="abstract8" class="collapse">
        <p style="text-align: justify;">
	        Composed Image Retrieval (CoIR) has recently gained popularity as a task that considers both text and image queries together, to search for relevant images in a database.
            Most CoIR approaches require manually annotated datasets, containing image-text-image triplets, where the text describes a modification from the query image to the target image.
            However, manual curation of CoIR triplets is expensive and prevents scalability.
            In this work, we instead propose a scalable automatic dataset creation methodology that generates triplets given video-caption pairs.
            To this end, we mine paired videos with a similar caption from a large database, and leverage a large language model to generate the corresponding modification text.
            We automatically construct our WebVid-CoVR dataset by applying this procedure to the large WebVid2M collection, resulting in 1.6M triplets.
            Moreover, we introduce a new benchmark for composed video retrieval (CoVR) and contribute a manually annotated evaluation set, along with baseline results.
            We further show that training a CoVR model on our dataset transfers well to CoIR, improving the state of the art in the zero-shot setup on both the CIRR and FashionIQ benchmarks.
            </p>
        </div>
          <a href="https://github.com/lucas-ventura/covr/"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          <a class="github-button" href="https://github.com/lucas-ventura/covr/" data-icon="octicon-star" data-show-count="true" aria-label="Star google-research/scenic on GitHub">Star</a>
      </div>
    </div>
<!-- end of CoVR -->

<!-- Vid2Seq, CVPR 2023 -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4">
		<a class="thumbnail">
		<img src="./img/vid2seq.png" alt="Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning</strong> </br>
	<u>Antoine Yang</u>, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, Cordelia Schmid<br>
          CVPR 2023 <br>
          <a href="https://arxiv.org/pdf/2302.14115"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	<a href="vid2seq.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex7">bibtex</button>
	<div id="bibtex7" class="collapse">
	  <pre><tt>@inproceedings{yang2023vid2seq,
title = {Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning},
author={Antoine Yang and Arsha Nagrani and Paul Hongsuck Seo and Antoine Miech and Jordi Pont-Tuset and Ivan Laptev and Josef Sivic and Cordelia Schmid},
booktitle={CVPR},
year = {2023}}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract7">abstract</button>
	<div id="abstract7" class="collapse">
        <p style="text-align: justify;">
	        In this work, we introduce Vid2Seq, a multi-modal single-stage dense event captioning model pretrained on narrated videos which are readily-available at scale.
            The Vid2Seq architecture augments a language model with special time tokens, allowing it to seamlessly predict event boundaries and textual descriptions in the same output sequence.
            Such a unified model requires large-scale training data, which is not available in current annotated datasets.
            We show that it is possible to leverage unlabeled narrated videos for dense video captioning, by reformulating sentence boundaries of transcribed speech as pseudo event boundaries, and using the transcribed speech sentences as pseudo event captions.
            The resulting Vid2Seq model pretrained on the YT-Temporal-1B dataset improves the state of the art on a variety of dense video captioning benchmarks including YouCook2, ViTT and ActivityNet Captions.
            Vid2Seq also generalizes well to the tasks of video paragraph captioning and video clip captioning, and to few-shot settings.
        </p>
        </div>
          <a href="https://ai.googleblog.com/2023/03/vid2seq-pretrained-visual-language.html"><button type="button" class="btn btn-danger btn-xs">blogpost</button></a>
          <a href="https://github.com/google-research/scenic/tree/main/scenic/projects/vid2seq"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          <a class="github-button" href="https://github.com/google-research/scenic" data-icon="octicon-star" data-show-count="true" aria-label="Star google-research/scenic on GitHub">Star</a>
      </div>
    </div>
<!-- end of Vid2Seq -->

<!-- FrozenBiLM, NeurIPS 2022 -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4">
		<a class="thumbnail">
		<img src="./img/frozenbilm.png" alt="Zero-Shot Video Question Answering via Frozen Bidirectional Language Models">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>Zero-Shot Video Question Answering via Frozen Bidirectional Language Models</strong> </br>
	<u>Antoine Yang</u>, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid<br>
          NeurIPS 2022 <br>
          <a href="https://arxiv.org/pdf/2206.08155.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	<a href="frozenbilm.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex6">bibtex</button>
	<div id="bibtex6" class="collapse">
	  <pre><tt>@inproceedings{yang2022frozenbilm,
title = {Zero-Shot Video Question Answering via Frozen Bidirectional Language Models},
author={Antoine Yang and Antoine Miech and Josef Sivic and Ivan Laptev and Cordelia Schmid},
booktitle={NeurIPS}
year = {2022}}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract6">abstract</button>
	<div id="abstract6" class="collapse">
        <p style="text-align: justify;">
	        Video question answering (VideoQA) is a complex task that requires diverse multi-modal data for training.
            Manual annotation of question and answers for videos, however, is tedious and prohibits scalability.
            To tackle this problem, recent methods consider zero-shot settings with no manual annotation of visual question-answer.
            In particular, a promising approach adapts frozen autoregressive language models pretrained on Web-scale text-only data to multi-modal inputs.
            In contrast, we here build on frozen bidirectional language models (BiLM) and show that such an approach provides a stronger and cheaper alternative for zero-shot VideoQA.
            In particular, (i) we combine visual inputs with the frozen BiLM using light trainable modules,
            (ii) we train such modules using Web-scraped multi-modal data, and finally
            (iii) we perform zero-shot VideoQA inference through masked language modeling, where the masked text is the answer to a given question.
            Our proposed approach, FrozenBiLM, outperforms the state of the art in zero-shot VideoQA by a significant margin on a variety of datasets, including LSMDC-FiB, iVQA, MSRVTT-QA, MSVD-QA, ActivityNet-QA, TGIF-FrameQA, How2QA and TVQA.
            It also demonstrates competitive performance in the few-shot and fully-supervised setting.
        </p>
        </div>
          <a href="https://github.com/antoyang/FrozenBiLM"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          <a class="github-button" href="https://github.com/antoyang/FrozenBiLM" data-icon="octicon-star" data-show-count="true" aria-label="Star antoyang/FrozenBiLM on GitHub">Star</a>
      </div>
    </div>
<!-- end of FrozenBiLM -->

<!-- Just Ask extension, TPAMI -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4">
		<a class="thumbnail">
		<img src="./img/justaskextension.png" alt="Learning to Answer Visual Questions from Web Videos">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>Learning to Answer Visual Questions from Web Videos</strong> (journal extension of <strong>Just Ask</strong>) </br>
	<u>Antoine Yang</u>, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid<br>
          TPAMI Special Issue on the Best Papers of ICCV 2021 <br>
          <a href="https://arxiv.org/pdf/2205.05019v2.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	<a href="just-ask.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex5">bibtex</button>
	<div id="bibtex5" class="collapse">
	  <pre><tt>@article{yang2022learningta,
title={Learning to Answer Visual Questions from Web Videos},
author={Antoine Yang and Antoine Miech and Josef Sivic and Ivan Laptev and Cordelia Schmid},
journal={IEEE TPAMI},
year={2022}}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract5">abstract</button>
	<div id="abstract5" class="collapse">
        <p style="text-align: justify;">
	        Recent methods for visual question answering rely on large-scale annotated datasets.
            Manual annotation of questions and answers for videos, however, is tedious, expensive and prevents scalability.
            In this work, we propose to avoid manual annotation and generate a large-scale training dataset for video question answering making use of automatic cross-modal supervision.
            We leverage a question generation transformer trained on text data and use it to generate question-answer pairs from transcribed video narrations.
            Given narrated videos, we then automatically generate the HowToVQA69M dataset with 69M video-question-answer triplets.
            To handle the open vocabulary of diverse answers in this dataset, we propose a training procedure based on a contrastive loss between a video-question multi-modal transformer and an answer transformer.
            We introduce the zero-shot VideoQA task and the VideoQA feature probe evaluation setting and show excellent results.
            Furthermore, our method achieves competitive results on MSRVTT-QA, ActivityNet-QA, MSVD-QA and How2QA datasets.
            We also show that our approach generalizes to another source of web video and text data.
            We generate the WebVidVQA3M dataset from videos with alt-text annotations, and show its benefits for training VideoQA models.
            Finally, for a detailed evaluation we introduce iVQA, a new VideoQA dataset with reduced language bias and high-quality manual annotations.
	</p>
        </div>
          <a href="http://videoqa.paris.inria.fr/"><button type="button" class="btn btn-danger btn-xs">demo</button></a>
          <a href="https://github.com/antoyang/just-ask"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          <a class="github-button" href="https://github.com/antoyang/just-ask" data-icon="octicon-star" data-show-count="true" aria-label="Star antoyang/just-ask on GitHub">Star</a>
      </div>
    </div>
<!-- end of Just Ask extension -->

<!-- TubeDETR, CVPR 2022 -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4">
		<a class="thumbnail">
		<img src="./img/tubedetr.png" alt="TubeDETR: Spatio-Temporal Video Grounding with Transformers">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <strong>TubeDETR: Spatio-Temporal Video Grounding with Transformers</strong> </br>
	<u>Antoine Yang</u>, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid<br>
          CVPR 2022 (<strong><font color="red">oral</font></strong>: top 4% submissions) <br>
          <a href="https://arxiv.org/pdf/2203.16434.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	<a href="tubedetr.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex4">bibtex</button>
	<div id="bibtex4" class="collapse">
	  <pre><tt>@inproceedings{yang2022tubedetr,
author={Antoine Yang and Antoine Miech and Josef Sivic and Ivan Laptev and Cordelia Schmid},
title={TubeDETR: Spatio-Temporal Video Grounding With Transformers},
booktitle={CVPR},
year={2022}}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract4">abstract</button>
	<div id="abstract4" class="collapse">
        <p style="text-align: justify;">
	        We consider the problem of localizing a spatio-temporal tube in a video corresponding to a given text query.
            This is a challenging task that requires the joint and efficient modeling of temporal, spatial and multi-modal interactions.
            To address this task, we propose TubeDETR, a transformer-based architecture inspired by the recent success of such models for text-conditioned object detection.
            Our model notably includes: (i) an efficient video and text encoder that models spatial multi-modal interactions over sparsely sampled frames and
            (ii) a space-time decoder that jointly performs spatio-temporal localization.
            We demonstrate the advantage of our proposed components through an extensive ablation study.
            We also evaluate our full approach on the spatio-temporal video grounding task and demonstrate improvements over the state of the art on the challenging VidSTG and HC-STVG benchmarks.
	</p>
        </div>
          <a href="http://stvg.paris.inria.fr/"><button type="button" class="btn btn-danger btn-xs">demo</button></a>
          <a href="https://github.com/antoyang/TubeDETR"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          <a class="github-button" href="https://github.com/antoyang/TubeDETR" data-icon="octicon-star" data-show-count="true" aria-label="Star antoyang/TubeDETR on GitHub">Star</a>

      </div>
    </div>
<!-- end of TubeDETR -->

<!-- Just Ask, ICCV 2021 -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4">
		<a class="thumbnail">
		<img src="./img/justask.PNG" alt="Just Ask: Learning to Answer Questions from Millions of Narrated Videos">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <!-- <font color="red">[New]</font> -->
          <strong>Just Ask: Learning to Answer Questions from Millions of Narrated Videos</strong> </br>
	<u>Antoine Yang</u>, Antoine Miech, Josef Sivic, Ivan Laptev, Cordelia Schmid<br>
          ICCV 2021 (<strong><font color="red">oral</font></strong>: top 3% submissions) <br>
          <a href="https://arxiv.org/pdf/2012.00451.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	<a href="just-ask.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex3">bibtex</button>
	<div id="bibtex3" class="collapse">
	  <pre><tt>@inproceedings{yang2021justask,
title={Just Ask: Learning To Answer Questions From Millions of Narrated Videos},
author={Antoine Yang and Antoine Miech and Josef Sivic and Ivan Laptev and Cordelia Schmid},
booktitle={ICCV},
year={2021}}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract3">abstract</button>
	<div id="abstract3" class="collapse">
        <p style="text-align: justify;">
	        Recent methods for visual question answering rely on large-scale annotated datasets.
            Manual annotation of questions and answers for videos, however, is tedious, expensive and prevents scalability.
            In this work, we propose to avoid manual annotation and generate a large-scale training dataset for video question answering making use of automatic cross-modal supervision.
            We leverage a question generation transformer trained on text data and use it to generate question-answer pairs from transcribed video narrations.
            Given narrated videos, we then automatically generate the HowToVQA69M dataset with 69M video-question-answer triplets.
            To handle the open vocabulary of diverse answers in this dataset, we propose a training procedure based on a contrastive loss between a video-question multi-modal transformer and an answer transformer.
            We introduce the zero-shot VideoQA task and show excellent results, in particular for rare answers.
            Furthermore, we demonstrate our method to significantly outperform the state of the art on MSRVTT-QA, MSVD-QA, ActivityNet-QA and How2QA.
            Finally, for a detailed evaluation we introduce iVQA, a new VideoQA dataset with reduced language biases and high-quality redundant manual annotations.
	</p>
        </div>
          <a href="http://videoqa.paris.inria.fr/"><button type="button" class="btn btn-danger btn-xs">demo</button></a>
          <a href="https://github.com/antoyang/just-ask"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          <a class="github-button" href="https://github.com/antoyang/just-ask" data-icon="octicon-star" data-show-count="true" aria-label="Star antoyang/just-ask on GitHub">Star</a>
      </div>
    </div>
<!-- end of Just Ask -->

<!-- NASEFH, ICLR 2020 -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="./img/nasefh.PNG" alt="NAS evaluation is frustratingly hard">
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong>NAS evaluation is frustratingly hard</strong><br>
		<u>Antoine Yang</u>, Pedro M. Esperança, Fabio Maria Carlucci<br>
                ICLR 2020<br>
                  <a href="https://arxiv.org/pdf/1912.12522.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
                  <a href="nasefh.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex2">bibtex</button>
                <div id="bibtex2" class="collapse">
                  <pre><tt>@inproceedings{yang2020nasefh,
title={NAS evaluation is frustratingly hard},
author={Antoine Yang and Pedro M. Esperança and Fabio M. Carlucci},
booktitle={ICLR},
year={2020}}</tt></pre>
                </div>
                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract2">abstract</button>
                <div id="abstract2" class="collapse">
		  <div>
              Neural Architecture Search (NAS) is an exciting new field which promises to be as much as a game-changer as Convolutional Neural Networks were in 2012.
              Despite many great works leading to substantial improvements on a variety of tasks, comparison between different methods is still very much an open issue.
              While most algorithms are tested on the same datasets, there is no shared experimental protocol followed by all.
              As such, and due to the under-use of ablation studies, there is a lack of clarity regarding why certain methods are more effective than others. Our first contribution is a benchmark of 8 NAS methods on 5 datasets.
              To overcome the hurdle of comparing methods with different search spaces, we propose using a method’s relative improvement over the randomly sampled average architecture, which effectively removes advantages arising from expertly engineered search spaces or training protocols.
              Surprisingly, we find that many NAS techniques struggle to significantly beat the average architecture baseline. We perform further experiments with the commonly used DARTS search space in order to understand the contribution of each component in the NAS pipeline.
              These experiments highlight that: (i) the use of tricks in the evaluation protocol has a predominant impact on the reported performance of architectures; (ii) the cell-based search space has a very narrow accuracy range, such that the seed has a considerable impact on architecture rankings; (iii) the hand-designed macrostructure (cells) is more important than the searched micro-structure (operations); and (iv) the depth-gap is a real phenomenon, evidenced by the change in rankings between 8 and 20 cell architectures.
              To conclude, we suggest best practices, that we hope will prove useful for the community and help mitigate current NAS pitfalls, e.g. difficulties in reproducibility and comparison of search methods.
                  </div>
                </div>
                  <a href="https://medium.com/@antoyang/is-neural-architecture-search-really-worth-it-2d0b9f28a1ed"><button type="button" class="btn btn-danger btn-xs">blogpost</button></a>
                  <a href="https://github.com/antoyang/NAS-Benchmark"><button type="button" class="btn btn-primary btn-xs">code</button></a>
                  <a class="github-button" href="https://github.com/antoyang/NAS-Benchmark" data-icon="octicon-star" data-show-count="true" aria-label="Star antoyang/NAS-Benchmark on GitHub">Star</a>
              </div>
            </div>
<!-- end of NASEFH -->

<!-- MANAS -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="./img/manas.PNG" style="height:200px;" alt="MANAS">
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong>MANAS: Multi-Agent Neural Architecture Search</strong><br>
		Vasco Lopes, Fabio Maria Carlucci, Pedro M. Esperança, Marco Singh, <u>Antoine Yang</u>, Victor Gabillon, Hang Xu, Zewei Chen, Jun Wang<br>
                Machine Learning 2023 <br>
                  <a href="https://arxiv.org/pdf/1909.01051.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex1">bibtex</button>
                <div id="bibtex1" class="collapse">
                  <pre><tt>@article{lopes2023manas,
  title={MANAS: Multi-Agent Neural Architecture Search},
  author={Lopes Vasco and Fabio Maria Carlucci and Pedro M Esperan{\c{c}}a and Marco Singh and Antoine Yang and Victor Gabillon and Hang Xu and Zewei Chen and Jun Wang},
  journal={Machine Learning},
  year={2023}}</tt></pre>
                </div>
                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract1">abstract</button>
                <div id="abstract1" class="collapse">
		  <div>
              The Neural Architecture Search (NAS) problem is typically formulated as a graph search problem where the goal is to learn the optimal operations over edges in order to maximise a graph-level global objective.
              Due to the large architecture parameter space, efficiency is a key bottleneck preventing NAS from its practical use.
              In this paper, we address the issue by framing NAS as a multi-agent problem where agents control a subset of the network and coordinate to reach optimal architectures.
              We provide two distinct lightweight implementations, with reduced memory requirements (1/8th of state-of-the-art), and performances above those of much more computationally expensive methods.
              Theoretically, we demonstrate vanishing regrets of the form O(sqrt(T)), with T being the total number of rounds.
              Finally, aware that random search is an, often ignored, effective baseline we perform additional experiments on 3 alternative datasets and 2 network configurations, and achieve favourable results in comparison.
                  </div>
                </div>
              </div>
            </div>
<!-- end of MANAS -->

          </div>
        </div> <!-- end of projects -->

          <hr>

      <!-- Talks -->
        <div class="row" id="talks" style="padding-top:30px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>Talks</h2>
            <ul>
                <li>03/2024 - <a href="https://www.qmul.ac.uk/deri/events--seminars/">Seminar of the Digital Environment Research Institute at Queen Mary, 20-min Talk (London, United Kingdom) </a> - Vid2Seq and VidChapters-7M projects </li>
                <li>01/2024 - <a href="https://www.bmva.org/meetings/24-01-17-Vision%20and%20Language.html">BMVA Symposium on Vision and Language, Poster Session (London, United Kingdom) </a> - VidChapters-7M project - <a href="slides/vidchapters-neurips-poster.pdf"> [Poster] </a> </li>
                <li>12/2023 - <a href="https://neurips.cc/">NeurIPS 2023, Poster Session (New Orleans, Louisiana) </a> - VidChapters-7M project - <a href="https://www.youtube.com/watch?v=prn9SP8D5uE&ab_channel=AntoineYang">[5 min Video] </a> - <a href="slides/vidchapters-neurips.pdf"> [Slides] </a> - <a href="slides/vidchapters-neurips-poster.pdf"> [Poster] </a> </li>
                <li>11/2023 - <a href="https://theaitalks.org/">The AI Talks, 45-min Talk (Virtual) </a> - VLM for video understanding - <a href="https://www.youtube.com/watch?v=njUDbJz6zWI&ab_channel=TheAITalks"> [Recording] </a> </li>
		        <li>11/2023 - <a href="https://www.ens.psl.eu/">ENS PhD Defense, 45-min Presentation (Paris, France)</a> - VLM for video understanding - <a href="slides/phd-defense.pdf"> [Slides] </a> </li>
                <li>10/2023 - <a href="https://www.hceres.fr/fr">HCERES visit, Poster Session (Paris, France) </a> - VidChapters-7M project - <a href="slides/vid2seq-cvpr-poster.pdf"> [Poster] </a> </li>
                <li>10/2023 - <a href="https://iccv2023.thecvf.com//">ICCV 2023 Doctoral Consortium, 15-min Discussion (Paris, France) </a> - VidChapters-7M project - <a href="slides/vidchapters-neurips.pdf"> [Slides] </a> </li>
                <li>07/2023 - <a href="https://iplab.dmi.unict.it/icvss2023/Home">ICVSS 2023, Poster Session (Sampieri, Italy) </a> - Vid2Seq project - <a href="slides/vid2seq-cvpr-poster.pdf"> [Poster] </a> </li>
                <li>07/2023 - <a href="https://imagine-lab.enpc.fr/">Imagine ENPC Seminar (Champs-sur-Marne, France), 1-min Highlight </a> - Vid2Seq project
                <li>06/2023 - <a href="https://cvpr2023.thecvf.com/">CVPR 2023, Poster Session (Vancouver, British Columbia) </a> - Vid2Seq project - <a href="https://www.youtube.com/watch?v=hXP-2fYzq4g&ab_channel=AntoineYang">[8 min Video] </a> - <a href="slides/vid2seq-cvpr.pdf"> [Slides] </a> - <a href="slides/vid2seq-cvpr-poster.pdf"> [Poster] </a> </li>
                <li>05/2023 - <a href="https://ellis.eu/">ELLIS computer vision workshop, Spotlight and Poster Session (Metzingen, Germany) </a> - Vid2Seq project - <a href="slides/vid2seq-cvpr-poster.pdf"> [Poster] </a> </li>
                <li>02/2023 - <a href="https://research.google/teams/perception/">Google Perception Spotlights, 5-min Talk (Virtual) </a> - Vid2Seq project </li>
                <li>01/2023 - <a href="https://prairie-institute.fr/">PRAIRIE workshop, Poster Session (Paris, France) </a> - FrozenBiLM project - <a href="slides/frozenbilm-neurips-poster.pdf"> [Poster] </a> </li>
                <li>11/2022 - <a href="https://nips.cc/">NeurIPS 2022, Poster Session (New Orleans, Louisiana) </a> - FrozenBiLM project - <a href="https://www.youtube.com/watch?v=dedoSjAiVL4&ab_channel=AntoineYang">[5 min Video] </a> - <a href="slides/frozenbilm-neurips.pdf"> [Slides] </a> - <a href="slides/frozenbilm-neurips-poster.pdf"> [Poster] </a> </li>
                <li>11/2022 - <a href="https://scai.sorbonne-universite.fr/public/events/view/7754b3ff1feea83b10d5/6">NeurIPS@Paris 2022, 6-min Talk (Paris, France) </a> - FrozenBiLM project - <a href="slides/frozenbilm-neurips.pdf"> [Slides] </a> - <a href="slides/frozenbilm-neurips-poster.pdf"> [Poster] </a> </li>
                <li>10/2022 - <a href="https://www.di.ens.fr/willow/">Seminar of Inria Willow and Sierra teams, Poster Session (Saint-Raphaël, France) </a> - FrozenBiLM project - <a href="slides/frozenbilm-neurips-poster.pdf"> [Poster] </a> </li>
                <li>06/2022 - <a href="https://cvpr2022.thecvf.com/">CVPR 2022, Oral and Poster Session (New Orleans, Louisiana) </a> - TubeDETR project - <a href="https://www.youtube.com/watch?v=FT_50ylQMNs&ab_channel=AntoineYang">[5 min Video] </a> - <a href="slides/tubedetr-cvpr.pdf"> [Slides] </a> - <a href="slides/tubedetr-cvpr-poster.pdf"> [Poster] </a> </li>
                <li>06/2022 - <a href="https://www.di.ens.fr/"> Seminar of the Computer Science department of &Eacute;cole Normale Sup&eacute;rieure, 15-min Talk (Mûr-de-Bretagne, France) </a> - FrozenBiLM project - <a href="slides/frozenbilm-diens.pdf"> [Slides] </a> </li>
                <li>10/2021 - <a href="https://www.di.ens.fr/willow/">Seminar of Inria Willow and Sierra teams, Poster Session (Avignon, France) </a> - Just Ask project -  <a href="slides/just-ask-iccv-poster.pdf"> [Poster] </a> </li>
                <li>10/2021 - <a href="https://iccv2021.thecvf.com/home">ICCV 2021, Oral and Poster Session (Virtual) </a> - Just Ask project - <a href="https://www.youtube.com/watch?v=eyEW5Z23Ofs&ab_channel=AntoineYang">[12 min Video] </a> - <a href="slides/just-ask-iccv.pdf"> [Slides] </a> - <a href="slides/just-ask-iccv-poster.pdf"> [Poster] </a> </li>
                <li>06/2021 - <a href="https://holistic-video-understanding.github.io/workshops/cvpr2021.html">CVPR 2021 Holistic Video Understanding Workshop, 10-min Invited Talk (Virtual) </a> - Just Ask project - <a href="slides/just-ask-hvu-cvpr21.pdf"> [Slides] </a> - <a href="https://youtu.be/jzXdRT5W3C4?t=17280"> [Recording] </a> </li>
                <li>05/2021 - <a href="https://www.rocq.inria.fr/semdoc/">Inria Junior Seminar, 30-min Talk (Virtual) </a> - Just Ask project - <a href="https://www.rocq.inria.fr/semdoc/Presentations/just-ask-junior-seminar.pdf"> [Slides] </a> - <a href="https://webconf.math.cnrs.fr/playback/presentation/2.0/playback.html?meetingId=38b91f46f61e82195e2275670f5d26218e2f9bae-1621340178022"> [Recording] </a> </li>
                <li>09/2020 - <a href="https://www.master-mva.com/">MVA Master Defense, Inria Internship 20-min Presentation (Paris, France) </a> - Video Question Answering </li>
                <li>04/2020 - <a href="https://iclr.cc/virtual_2020/poster_HygrdpVKvr.html">ICLR 2020, Poster Session (Virtual) </a> - NAS evaluation project - <a href="https://www.youtube.com/watch?v=jB7vrGR_4ak&ab_channel=AntoineYang">[5 min Video] </a> - <a href="slides/nasefh-iclr20.pdf"> [Slides] </a></li>
                <li>09/2019 - <a href="https://www.polytechnique.edu/">Ecole Polytechnique Master Defense, Huawei Internship 40-min Presentation (Palaiseau, France) </a> - NAS evaluation project </li>
            </ul>
          </div>
        </div>

          <hr>

      <!-- Teaching -->
        <div class="row" id="teaching" style="padding-top:30px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>Teaching</h2>
            <!--
            I also regularly give private lessons, notably for undergraduate students in preparatory classes in mathematics.
            -->
            <!--
            <ul>
                <li>Fall 2022 - Object Recognition and Computer Vision, Teacher Assistant - Master level (MVA) - 40 hours - ENS Paris-Saclay </li>
                <li>Fall 2021 - Object Recognition and Computer Vision, Project advisor - Master level (MVA) - Volunteering - ENS Paris-Saclay </li>
                <li>2021-2022 - Mathematics, Oral examiner - Undergraduate level (MPSI, MP and MP<sup>*</sup>) - 80 hours - Lycée Marcelin Berthelot </li>
                <li>Spring 2021 - Differential equations, Teacher assistant - Undergraduate level (L2) - 38 hours - Sorbonne Université </li>
                <li>Fall 2020 - Object Recognition and Computer Vision, Project advisor - Master level (MVA) - Volunteering - ENS Paris-Saclay </li>
                <li>2019-2020 - Mathematics, Oral examiner - Undergraduate level (MPSI) - 60 hours - Lycée Marcelin Berthelot </li>
                <li>Fall 2019 - Functional programming, Tutor - Undergraduate level (BSc) - 24 hours - École Polytechnique </li>
                <li>2017-2018 - Mathematics, Oral examiner - Undergraduate level (MPSI) - 60 hours - Lycée Marcelin Berthelot </li>
                <li>2017-2018 - Mathematics, Oral examiner - Undergraduate level (MPSI) - 60 hours - Lycée Marcelin Berthelot </li>

            </ul>
            -->

          <!-- PSL 2023 -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Spring 2023</span>
              </div>
              <div class="col-sm-11 col-md-11">
               &nbsp; Introduction to computer vision, Teacher Assistant - Master level - 3 hours - Dauphine-PSL University
              </div>
            </div>

            <!-- ORCV 2022 -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Fall&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2022</span>
              </div>
              <div class="col-sm-11 col-md-11">
               &nbsp; Object Recognition and Computer Vision, Teacher Assistant - Master level (MVA) - 50 hours - ENS Paris-Saclay
              </div>
            </div>

            <!-- PSL 2022 -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Spring 2022</span>
              </div>
              <div class="col-sm-11 col-md-11">
               &nbsp; Introduction to computer vision, Teacher Assistant - Master level - 3 hours - Dauphine-PSL University
              </div>
            </div>

              <!-- ORCV 2021 -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Fall&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2021</span>
              </div>
              <div class="col-sm-11 col-md-11">
               &nbsp; Object Recognition and Computer Vision, Project advisor - Master level (MVA) - Volunteering - ENS Paris-Saclay
              </div>
            </div>

              <!-- Colles PhD2 -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">2021 &nbsp;-&nbsp;2022</span>
              </div>
              <div class="col-sm-11 col-md-11">
                  &nbsp; Mathematics, Oral examiner - Undergraduate level (MPSI, MP and MP<sup>*</sup>) - 80 hours - Lycée Marcelin Berthelot
              </div>
            </div>

              <!-- Equa diff PhD1 -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Spring 2021</span>
              </div>
              <div class="col-sm-11 col-md-11">
               &nbsp; Differential equations, Teacher assistant - Undergraduate level (L2) - 38 hours - Sorbonne Université
              </div>
            </div>

              <!-- ORCV 2020 -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Fall&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2020</span>
              </div>
              <div class="col-sm-11 col-md-11">
               &nbsp; Object Recognition and Computer Vision, Project advisor - Master level (MVA) - Volunteering - ENS Paris-Saclay
              </div>
            </div>

              <!-- Colles 4A -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">2019 &nbsp;-&nbsp;2020</span>
              </div>
              <div class="col-sm-11 col-md-11">
               &nbsp; Mathematics, Oral examiner - Undergraduate level (MPSI) - 60 hours - Lycée Marcelin Berthelot
              </div>
            </div>

              <!-- Tutor bachelors -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Fall&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2019</span>
              </div>
              <div class="col-sm-11 col-md-11">
                &nbsp; Functional programming, Tutor - Undergraduate level (BSc) - 24 hours - École Polytechnique
              </div>
            </div>

              <!-- Colles 2A -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">2017 &nbsp;-&nbsp; 2018</span>
              </div>
              <div class="col-sm-11 col-md-11">
                &nbsp; Mathematics, Oral examiner - Undergraduate level (MPSI) - 60 hours - Lycée Marcelin Berthelot
              </div>
            </div>

              <!-- FH Internship -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Spring&nbsp;&nbsp;2017 </span>
              </div>
              <div class="col-sm-11 col-md-11">
                &nbsp; Multidisciplinary support, Socio-educational facilitator intern - Middle school - Collège Saint-Charles
              </div>
            </div>

          </div>
        </div> <!-- end of teaching -->

          <hr>

          <!-- Misc -->
        <div class="row" id="misc" style="padding-top:30px; margin-top:-60px;">
            <div class="col-md-12">
            <h2>Misc.</h2>
                <p>I am a reviewer for CVPR 2022, ECCV 2022, CVPR 2023, IJCV 2023, ICCV 2023, TPAMI 2023, NeurIPS 2023, and ICML 2024.</p>

                <p>Besides research, my passions include running, hiking and travels.</p>
            </div>
        </div> <!-- end of misc -->

      <hr>

      <div class="container">
        <footer>
          <p align="right"><small>Copyright © Antoine Yang &nbsp;/&nbsp; Last update: February 2024 </small></p>
        </footer>
        <div style="height:10px;"></div>
      </div>

      <!-- Bootstrap core JavaScript -->
      <!-- Placed at the end of the document so the pages load faster -->
      <script src="jq/jquery-1.11.1.min.js"></script>
      <script src="js/bootstrap.min.js"></script>
      <script src="js/docs.min.js"></script>

      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-75922772-1', 'auto');
        ga('send', 'pageview');

      </script>
      </div>
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    </body>
</html>
