<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="google-site-verification" content="FeA8wZzKiI6DkLLqCJMlBhKi6PsRnR1Dmdc2TDgTYcI" />
        <title>Antoine Yang</title>

        <link rel='icon' href='img/favicon.ico' type='image/x-icon'/>
        <link href="./css/bootstrap.min.css" rel="stylesheet">
        <link rel="stylesheet" href="./assets/academicons-1.7.0/css/academicons.css"/>
        <link rel="stylesheet" href="./assets/font-awesome-4.7.0/css/font-awesome.min.css"/>
	<script async defer src="https://buttons.github.io/buttons.js"></script>
    </head>

    <body>

      <!-- Navigation bar -->
      <div class="navbar navbar-default  navbar-fixed-top bg-info">
        <div class="container">
          <div class="navbar-header">

            <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
          </div>
          <div class="navbar-collapse collapse" id="navbar-main">

            <ul class="nav navbar-nav navbar-left">
              <li ><a href="#home">Home</a></li>
                <li ><a href="#news">News</a></li>
              <li ><a href="#research">Research</a></li>
                <li ><a href="#talks">Talks</a></li>
                <li ><a href="#teaching">Teaching</a></li>
                <li ><a href="#misc">Misc</a></li>
            </ul>
          </div>
        </div>
      </div>

      <!-- end of navigation bar -->
      <div style="height:40px;" id="home"></div>
      <div style="height:40px;"></div>

      <!-- CONTENTS -->
      <div class="container">
        <!-- Aboutme -->
        <div class="row" >
          <div class="col-xs-6 col-sm-4 col-md-2">
            <a class="thumbnail">
              <img src="./img/antoine.jpg" alt="Antoine Yang" class="img-rounded">
            </a>
          </div>

          <div class="col-xs-10 col-sm-6 col-md-4">
            <h1 class="text-info">Antoine Yang</h1>
            <h4 class="text-info">PhD student, Inria</h4>
            <h5>
              <a href="mailto:antoine.yang@inria.fr" class="text-info" title="e-Mail"><i class="fa fa-envelope-square fa-2x"></i></a>
	      <a href="https://scholar.google.com/citations?user=SlMKN6IAAAAJ" class="text-info" title="Google Scholar"><i class="ai ai-google-scholar-square ai-2x"></i></a>
              <a href="https://github.com/antoyang" class="text-info" title="GitHub"><i class="fa fa-github-square fa-2x"></i></a>
              <a href="https://www.linkedin.com/in/antoyang/" class="text-info" title="LinkedIn"><i class="fa fa-linkedin-square fa-2x"></i></a>
              <a href="https://twitter.com/AntoineYang2" class="text-info" title="Twitter"><i class="fa fa-twitter-square fa-2x"></i></a>
                <a href="https://www.youtube.com/channel/UCYJ4NEILh5DeYf-62eb1RBg" class="text-info" title="Twitter"><i class="fa fa-youtube-square fa-2x"></i></a>
		  </h5>
          </div>
        </div> <!-- end of Aboutme -->

         <p align="justify">
		 I am a student researcher at <a href="https://research.google/">Google</a>, and a PhD student in the <a href="http://www.di.ens.fr/willow">WILLOW team</a> of <a href="http://www.inria.fr/">Inria</a> and <a href="http://www.ens.fr">&Eacute;cole Normale Sup&eacute;rieure</a>, advised by <a href="https://antoine77340.github.io/">Antoine Miech</a>, <a href="http://people.ciirc.cvut.cz/~sivic/">Josef Sivic</a>, <a href="http://www.di.ens.fr/~laptev">Ivan Laptev</a> and <a href="https://www.di.ens.fr/willow/people_webpages/cordelia/">Cordelia Schmid</a>.
             My current research is focused on learning multi-modal video representations using vision and language.
             I received an <a href="https://programmes.polytechnique.edu/cycle-ingenieur-polytechnicien/cycle-ingenieur-polytechnicien"> engineering degree </a> from <a href="https://www.polytechnique.edu/">École Polytechnique </a> and a MSc degree in <a href="https://www.master-mva.com/">Mathematics, Vision and Learning</a> from <a href="http://ens-paris-saclay.fr/">ENS Paris-Saclay</a> in 2020.
             See my <a href="https://www.linkedin.com/in/antoyang/">LinkedIn profile</a> for a full resume.
         </p>
        <hr>

           <!-- News header-->
        <div class="row" id="news" style="padding-top:30px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>News</h2>
          </div>
        </div>
        <!-- End news header -->
	      
    <!-- FrozenBiLM accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">09 / 2022</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="frozenbilm.html">FrozenBiLM</a> is accepted at <a href="https://nips.cc/Conferences/2022">NeurIPS 2022</a>!
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Google internship -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">06 / 2022</span>
          </div>
          <div class="col-sm-11 col-md-11">
              I am starting a 6-month research internship at <a href="https://research.google/">Google Research</a> (<a href="https://research.google/teams/perception/">Perception team</a>) in Grenoble.
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Just Ask extension accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">04 / 2022</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="just-ask.html">Just Ask extension</a> is accepted at the <a href="https://www.computer.org/csdl/journal/tp">TPAMI Special Issue on the Best Papers of ICCV 2021</a>!
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- TubeDETR accepted https://flatui.colorion.co/palette/87 -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">03 / 2022</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="tubedetr.html">TubeDETR</a> is accepted at <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a> as an oral!
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Just Ask accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">07 / 2021</span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="just-ask.html">Just Ask</a> is accepted at <a href="http://iccv2021.thecvf.com/home">ICCV 2021</a> as an oral!
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Just Ask - HVU - CVPR21 Talk -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">06 / 2021</span>
          </div>
          <div class="col-sm-11 col-md-11">
              I gave talks about <a href="just-ask.html">Just Ask</a> at the <a href="https://holistic-video-understanding.github.io/workshops/cvpr2021.html"> CVPR 2021 Holistic Video Understanding Workshop</a> and at the <a href="https://www.rocq.inria.fr/semdoc/">Inria Junior Seminar</a>.
          </div>
        </div>
        <div style="height:3px;"></div>

	<!-- Willow PhD -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">09 / 2020</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I am starting my PhD at <a href="http://www.di.ens.fr/willow">Inria WILLOW</a> in Paris.
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- MVA degree -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">09 / 2020</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I have received a <a href="https://www.master-mva.com/"> MSc degree </a> with highest honors and jury congratulations from <a href="https://ens-paris-saclay.fr/">ENS Paris-Saclay</a>.
          </div>
        </div>
        <div style="height:3px;"></div>

	<!-- Willow internship -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">04 / 2020</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I am starting a 5-month research internship at <a href="http://www.di.ens.fr/willow">Inria WILLOW</a> in Paris.
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- NASEFH accepted -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-success">12 &nbsp;/ 2019 </span>
          </div>
          <div class="col-sm-11 col-md-11">
              <a href="nasefh.html">NAS evaluation is frustratingly hard</a> is accepted at <a href="https://iclr.cc/Conferences/2020">ICLR 2020</a>!
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- X degree -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">09 / 2019</span>
          </div>
          <div class="col-sm-11 col-md-11">
            I have received an <a href="https://programmes.polytechnique.edu/cycle-ingenieur-polytechnicien/cycle-ingenieur-polytechnicien"> engineering degree </a> from <a href="https://www.polytechnique.edu/">École Polytechnique</a>.
          </div>
        </div>
        <div style="height:3px;"></div>

    <!-- Huawei internship -->
        <div class="row">
          <div class="col-sm-1 col-md-1">
            <span class="label label-info">04 / 2019</span>
          </div>
          <div class="col-sm-11 col-md-11">
              I am starting a 5-month research internship at <a href="http://www.noahlab.com.hk/">Huawei Noah's Ark Lab</a> (<a href="http://dev3.noahlab.com.hk/research.html">AI Theory team</a>) in London.
          </div>
        </div>
        <div style="height:3px;"></div>

        <!-- end of news -->
        <hr>

        <!-- Research -->
        <div class="row"  id="research" style="padding-top:30px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>Research</h2>
              <p>See my <a href="https://scholar.google.com/citations?user=SlMKN6IAAAAJ">Google Scholar </a> and <a href="https://github.com/antoyang"> GitHub </a> profiles for more information.</p>

<!-- FrozenBiLM, NeurIPS 2022 -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4">
		<a class="thumbnail">
		<img src="./img/frozenbilm.png" alt="Zero-Shot Video Question Answering via Frozen Bidirectional Language Models">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <!-- <font color="red">[New]</font> -->
          <strong>Zero-Shot Video Question Answering via Frozen Bidirectional Language Models</strong> </br>
	<u>Antoine Yang</u>, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid<br>
          NeurIPS 2022 <br>
          <a href="https://arxiv.org/pdf/2206.08155.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	<a href="frozenbilm.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex6">bibtex</button>
	<div id="bibtex6" class="collapse">
	  <pre><tt>@inproceedings{yang2022frozenbilm,
title = {Zero-Shot Video Question Answering via Frozen Bidirectional Language Models},
author = {Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
booktitle={Advances in Neural Information Processing Systems}
year = {2022},
}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract6">abstract</button>
	<div id="abstract6" class="collapse">
        <p style="text-align: justify;">
	        Video question answering (VideoQA) is a complex task that requires diverse multi-modal data for training.
            Manual annotation of question and answers for videos, however, is tedious and prohibits scalability.
            To tackle this problem, recent methods consider zero-shot settings with no manual annotation of visual question-answer.
            In particular, a promising approach adapts frozen autoregressive language models pretrained on Web-scale text-only data to multi-modal inputs.
            In contrast, we here build on frozen bidirectional language models (BiLM) and show that such an approach provides a stronger and cheaper alternative for zero-shot VideoQA.
            In particular, (i) we combine visual inputs with the frozen BiLM using light trainable modules,
            (ii) we train such modules using Web-scraped multi-modal data, and finally
            (iii) we perform zero-shot VideoQA inference through masked language modeling, where the masked text is the answer to a given question.
            Our proposed approach, FrozenBiLM, outperforms the state of the art in zero-shot VideoQA by a significant margin on a variety of datasets, including LSMDC-FiB, iVQA, MSRVTT-QA, MSVD-QA, ActivityNet-QA, TGIF-FrameQA, How2QA and TVQA.
            It also demonstrates competitive performance in the few-shot and fully-supervised setting.
        </p>
        </div>
          <a href="https://github.com/antoyang/FrozenBiLM"><button type="button" class="btn btn-primary btn-xs">code</button></a>
      </div>
    </div>
<!-- end of FrozenBiLM -->

<!-- Just Ask extension, TPAMI -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4">
		<a class="thumbnail">
		<img src="./img/justaskextension.png" alt="Learning to Answer Visual Questions from Web Videos">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <!-- <font color="red">[New]</font> -->
          <strong>Learning to Answer Visual Questions from Web Videos</strong> (journal extension of <strong>Just Ask</strong>) </br>
	<u>Antoine Yang</u>, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid<br>
          To appear in TPAMI (Special Issue on the Best Papers of ICCV 2021) <br>
          <a href="https://arxiv.org/pdf/2205.05019v2.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	<a href="just-ask.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex5">bibtex</button>
	<div id="bibtex5" class="collapse">
	  <pre><tt>@article{yang2022learningta,
title={Learning to Answer Visual Questions from Web Videos},
author={Antoine Yang and Antoine Miech and Josef Sivic and Ivan Laptev and Cordelia Schmid},
journal={IEEE transactions on pattern analysis and machine intelligence},
year={2022},
volume={PP}
}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract5">abstract</button>
	<div id="abstract5" class="collapse">
        <p style="text-align: justify;">
	        Recent methods for visual question answering rely on large-scale annotated datasets.
            Manual annotation of questions and answers for videos, however, is tedious, expensive and prevents scalability.
            In this work, we propose to avoid manual annotation and generate a large-scale training dataset for video question answering making use of automatic cross-modal supervision.
            We leverage a question generation transformer trained on text data and use it to generate question-answer pairs from transcribed video narrations.
            Given narrated videos, we then automatically generate the HowToVQA69M dataset with 69M video-question-answer triplets.
            To handle the open vocabulary of diverse answers in this dataset, we propose a training procedure based on a contrastive loss between a video-question multi-modal transformer and an answer transformer.
            We introduce the zero-shot VideoQA task and the VideoQA feature probe evaluation setting and show excellent results.
            Furthermore, our method achieves competitive results on MSRVTT-QA, ActivityNet-QA, MSVD-QA and How2QA datasets.
            We also show that our approach generalizes to another source of web video and text data.
            We generate the WebVidVQA3M dataset from videos with alt-text annotations, and show its benefits for training VideoQA models.
            Finally, for a detailed evaluation we introduce iVQA, a new VideoQA dataset with reduced language bias and high-quality manual annotations.
	</p>
        </div>
          <a href="https://github.com/antoyang/just-ask"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          <a href="http://videoqa.paris.inria.fr/"><button type="button" class="btn btn-primary btn-xs">demo</button></a>
      </div>
    </div>
<!-- end of Just Ask extension -->

<!-- TubeDETR, CVPR 2022 -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4">
		<a class="thumbnail">
		<img src="./img/tubedetr.png" alt="TubeDETR: Spatio-Temporal Video Grounding with Transformers">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <!-- <font color="red">[New]</font> -->
          <strong>TubeDETR: Spatio-Temporal Video Grounding with Transformers</strong> </br>
	<u>Antoine Yang</u>, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid<br>
          CVPR 2022 (<strong><font color="red">oral</font></strong>: top 4% submissions) <br>
          <a href="https://arxiv.org/pdf/2203.16434.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	<a href="tubedetr.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex4">bibtex</button>
	<div id="bibtex4" class="collapse">
	  <pre><tt>@inproceedings{yang2022tubedetr,
author    = {Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
title     = {TubeDETR: Spatio-Temporal Video Grounding With Transformers},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
year      = {2022},
pages     = {16442-16453}}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract4">abstract</button>
	<div id="abstract4" class="collapse">
        <p style="text-align: justify;">
	        We consider the problem of localizing a spatio-temporal tube in a video corresponding to a given text query.
            This is a challenging task that requires the joint and efficient modeling of temporal, spatial and multi-modal interactions.
            To address this task, we propose TubeDETR, a transformer-based architecture inspired by the recent success of such models for text-conditioned object detection.
            Our model notably includes: (i) an efficient video and text encoder that models spatial multi-modal interactions over sparsely sampled frames and
            (ii) a space-time decoder that jointly performs spatio-temporal localization.
            We demonstrate the advantage of our proposed components through an extensive ablation study.
            We also evaluate our full approach on the spatio-temporal video grounding task and demonstrate improvements over the state of the art on the challenging VidSTG and HC-STVG benchmarks.
	</p>
        </div>
          <a href="https://github.com/antoyang/TubeDETR"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          <a href="http://stvg.paris.inria.fr/"><button type="button" class="btn btn-primary btn-xs">demo</button></a>
      </div>
    </div>
<!-- end of TubeDETR -->

<!-- Just Ask, ICCV21 -->
    <div class="row">
	<div class="col-xs-10 col-sm-4 col-md-4">
		<a class="thumbnail">
		<img src="./img/justask.PNG" alt="Just Ask: Learning to Answer Questions from Millions of Narrated Videos">
                </a>
	</div>
      <div class="col-xs-12 col-sm-8 col-md-8">
          <!-- <font color="red">[New]</font> -->
          <strong>Just Ask: Learning to Answer Questions from Millions of Narrated Videos</strong> </br>
	<u>Antoine Yang</u>, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid<br>
          ICCV 2021 (<strong><font color="red">oral</font></strong>: top 3% submissions) <br>
          <a href="https://arxiv.org/pdf/2012.00451.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
	<a href="just-ask.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
          <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex3">bibtex</button>
	<div id="bibtex3" class="collapse">
	  <pre><tt>@inproceedings{yang2021justask,
title={Just ask: Learning to answer questions from millions of narrated videos},
author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
pages={1686--1697},
year={2021}}</tt></pre>
	</div>
	<button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract3">abstract</button>
	<div id="abstract3" class="collapse">
        <p style="text-align: justify;">
	        Recent methods for visual question answering rely on large-scale annotated datasets.
            Manual annotation of questions and answers for videos, however, is tedious, expensive and prevents scalability.
            In this work, we propose to avoid manual annotation and generate a large-scale training dataset for video question answering making use of automatic cross-modal supervision.
            We leverage a question generation transformer trained on text data and use it to generate question-answer pairs from transcribed video narrations.
            Given narrated videos, we then automatically generate the HowToVQA69M dataset with 69M video-question-answer triplets.
            To handle the open vocabulary of diverse answers in this dataset, we propose a training procedure based on a contrastive loss between a video-question multi-modal transformer and an answer transformer.
            We introduce the zero-shot VideoQA task and show excellent results, in particular for rare answers.
            Furthermore, we demonstrate our method to significantly outperform the state of the art on MSRVTT-QA, MSVD-QA, ActivityNet-QA and How2QA.
            Finally, for a detailed evaluation we introduce iVQA, a new VideoQA dataset with reduced language biases and high-quality redundant manual annotations.
	</p>
        </div>
          <a href="https://github.com/antoyang/just-ask"><button type="button" class="btn btn-primary btn-xs">code</button></a>
          <a href="http://videoqa.paris.inria.fr/"><button type="button" class="btn btn-primary btn-xs">demo</button></a>
      </div>
    </div>
<!-- end of Just Ask -->

<!-- NASEFH, ICLR20 -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="./img/nasefh.PNG" alt="NAS evaluation is frustratingly hard">
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong>NAS evaluation is frustratingly hard</strong><br>
		<u>Antoine Yang</u>, Pedro M. Esperança, and Fabio Maria Carlucci<br>
                ICLR 2020<br>
                  <a href="https://arxiv.org/pdf/1912.12522.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
                  <a href="nasefh.html"><button type="button" class="btn btn-primary btn-xs">project page</button></a>
                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex2">bibtex</button>
                <div id="bibtex2" class="collapse">
                  <pre><tt>@inproceedings{yang2020nasefh,
title={NAS evaluation is frustratingly hard},
author={Antoine Yang and Pedro M. Esperança and Fabio M. Carlucci},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HygrdpVKvr}}</tt></pre>
                </div>
                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract2">abstract</button>
                <div id="abstract2" class="collapse">
		  <div>
              Neural Architecture Search (NAS) is an exciting new field which promises to be as much as a game-changer as Convolutional Neural Networks were in 2012.
              Despite many great works leading to substantial improvements on a variety of tasks, comparison between different methods is still very much an open issue.
              While most algorithms are tested on the same datasets, there is no shared experimental protocol followed by all.
              As such, and due to the under-use of ablation studies, there is a lack of clarity regarding why certain methods are more effective than others. Our first contribution is a benchmark of 8 NAS methods on 5 datasets.
              To overcome the hurdle of comparing methods with different search spaces, we propose using a method’s relative improvement over the randomly sampled average architecture, which effectively removes advantages arising from expertly engineered search spaces or training protocols.
              Surprisingly, we find that many NAS techniques struggle to significantly beat the average architecture baseline. We perform further experiments with the commonly used DARTS search space in order to understand the contribution of each component in the NAS pipeline.
              These experiments highlight that: (i) the use of tricks in the evaluation protocol has a predominant impact on the reported performance of architectures; (ii) the cell-based search space has a very narrow accuracy range, such that the seed has a considerable impact on architecture rankings; (iii) the hand-designed macrostructure (cells) is more important than the searched micro-structure (operations); and (iv) the depth-gap is a real phenomenon, evidenced by the change in rankings between 8 and 20 cell architectures.
              To conclude, we suggest best practices, that we hope will prove useful for the community and help mitigate current NAS pitfalls, e.g. difficulties in reproducibility and comparison of search methods.
                  </div>
                </div>
                  <a href="https://github.com/antoyang/NAS-Benchmark"><button type="button" class="btn btn-primary btn-xs">code</button></a>
              </div>
            </div>
<!-- end of NASEFH -->

<!-- MANAS -->
            <div class="row">
              <div class="col-xs-10 col-sm-4 col-md-4">
                <a class="thumbnail">
                  <img src="./img/manas.PNG" style="height:200px;" alt="MANAS">
                </a>
              </div>
              <div class="col-xs-12 col-sm-8 col-md-8">
                <strong>MANAS: Multi-Agent Neural Architecture Search</strong><br>
		Fabio Maria Carlucci, Pedro M. Esperança, Marco Singh, Victor Gabillon, <u>Antoine Yang</u>, Hang Xu, Zewei Chen and Jun Wang<br>
                arXiv 2019 <br>
                  <a href="https://arxiv.org/pdf/1909.01051.pdf"><button type="button" class="btn btn-primary btn-xs">pdf</button></a>
                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#bibtex1">bibtex</button>
                <div id="bibtex1" class="collapse">
                  <pre><tt>@article{carlucci2019manas,
  title={MANAS: multi-agent neural architecture search},
  author={Carlucci, Fabio Maria and Esperan{\c{c}}a, Pedro M and Singh, Marco and Gabillon, Victor and Yang, Antoine and Xu, Hang and Chen, Zewei and Wang, Jun},
  journal={arXiv preprint arXiv:1909.01051},
  year={2019}
}</tt></pre>
                </div>
                <button type="button" class="btn btn-primary btn-xs" data-toggle="collapse" data-target="#abstract1">abstract</button>
                <div id="abstract1" class="collapse">
		  <div>
              The Neural Architecture Search (NAS) problem is typically formulated as a graph search problem where the goal is to learn the optimal operations over edges in order to maximise a graph-level global objective.
              Due to the large architecture parameter space, efficiency is a key bottleneck preventing NAS from its practical use.
              In this paper, we address the issue by framing NAS as a multi-agent problem where agents control a subset of the network and coordinate to reach optimal architectures.
              We provide two distinct lightweight implementations, with reduced memory requirements (1/8th of state-of-the-art), and performances above those of much more computationally expensive methods.
              Theoretically, we demonstrate vanishing regrets of the form O(sqrt(T)), with T being the total number of rounds.
              Finally, aware that random search is an, often ignored, effective baseline we perform additional experiments on 3 alternative datasets and 2 network configurations, and achieve favourable results in comparison.
                  </div>
                </div>
              </div>
            </div>
<!-- end of MANAS -->

          </div>
        </div> <!-- end of projects -->

          <hr>

      <!-- Talks -->
        <div class="row" id="talks" style="padding-top:30px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>Talks</h2>
            <ul>
                <li>11/2022 - <a href="https://nips.cc/">NeurIPS 2022, Poster Session (New Orleans, Louisiana) </a> - Zero-Shot Video Question Answering via Frozen Bidirectional Language Models - <a href="https://www.youtube.com/watch?v=dedoSjAiVL4&ab_channel=AntoineYang">[5 min Video] </a> - <a href="slides/frozenbilm-neurips.pdf"> [Slides] </a> - <a href="slides/frozenbilm-neurips-poster.pdf"> [Poster] </a> </li>
                <li>11/2022 - <a href="https://scai.sorbonne-universite.fr/public/events/view/7754b3ff1feea83b10d5/6">NeurIPS@Paris 2022, 6-min Talk (Paris, France) </a> - Zero-Shot Video Question Answering via Frozen Bidirectional Language Models - <a href="slides/frozenbilm-neurips.pdf"> [Slides] </a> - <a href="slides/frozenbilm-neurips-poster.pdf"> [Poster] </a> </li>
                <!--<li>10/2022 - <a href="https://www.di.ens.fr/willow/">Seminar of Inria Willow and Sierra teams, Poster Session (Saint-Raphaël, France) </a> - Zero-Shot Video Question Answering via Frozen Bidirectional Language Models - <a href="slides/frozenbilm-neurips-poster.pdf"> [Poster] </a> </li>-->
                <li>06/2022 - <a href="https://cvpr2022.thecvf.com/">CVPR 2022, Oral and Poster Session (New Orleans, Louisiana) </a> - TubeDETR: Spatio-Temporal Video Grounding with Transformers - <a href="https://www.youtube.com/watch?v=FT_50ylQMNs&ab_channel=AntoineYang">[5 min Video] </a> - <a href="slides/tubedetr-cvpr.pdf"> [Slides] </a> - <a href="slides/tubedetr-cvpr-poster.pdf"> [Poster] </a> </li>
                <li>06/2022 - <a href="https://www.di.ens.fr/"> Seminar of the Computer Science department of &Eacute;cole Normale Sup&eacute;rieure, 15-min Talk (Mûr-de-Bretagne, France) </a> - Zero-Shot Video Question Answering - <a href="slides/frozenbilm-diens.pdf"> [Slides] </a> </li>
                <!--<li>10/2021 - <a href="https://www.di.ens.fr/willow/">Seminar of Inria Willow and Sierra teams, Poster Session (Avignon, France) </a> - Just Ask: Learning to Answer Questions from Millions of Narrated Videos -  <a href="slides/just-ask-iccv-poster.pdf"> [Poster] </a> </li>-->
                <li>10/2021 - <a href="https://iccv2021.thecvf.com/home">ICCV 2021, Oral and Poster Session (Virtual) </a> - Just Ask: Learning to Answer Questions from Millions of Narrated Videos - <a href="https://www.youtube.com/watch?v=eyEW5Z23Ofs&ab_channel=AntoineYang">[12 min Video] </a> - <a href="slides/just-ask-iccv.pdf"> [Slides] </a> - <a href="slides/just-ask-iccv-poster.pdf"> [Poster] </a> </li>
                <li>06/2021 - <a href="https://holistic-video-understanding.github.io/workshops/cvpr2021.html">CVPR 2021 Holistic Video Understanding Workshop, 10-min Invited Talk (Virtual) </a> - Just Ask: Learning to Answer Questions from Millions of Narrated Videos - <a href="slides/just-ask-hvu-cvpr21.pdf"> [Slides] </a> - <a href="https://youtu.be/jzXdRT5W3C4?t=17280"> [Recording] </a> </li>
                <li>05/2021 - <a href="https://www.rocq.inria.fr/semdoc/">Inria Junior Seminar, 30-min Talk (Virtual) </a> - Just Ask: Learning to Answer Questions from Millions of Narrated Videos - <a href="https://www.rocq.inria.fr/semdoc/Presentations/just-ask-junior-seminar.pdf"> [Slides] </a> - <a href="https://webconf.math.cnrs.fr/playback/presentation/2.0/playback.html?meetingId=38b91f46f61e82195e2275670f5d26218e2f9bae-1621340178022"> [Recording] </a> </li>
                <li>04/2020 - <a href="https://iclr.cc/virtual_2020/poster_HygrdpVKvr.html">ICLR 2020, Poster Session (Virtual) </a> - NAS evaluation is frustratingly hard - <a href="https://www.youtube.com/watch?v=jB7vrGR_4ak&ab_channel=AntoineYang">[5 min Video] </a> - <a href="slides/nasefh-iclr20.pdf"> [Slides] </a></li>
            </ul>
          </div>
        </div>

          <hr>

      <!-- Teaching -->
        <div class="row" id="teaching" style="padding-top:30px; margin-top:-60px;">
          <div class="col-md-12">
            <h2>Teaching</h2>
            <!--
            I also regularly give private lessons, notably for undergraduate students in preparatory classes in mathematics.
            -->
            <!--
            <ul>
                <li>Fall 2022 - Object Recognition and Computer Vision, Teacher Assistant - Master level (MVA) - 40 hours - ENS Paris-Saclay </li>
                <li>Fall 2021 - Object Recognition and Computer Vision, Project advisor - Master level (MVA) - Volunteering - ENS Paris-Saclay </li>
                <li>2021-2022 - Mathematics, Oral examiner - Undergraduate level (MPSI, MP and MP<sup>*</sup>) - 80 hours - Lycée Marcelin Berthelot </li>
                <li>Spring 2021 - Differential equations, Teacher assistant - Undergraduate level (L2) - 38 hours - Sorbonne Université </li>
                <li>Fall 2020 - Object Recognition and Computer Vision, Project advisor - Master level (MVA) - Volunteering - ENS Paris-Saclay </li>
                <li>2019-2020 - Mathematics, Oral examiner - Undergraduate level (MPSI) - 60 hours - Lycée Marcelin Berthelot </li>
                <li>Fall 2019 - Functional programming, Tutor - Undergraduate level (BSc) - 24 hours - École Polytechnique </li>
                <li>2017-2018 - Mathematics, Oral examiner - Undergraduate level (MPSI) - 60 hours - Lycée Marcelin Berthelot </li>
                <li>2017-2018 - Mathematics, Oral examiner - Undergraduate level (MPSI) - 60 hours - Lycée Marcelin Berthelot </li>

            </ul>
            -->

            <!-- ORCV 2022 -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Fall&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2022</span>
              </div>
              <div class="col-sm-11 col-md-11">
               &nbsp; Object Recognition and Computer Vision, Teacher Assistant - Master level (MVA) - 50 hours - ENS Paris-Saclay
              </div>
            </div>


              <!-- ORCV 2021 -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Fall&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2021</span>
              </div>
              <div class="col-sm-11 col-md-11">
               &nbsp; Object Recognition and Computer Vision, Project advisor - Master level (MVA) - Volunteering - ENS Paris-Saclay
              </div>
            </div>

              <!-- Colles PhD2 -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">2021 &nbsp;-&nbsp;2022</span>
              </div>
              <div class="col-sm-11 col-md-11">
                  &nbsp; Mathematics, Oral examiner - Undergraduate level (MPSI, MP and MP<sup>*</sup>) - 80 hours - Lycée Marcelin Berthelot
              </div>
            </div>

              <!-- Equa diff PhD1 -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Spring 2021</span>
              </div>
              <div class="col-sm-11 col-md-11">
               &nbsp; Differential equations, Teacher assistant - Undergraduate level (L2) - 38 hours - Sorbonne Université
              </div>
            </div>

              <!-- ORCV 2020 -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Fall&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2020</span>
              </div>
              <div class="col-sm-11 col-md-11">
               &nbsp; Object Recognition and Computer Vision, Project advisor - Master level (MVA) - Volunteering - ENS Paris-Saclay
              </div>
            </div>

              <!-- Colles 4A -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">2019 &nbsp;-&nbsp;2020</span>
              </div>
              <div class="col-sm-11 col-md-11">
               &nbsp; Mathematics, Oral examiner - Undergraduate level (MPSI) - 60 hours - Lycée Marcelin Berthelot
              </div>
            </div>

              <!-- Tutor bachelors -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Fall&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2019</span>
              </div>
              <div class="col-sm-11 col-md-11">
                &nbsp; Functional programming, Tutor - Undergraduate level (BSc) - 24 hours - École Polytechnique
              </div>
            </div>

              <!-- Colles 2A -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">2017 &nbsp;-&nbsp; 2018</span>
              </div>
              <div class="col-sm-11 col-md-11">
                &nbsp; Mathematics, Oral examiner - Undergraduate level (MPSI) - 60 hours - Lycée Marcelin Berthelot
              </div>
            </div>

              <!-- FH Internship -->
            <div class="row">
              <div class="col-sm-1 col-md-1">
                <span class="label label-teaching">Spring&nbsp;&nbsp;2017 </span>
              </div>
              <div class="col-sm-11 col-md-11">
                &nbsp; Multidisciplinary support, Socio-educational facilitator intern - Middle school - Collège Saint-Charles
              </div>
            </div>

          </div>
        </div> <!-- end of teaching -->

          <hr>

          <!-- Misc -->
        <div class="row" id="misc" style="padding-top:30px; margin-top:-60px;">
            <div class="col-md-12">
            <h2>Misc.</h2>
            Reviewer for CVPR 2022, ECCV 2022, CVPR 2023 and IJCV 2023.
            </div>
        </div> <!-- end of misc -->

      <hr>

      <div class="container">
        <footer>
          <p align="right"><small>Copyright © Antoine Yang &nbsp;/&nbsp; Last update: November 2022 </small></p>
        </footer>
        <div style="height:10px;"></div>
      </div>

      <!-- Bootstrap core JavaScript -->
      <!-- Placed at the end of the document so the pages load faster -->
      <script src="jq/jquery-1.11.1.min.js"></script>
      <script src="js/bootstrap.min.js"></script>
      <script src="js/docs.min.js"></script>

      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
            })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-75922772-1', 'auto');
        ga('send', 'pageview');

      </script>
      </div></body>
</html>
