<html lang="en">
   <head>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <link rel='icon' href='img/favicon.ico' type='image/x-icon'/>
      <meta name="description" content="Just Ask: Learning to Answer Questions from Millions of Narrated Videos">
      <meta name="author" content="WILLOW team">
      <title>Just Ask: Learning to Answer Questions from Millions of Narrated Videos</title>
      <link href="css/bootstrap.min.css" rel="stylesheet">
   </head>
   <body>

      <!-- Navigation bar -->
      <div class="navbar navbar-default  navbar-fixed-top bg-info">
        <div class="container">
          <div class="navbar-header">

            <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
          </div>
          <div class="navbar-collapse collapse" id="navbar-main">

            <ul class="nav navbar-nav navbar-left">
              <li ><a href="https://arxiv.org/abs/2012.00451">Paper</a></li>
                <li ><a href="https://github.com/antoyang/just-ask"><i class="fab fa-github"></i>Code</a></li>
               <li ><a href="#ivqa">Data</a></li>
              <li ><a href="http://videoqa.paris.inria.fr/">Demo</a></li>
               <li ><a href="https://www.youtube.com/watch?v=zxaAEtvt0wc&ab_channel=AntoineYang">Video (2 min)</a></li>
               <li ><a href="https://www.youtube.com/watch?v=eyEW5Z23Ofs&ab_channel=AntoineYang">Video (12 min)</a></li>
               <li ><a href="https://www.youtube.com/watch?v=8ZjnbehPzmE&ab_channel=AntoineYang">Video (extra results)</a></li>
            </ul>
          </div>
        </div>
      </div>

      <!-- end of navigation bar -->
      <div style="height:40px;" id="home"></div>
      <div style="height:20px;"></div>

      <div class="container">

         <div class="header">
            <h3>
               <center> <b>Just Ask: Learning to Answer Questions from Millions of Narrated Videos</b> </center>
            </h3>
         </div>
         <div style="height:10px;"></div>

         <center>
            <img id="image" src="img/justask-header.png" width="80%">
         </center>

         <div class="row">
            <h3>Abstract</h3>
            <p style="text-align: justify;">
               Recent methods for visual question answering rely on large-scale annotated datasets.
               Manual annotation of questions and answers for videos, however, is tedious, expensive and prevents scalability.
               In this work, we propose to avoid manual annotation and generate a large-scale training dataset for video question answering making use of automatic cross-modal supervision.
               We leverage a question generation transformer trained on text data and use it to generate question-answer pairs from transcribed video narrations.
               Given narrated videos, we then automatically generate the HowToVQA69M dataset with 69M video-question-answer triplets.
               To handle the open vocabulary of diverse answers in this dataset, we propose a training procedure based on a contrastive loss between a video-question multi-modal transformer and an answer transformer.
               We introduce the zero-shot VideoQA task and show excellent results, in particular for rare answers.
               Furthermore, we demonstrate our method to significantly outperform the state of the art on MSRVTT-QA, MSVD-QA, ActivityNet-QA and How2QA.
               Finally, for a detailed evaluation we introduce iVQA, a new VideoQA dataset with reduced language biases and high-quality redundant manual annotations.
	        </p>
         </div>

         <div class="row" id="demo">
            <h3>Online VideoQA Demo</h3>

            At <a href="http://videoqa.paris.inria.fr/">this link</a>, we host an online demo where you can ask the question of your choice to our models about a large set of videos. Here is an example below:

         <div class="row">
          <div class="col-sm-offset-1 col-sm-6">
         <iframe width="560" height="315" src="https://www.youtube.com/embed/XbxRqlHtKUE?start=107&end=136" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>

         <div class="col-sm-3">
         <form action="http://videoqa.paris.inria.fr/vqa?" method="get" autocomplete="off">
            <label for="video_id">YouTube Video ID</label>
            <input type="text" id="video_id" name="video_id" size="15" value="XbxRqlHtKUE"><br><br>
            <label for="start">Start second</label>
            <input type="text" id="start" name="start" size="5" value=107><br><br>
            <label for="end">End second</label>
            <input type="text" id="end" name="end" size="5" value=136><br><br>
             <b> Type your question below: </b> <br>
            <input type="text" id="question" name="question" size="50" value="What type of animal do we see in this video?"><br><br>
			 <b> Choose the model below: </b> <br>
            <input type="radio" id="finetuned" name="model" value="finetuned" checked>
             <label for="finetuned">Finetuned</label>
            <input type="radio" id="zeroshot" name="model" value="zeroshot">
             <label for="zeroshot">Zero-Shot</label><br>
            <input type="submit" value="Submit"><br>
         </form>
         </div>
         </div>

         </div>

         <div class="row" id="2min">
            <h3>Video: 2 min presentation </h3>
            <center>
               <iframe width="640" height="360" src="https://www.youtube.com/embed/zxaAEtvt0wc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </center>
         </div>

         <div class="row" id="12min">
            <h3>Video: 12 min presentation </h3>
            <center>
               <iframe width="640" height="360" src="https://www.youtube.com/embed/eyEW5Z23Ofs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </center>
         </div>

         <div class="row" id="extra">
            <h3>Video: extra results </h3>
            <center>
               <iframe width="640" height="360" src="https://www.youtube.com/embed/8ZjnbehPzmE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
            </center>
         </div>

         <div class="row" id="paper">
            <h3>Paper </h3>
            <ul class="list" style="list-style-type:square">
               <li><a href="https://arxiv.org/abs/2012.00451">arXiv</a></li>
               <li><a href="https://hal.archives-ouvertes.fr/hal-03328749/">HAL</a></li>
               <li><a href="https://openaccess.thecvf.com/content/ICCV2021/html/Yang_Just_Ask_Learning_To_Answer_Questions_From_Millions_of_Narrated_ICCV_2021_paper.html">ICCV 2021 Proceedings (CVF)</a> </li>
            </ul>

            <h4>BibTeX</h4>
               <pre><tt>@InProceedings{Yang_2021_ICCV,
    author    = {Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
    title     = {Just Ask: Learning To Answer Questions From Millions of Narrated Videos},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {1686-1697}
}</tt></pre>
         </div>

         <div class="row" id="code">
            <h3> Code </h3>
            <ul class="list" style="list-style-type:square">
               <li><a href="https://github.com/antoyang/just-ask">GitHub repository </a></li>
               <li><a href="https://drive.google.com/drive/folders/1bMfT9WjBiNWgfdVl2dej4mUaXvICGGRH?usp=sharing"> Processed data and features </a></li>
               <li><a href="https://drive.google.com/drive/folders/1DumwduSFaHB-JVZGHtkV1-f4VBYHH9xT?usp=sharing"> Model checkpoints </a></li>
            </ul>
         </div>

         <div class="row" id="ivqa">
            <h3>iVQA Data </h3>
            <ul class="list" style="list-style-type:square">
               <li><a href="https://drive.google.com/drive/folders/14e7hUcy4Ti25HdRRH5LUpuJTik2cjlzb?usp=sharing"> iVQA annotations and features </a> </li>
               <li><a href="https://docs.google.com/forms/d/e/1FAIpQLSecsMA0A0jduqEWt9EXY5wa6j-TT1GbWmBM-elQBbPhXBJkkA/viewform?usp=sf_link"> iVQA form to download raw videos </a> </li>
               <li> For a quick overview, you can visualize examples of annotations in the <a href="#extra">video of extra results</a> or in the <a href=#demo>Online VideoQA Demo </a>. </li>
            </ul>
         </div>

         <div class="row" id="howtovqa">
            <h3>HowToVQA69M Data </h3>
            <ul class="list" style="list-style-type:square">
               <li><a href="https://drive.google.com/drive/folders/1ZlpgjjcBnpTRgjwpW1z6x2PY513yhpWA?usp=sharing"> HowToVQA69M annotations </a> </li>
               <li><a href="https://www.di.ens.fr/willow/research/howto100m/">HowToVQA69M raw videos and features </a></li>
            </ul>
            <center>
               <figure>
                  <img id="sqa" src="img/justask-sqa.png" width="80%">
                  <figcaption>Examples of automatically generated video-question-answer triplets in HowToVQA69M. <font color="green">The green color </font> indicates relevant examples, <font color="orange">the orange color </font> (penultimate row) indicates a failure of the question-answer generation, and <font color="red">the red color </font> (last row) indicates that the generated question-answer is unrelated to the visual content.</figcaption>
               </figure>
            </center>
         </div>

         <div class="row" id="disclaimer">
            <h3>Disclaimer</h3>
            Data sourced from YouTube may be prone to biases.
            Please be careful of unintended societal, gender, racial and other biases when training or deploying models trained on this data.
         </div>

         <div class="row" id="misc">
            <h3> Misc. </h3>
            <ul class="list" style="list-style-type:square">
               <li> <a href="https://antoyang.github.io/slides/just-ask-iccv-poster.pdf">Poster </a></li>
                <li> <a href="https://antoyang.github.io/slides/just-ask-iccv.pdf">Slides </a></li>
               <li> <a href="https://youtu.be/jzXdRT5W3C4?t=17280">Oral presentation at the CVPR 2021 Workshop on Large Scale Holistic Video Understanding </a></li>
            </ul>
         </div>

         <div class="row" id="people">
            <h3>People</h3>
            <center>
               <table style="width:70%">
                  <tbody>
                     <tr>
                        <td style="text-align: center; vertical-align: middle;"><a href="https://antoyang.github.io/"><img id="image1" src="img/antoine.jpg" width="60" height="60"> <br> Antoine <br> Yang</a></td>
                        <td style="text-align: center; vertical-align: middle;"><a href="https://antoine77340.github.io/"><img id="image2" src="img/amiech.jpg" width="60" height="60"> <br> Antoine <br>Miech</a></td>
                        <td style="text-align: center; vertical-align: middle;"><a href="https://www.di.ens.fr/~josef/"><img id="image3" src="img/josef.jpg" width="60" height="60"> <br>Josef <br>Sivic</a></td>
                        <td style="text-align: center; vertical-align: middle;"><a href="http://www.di.ens.fr/~laptev/"><img id="image4" src="img/ivan.jpg" width="60" height="60"> <br> Ivan <br>Laptev</a></td>
                        <td style="text-align: center; vertical-align: middle;"><a href="https://www.di.ens.fr/willow/people_webpages/cordelia/"><img id="image5" src="img/cordelia.jpg" width="60" height="60"> <br>Cordelia <br>Schmid</a></td>
                     </tr>
                  </tbody>
               </table>
            </center>
         </div>

         <div class="row" id="acknowledgements">
            <h3>Acknowledgements</h3>
            <p>
               This work was granted access to the HPC resources of IDRIS under the allocation 2020-101267 made by GENCI.
            </p>
            <p>
               This work was funded by a Google gift,  the French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute), the Louis Vuitton ENS Chair on Artificial Intelligence, the European Regional Development Fund under project IMPACT (reg. no. CZ.02.1.01/0.0/0.0/15 003/0000468) and A. Miech's Google PhD fellowship.
            </p>
            <p>
            </p>
            We thank P.-L. Guhur and M. Tapaswi for advices on using Amazon Mechanical Turk, E. Berthier, Q. Le Lidec and E. Chane-Sane for manual evaluation of a sample of generated data, and I. Rocco for proofreading.
            <p></p>
         </div>

         <div class="row" id="copyright">
            <h3>Copyright Notice</h3>
            <p>The documents contained in these directories are included by the contributing authors as a means to ensure timely dissemination of scholarly and technical work on a non-commercial basis. Copyright and all rights therein are maintained by the authors or by other copyright holders, notwithstanding that they have offered their works here electronically. It is understood that all persons copying this information will adhere to the terms and constraints invoked by each author's copyright.</p>
         </div>
      </div>

      <div class="row">
         <div class="col-md-3 col-sm-12">
            <center>
               <img src="img/inria.png" width="150">
            </center>
         </div>
         <div class="col-md-3 col-sm-12">
            <center>
               <img src="img/ens.png" width="150">
            </center>
         </div>
         <div class="col-md-3 col-sm-12">
            <center>
               <img src="img/ciirc.png" width="120">
            </center>
         </div>
         <div class="col-md-3 col-sm-12">
            <center>
               <img src="img/deepmind.png" width="50">
            </center>
         </div>
      </div>
      <!-- /container -->
      <!-- Bootstrap core JavaScript
         ================================================== -->
      <!-- Placed at the end of the document so the pages load faster -->
   </body>
</html>